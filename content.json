{"pages":[{"title":"关于我","text":"个人简介 90后的程序员一枚，想要把所有让我一开始觉得困惑的技术弄懂，崇拜大牛，喜欢打游戏，写博客，钻研技术。 个人寄语 no code，no bb","link":"/about/index.html"}],"posts":[{"title":"linux常用命令整理","text":"最近，整理一下常用的linux命令，方便自己日后的使用和查阅。 # cd /home 进入 '/home' 目录# cd .. 返回上一级目录# cd ../.. 返回上两级目录# cd - 返回上次所在目录# cp file1 file2 将file1复制为file2# cp -a dir1 dir2 复制一个目录# cp -a /tmp/dir1 . 复制一个目录到当前工作目录（.代表当前目录）# ls 查看目录中的文件# ls -a 显示隐藏文件# ls -l 显示详细信息# ls -lrt 按时间显示文件（l表示详细列表，r表示反向排序，t表示按时间排序）# pwd 显示工作路径# mkdir dir1 创建 'dir1' 目录# mkdir dir1 dir2 同时创建两个目录# mkdir -p /tmp/dir1/dir2 创建一个目录树# mv dir1 dir2 移动/重命名一个目录# rm -f file1 删除 'file1'# rm -rf dir1 删除 'dir1' 目录及其子目录内容查看文件内容:# cat file1 从第一个字节开始正向查看文件的内容# head -2 file1 查看一个文件的前两行# more file1 查看一个长文件的内容# tac file1 从最后一行开始反向查看一个文件的内容# tail -3 file1 查看一个文件的最后三行文本处理:# grep str /tmp/test 在文件 '/tmp/test' 中查找 &quot;str&quot;# grep ^str /tmp/test 在文件 '/tmp/test' 中查找以 &quot;str&quot; 开始的行# grep [0-9] /tmp/test 查找 '/tmp/test' 文件中所有包含数字的行# grep str -r /tmp/* 在目录 '/tmp' 及其子目录中查找 &quot;str&quot;# diff file1 file2 找出两个文件的不同处# sdiff file1 file2 以对比的方式显示两个文件的不同查找:# find / -name file1 从 '/' 开始进入根文件系统查找文件和目录# find / -user user1 查找属于用户 'user1' 的文件和目录# find /home/user1 -name \\*.bin 在目录 '/ home/user1' 中查找以 '.bin' 结尾的文件# find /usr/bin -type f -atime +100 查找在过去100天内未被使用过的执行文件# find /usr/bin -type f -mtime -10 查找在10天内被创建或者修改过的文件# locate \\*.ps 寻找以 '.ps' 结尾的文件，先运行 'updatedb' 命令# find -name '*.[ch]' | xargs grep -E 'expr' 在当前目录及其子目录所有.c和.h文件中查找 'expr'# find -type f -print0 | xargs -r0 grep -F 'expr' 在当前目录及其子目录的常规文件中查找 'expr'# find -maxdepth 1 -type f | xargs grep -F 'expr' 在当前目录中查找 'expr'压缩和解压:# bzip2 file1 压缩 file1# bunzip2 file1.bz2 解压 file1.bz2# gzip file1 压缩 file1# gzip -9 file1 最大程度压缩 file1# gunzip file1.gz 解压 file1.gz# tar -cvf archive.tar file1 把file1打包成 archive.tar（-c: 建立压缩档案；-v: 显示所有过程；-f: 使用档案名字，是必须的，是最后一个参数）# tar -cvf archive.tar file1 dir1 把 file1，dir1 打包成 archive.tar# tar -tf archive.tar 显示一个包中的内容# tar -xvf archive.tar 释放一个包# tar -xvf archive.tar -C /tmp 把压缩包释放到 /tmp目录下# zip file1.zip file1 创建一个zip格式的压缩包# zip -r file1.zip file1 dir1 把文件和目录压缩成一个zip格式的压缩包# unzip file1.zip 解压一个zip格式的压缩包到当前目录# unzip test.zip -d /tmp/ 解压一个zip格式的压缩包到 /tmp 目录yum工具:# yum -y install [package] 下载并安装一个rpm包# yum localinstall [package.rpm] 安装一个rpm包，使用你自己的软件仓库解决所有依赖关系# yum -y update 更新当前系统中安装的所有rpm包# yum update [package] 更新一个rpm包# yum remove [package] 删除一个rpm包# yum list 列出当前系统中安装的所有包# yum search [package] 在rpm仓库中搜寻软件包# yum clean [package] 清除缓存目录（/var/cache/yum）下的软件包# yum clean headers 删除所有头文件# yum clean all 删除所有缓存的包和头文件网络:# ifconfig eth0 显示一个以太网卡的配置# ifconfig eth0 192.168.1.1 netmask 255.255.255.0 配置网卡的IP地址# ifdown eth0 禁用 'eth0' 网络设备# ifup eth0 启用 'eth0' 网络设备# iwconfig eth1 显示一个无线网卡的配置# iwlist scan 显示无线网络# ip addr show 显示网卡的IP地址其他:# su - 切换到root权限（与su有区别）# shutdown -h now 关机# shutdown -r now 重启# top 罗列使用CPU资源最多的linux任务 （输入q退出）# pstree 以树状图显示程序# man ping 查看参考手册（例如ping 命令）# passwd 修改密码# df -h 显示磁盘的使用情况# cal -3 显示前一个月，当前月以及下一个月的月历# cal 10 1988 显示指定月，年的月历 参考 https://www.cnblogs.com/wqsbk/p/5649037.html","link":"/linux-comand/"},{"title":"vim常用命令整理","text":"","link":"/vim-command/"},{"title":"冒泡排序","text":"冒泡排序（Bubble Sort），是一种计算机科学领域的较简单的排序算法。 它重复地走访过要排序的元素列，依次比较两个相邻的元素，如果顺序（如从大到小、首字母从Z到A）错误就把他们交换过来。走访元素的工作是重复地进行直到没有相邻元素需要交换，也就是说该元素列已经排序完成。 这个算法的名字由来是因为越小的元素会经由交换慢慢“浮”到数列的顶端（升序或降序排列），就如同碳酸饮料中二氧化碳的气泡最终会上浮到顶端一样，故名“冒泡排序”。 方式一：public static void bubbleSort(int arr[]) { for(int i =0 ; i&lt;arr.length-1 ; i++) { for(int j=0 ; j&lt;arr.length-1-i ; j++) { if(arr[j]&gt;arr[j+1]) { int temp = arr[j]; arr[j]=arr[j+1]; arr[j+1]=temp; } } } } 3 1 4 2 5 1 3 4 2 5 1 3 2 4 5 。。。 上述代码会把最大的冒到最后面 方式二：此种方式为冒泡法的变形 public static void maopao1(int[] arr) { for (int i = 0; i &lt; arr.length; i++) { for (int j = i+1; j &lt; arr.length; j++) { if(arr[i]&gt;arr[j]) { int temp = arr[i]; arr[i]=arr[j]; arr[j]=temp; } } } }","link":"/undefined/"},{"title":"分治算法","text":"一、基本概念在计算机科学中，分治法是一种很重要的算法。字面上的解释是“分而治之”，就是把一个复杂的问题分成两个或更多的相同或相似的子问题，再把子问题分成更小的子问题……直到最后子问题可以简单的直接求解，原问题的解即子问题的解的合并。这个技巧是很多高效算法的基础，如排序算法(快速排序，归并排序)，傅立叶变换(快速傅立叶变换)…… 任何一个可以用计算机求解的问题所需的计算时间都与其规模有关。问题的规模越小，越容易直接求解，解题所需的计算时间也越少。例如，对于n个元素的排序问题，当n=1时，不需任何计算。n=2时，只要作一次比较即可排好序。n=3时只要作3次比较即可，…。而当n较大时，问题就不那么容易处理了。要想直接解决一个规模较大的问题，有时是相当困难的。 二、基本思想及策略分治法的设计思想是：将一个难以直接解决的大问题，分割成一些规模较小的相同问题，以便各个击破，分而治之。 分治策略是：对于一个规模为n的问题，若该问题可以容易地解决（比如说规模n较小）则直接解决，否则将其分解为k个规模较小的子问题，这些子问题互相独立且与原问题形式相同，递归地解这些子问题，然后将各子问题的解合并得到原问题的解。这种算法设计策略叫做分治法。 三、分治法适用的情况 分治法所能解决的问题一般具有以下几个特征： 该问题的规模缩小到一定的程度就可以容易地解决 该问题可以分解为若干个规模较小的相同问题，即该问题具有最优子结构性质。 利用该问题分解出的子问题的解可以合并为该问题的解； 该问题所分解出的各个子问题是相互独立的，即子问题之间不包含公共的子子问题。 第一条特征是绝大多数问题都可以满足的，因为问题的计算复杂性一般是随着问题规模的增加而增加； 第二条特征是应用分治法的前提它也是大多数问题可以满足的，此特征反映了递归思想的应用；、 第三条特征是关键，能否利用分治法完全取决于问题是否具有第三条特征，如果具备了第一条和第二条特征，而不具备第三条特征，则可以考虑用贪心法或动态规划法。 第四条特征涉及到分治法的效率，如果各子问题是不独立的则分治法要做许多不必要的工作，重复地解公共的子问题，此时虽然可用分治法，但一般用动态规划法较好。 四、分治法的基本步骤分治法在每一层递归上都有三个步骤： step1 分解：将原问题分解为若干个规模较小，相互独立，与原问题形式相同的子问题； step2 解决：若子问题规模较小而容易被解决则直接解，否则递归地解各个子问题 step3 合并：将各个子问题的解合并为原问题的解。 它的一般的算法设计模式如下： Divide-and-Conquer(P) \\1. if |P|≤n0 \\2. then return(ADHOC(P)) \\3. 将P分解为较小的子问题 P1 ,P2 ,…,Pk \\4. for i←1 to k \\5. do yi ← Divide-and-Conquer(Pi) △ 递归解决Pi \\6. T ← MERGE(y1,y2,…,yk) △ 合并子问题 \\7. return(T) 其中|P|表示问题P的规模；n0为一阈值，表示当问题P的规模不超过n0时，问题已容易直接解出，不必再继续分解。ADHOC(P)是该分治法中的基本子算法，用于直接解小规模的问题P。因此，当P的规模不超过n0时直接用算法ADHOC(P)求解。算法MERGE(y1,y2,…,yk)是该分治法中的合并子算法，用于将P的子问题P1 ,P2 ,…,Pk的相应的解y1,y2,…,yk合并为P的解。 五、分治法的复杂性分析 一个分治法将规模为n的问题分成k个规模为n／m的子问题去解。设分解阀值n0=1，且adhoc解规模为1的问题耗费1个单位时间。再设将原问题分解为k个子问题以及用merge将k个子问题的解合并为原问题的解需用f(n)个单位时间。用T(n)表示该分治法解规模为|P|=n的问题所需的计算时间，则有： T（n）= k T(n/m)+f(n) 通过迭代法求得方程的解： 递归方程及其解只给出n等于m的方幂时T(n)的值，但是如果认为T(n)足够平滑，那么由n等于m的方幂时T(n)的值可以估计T(n)的增长速度。通常假定T(n)是单调上升的，从而当 mi≤n 六、可使用分治法求解的一些经典问题 （1）二分搜索 （2）大整数乘法 （3）Strassen矩阵乘法 （4）棋盘覆盖 （5）合并排序 （6）快速排序 （7）线性时间选择 （8）最接近点对问题 （9）循环赛日程表 （10）汉诺塔 七、依据分治法设计程序时的思维过程 实际上就是类似于数学归纳法，找到解决本问题的求解方程公式，然后根据方程公式设计递归程序。 1、一定是先找到最小问题规模时的求解方法 2、然后考虑随着问题规模增大时的求解方法 3、找到求解的递归函数式后（各种规模或因子），设计递归程序即可。","link":"/undefined/"},{"title":"","text":"","link":"/undefined/"},{"title":"","text":"","link":"/undefined/"},{"title":"","text":"","link":"/undefined/"},{"title":"","text":"弗洛伊德算法一、目的求解任意两点之间的最短距离（权重）（有向图）","link":"/undefined/"},{"title":"","text":"","link":"/undefined/"},{"title":"桶排序","text":"","link":"/undefined/"},{"title":"","text":"一、基本概念​ 所谓贪心算法是指，在对问题求解时，总是做出在当前看来是最好的选择。也就是说，不从整体最优上加以考虑，它所做出的仅仅是在某种意义上的局部最优解。 贪心算法没有固定的算法框架，算法设计的关键是贪心策略的选择。必须注意的是，贪心算法不是对所有问题都能得到整体最优解，选择的贪心策略必须具备无后效性（即某个状态以后的过程不会影响以前的状态，只与当前状态有关。）所以，对所采用的贪心策略一定要仔细分析其是否满足无后效性。 二、贪心算法的基本思路 建立数学模型来描述问题 把求解的问题分成若干个子问题 对每个子问题求解，得到子问题的局部最优解 把子问题的解局部最优解合成原来问题的一个解 三、该算法存在的问题 不能保证求得的最后解是最佳的 不能用来求最大值或最小值的问题 只能求满足某些约束条件的可行解的范围 四、贪心算法适用的问题贪心策略适用的前提是：局部最优策略能导致产生全局最优解。 实际上，贪心算法适用的情况很少。一般对一个问题分析是否适用于贪心算法，可以先选择该问题下的几个实际数据进行分析，就可以做出判断。 五、贪心选择性质所谓贪心选择性质是指所求问题的整体最优解可以通过一系列局部最优的选择，换句话说，当考虑做何种选择的时候，我们只考虑对当前问题最佳的选择而不考虑子问题的结果。这是贪心算法可行的第一个基本要素。贪心算法以迭代的方式作出相继的贪心选择，每作一次贪心选择就将所求问题简化为规模更小的子问题。对于一个具体问题，要确定它是否具有贪心选择性质，必须证明每一步所作的贪心选择最终导致问题的整体最优解。 当一个问题的最优解包含其子问题的最优解时，称此问题具有最优子结构性质。问题的最优子结构性质是该问题可用贪心算法求解的关键特征。 六、贪心算法的实现框架 从问题的某一初始解出发： while (朝给定总目标前进一步) { 利用可行的决策，求出可行解的一个解元素。 } 由所有解元素组合成问题的一个可行解； 五、例题分析 【背包问题】有一个背包，容量是M=150，有7个物品，物品可以分割成任意大小。要求尽可能让装入背包中的物品总价值最大，但不能超过总容量。 物品：A B C D E F G 重量：35 30 60 50 40 10 25 价值：10 40 30 50 35 40 30 分析： 目标函数： ∑pi最大 约束条件是装入的物品总质量不超过背包容量：∑wi&lt;=M( M=150) （1）根据贪心的策略，每次挑选价值最大的物品装入背包，得到的结果是否最优？ （2）每次挑选所占重量最小的物品装入是否能得到最优解？ （3）每次选取单位重量价值最大的物品，成为解本题的策略 值得注意的是，贪心算法并不是完全不可以使用，贪心策略一旦经过证明成立后，它就是一种高效的算法。比如，求最小生成树的Prim算法和Kruskal算法都是漂亮的贪心算法。 贪心算法还是很常见的算法之一，这是由于它简单易行，构造贪心策略不是很困难。 可惜的是，它需要证明后才能真正运用到题目的算法中。 一般来说，贪心算法的证明围绕着：整个问题的最优解一定由在贪心策略中存在的子问题的最优解得来的。 对于例题中的3种贪心策略，都是无法成立（无法被证明）的，解释如下： 贪心策略：选取价值最大者。反例： W=30 物品：A B C 重量：28 12 12 价值：30 20 20 根据策略，首先选取物品A，接下来就无法再选取了，可是，选取B、C则更好。 （2）贪心策略：选取重量最小。它的反例与第一种策略的反例差不多。 （3）贪心策略：选取单位重量价值最大的物品。反例： W=30 物品：A B C 重量：28 20 10 价值：28 20 10 根据策略，三种物品单位重量价值一样，程序无法依据现有策略作出判断，如果选择A，则答案错误。但是果在条件中加一句当遇见单位价值相同的时候,优先装重量小的,这样的问题就可以解决. 所以需要说明的是，贪心算法可以与随机化算法一起使用，具体的例子就不再多举了。（因为这一类算法普及性不高，而且技术含量是非常高的，需要通过一些反例确定随机的对象是什么，随机程度如何，但也是不能保证完全正确，只能是极大的几率正确）。","link":"/undefined/"},{"title":"","text":"","link":"/undefined/"},{"title":"","text":"1.最佳页面置换算法（Optimal）是由Belady于1966年提出来的一种理论上的页面置换算法，其所选择的淘汰页面是以后永久不使用，或者是在未来最长时间内不再被访问的页面。 优点：最少的缺页率 实现：只是理论上的，具体实现不了.因为人们目前还无法预知，一个页面在未来不会被访问到。 2.FIFO先进先出页面置换算法优先淘汰最早进入内存的页面，亦即在内存中驻留时间最久的页面。该算法实现简单，只需把调入内存的页面根据先后次序链接成队列，设置一个指针总指向最早的页面。但该算法与进程实际运行时的规律不适应，因为在进程中，有的页面经常被访问。 3.LRU（最近最久未使用）的设计原则LRU算法的设计原则是：如果一个数据在最近一段时间没有被访问到，那么在将来它被访问的可能性也很小。也就是说，当限定的空间已存满数据时，应当把最久没有被访问到的数据淘汰。 2.","link":"/undefined/"},{"title":"java服务莫名挂掉的线上排查","text":"最近没有时间更新博客，今天放假前一天终于有时间了。先记录一下最近发生的服务莫名挂掉的问题吧 情景线上的服务总是莫名的挂掉，进程死掉。 排查 先去看线上进程的日志，上线之前有gc日志的记录，发现没有发生重GC，也没有OOM的问题。 用top命令去查当前的进程cpu的发现当前的java程序的cpu调用率不高 基于以上种种，觉得不是现在java的问题。 ==重点来了，隆重介绍以下命令== dmesg dmesg [-cn][-s &lt;缓冲区大小&gt;]Linux dmesg命令用于显示开机信息。kernel会将开机信息存储在ring buffer中。您若是开机时来不及查看信息，可利用dmesg来查看。开机信息亦保存在/var/log目录中，名称为dmesg的文件里。 我们用如下命令去排查 dmesg -T | grep 'Out of' 发现某些时间点，系统kill了java进程，原因是OOM，经过了解发现，这台服务器上还有Moray进程，在某些时间点有定时任务，会极大的占用内存，导致系统随机kill进程。 解决方案 对服务器进行扩容操作 对当前服务进行高可用的拓展。","link":"/online/"},{"title":"oom问题","text":"笔者在开发过程中遇到了几种情况的oom，特此记录一下排查及解决方案。 1. 程序本身的oom问题==现象== 程序处理异常缓慢 （假死） 接口访问404 （真死） 1.1 定位问题 查错误日志 日志明显的OOM错误信息 查gc日志 频繁的full GC 通过这两种方式，确认线上的问题是由于oom导致。 1.2 排查问题oom的问题一般都是由大对象导致。首先拿到线上的dump文件，这边存在两种情况，下面逐一介绍。 线上的启动脚本中未加入jvm参数，未在oom的时候拿到dump文件。 查找程序的pid ps -ef | grep *** 生成dump文件 jmap -dump:format=b,file=heap.hprof ${pid} 线上的启动脚本中已经有jvm参数 依赖于启动脚本中是否加入了如下的参数 -XX:+HeapDumpOnOutOfMemoryError-XX:HeapDumpPath=${目录} dump文件拿取完成，一般是这个样子的 1.3 分析问题笔者这边用的工具为JProfile，用法如下 打开Jprofile 选择打开快照文件 查看大对象 可以查看到大对象，并由此定位到oom的问题。 1.2 非程序本身的问题详情请看 java服务莫名挂掉的线上排查","link":"/oom/"},{"title":"状态模式","text":"七大设计原则 原则一：单一功能原则 Single Responsibility Principle, SRP 核心思想：解耦和增强内聚性（高内聚，低耦合） 类被修改的几率很大，因此应该专注于单一的功能。如果你把多个功能放在同一个类中，功能之间就形成了关联，改变其中一个功能，有可能中止另一个功能，这时就需要新一轮的测试来避免可能出现的问题 原则二：开闭原则 Open-Closed Principle, OCP 核心思想：对扩展开放，对修改关闭 扩展开放：模块添加新功能，不改变原有的代码 修改关闭：某模块被其他模块调用，如果该模块的源代码不允许修改，则该模块修改关闭的 原则三：里氏替换原则 Liskov Substitution Principle, LSP 核心思想：任何父类出现的地方，子类都可以替代出现 原则四：依赖倒转原则 Dependence Inversion Principle, DIP 核心思想：要依赖于抽象，不要依赖于具体的实现 原则五：接口分离原则 Interface Segregation Principle, ISP 核心思想：不应该强迫客户程序依赖他们不需要使用的方法 一个接口不需要提供太多的行为，一个接口应该只提供一种对外的功能，不应该把所有的操作都封装到一个接口当中 原则六：合成复用原则 Composite Reuse Principle, CRP 核心思想：尽量使用对象组合，而不是继承来达到复用的目的 继承关系是强耦合，组合关系是低耦合 原则七：迪米特原则 Law of Demeter, LoD 又称最少知识原则 核心思想：一个对象应当对其他对象有尽可能少的了解,不和陌生人说话 降低各个对象之间的耦合，提高系统的可维护性 参考 https://www.cnblogs.com/chusiyong/p/11434894.html 23种设计模式1. 根据目的来分根据模式是用来完成什么工作来划分，这种方式可分为创建型模式、结构型模式和行为型模式 3 种。 创建型模式：用于描述“怎样创建对象”，它的主要特点是“将对象的创建与使用分离”。GoF 中提供了单例、原型、工厂方法、抽象工厂、建造者等 5 种创建型模式。 结构型模式：用于描述如何将类或对象按某种布局组成更大的结构，GoF 中提供了代理、适配器、桥接、装饰、外观、享元、组合等 7 种结构型模式。 行为型模式：用于描述类或对象之间怎样相互协作共同完成单个对象都无法单独完成的任务，以及怎样分配职责。GoF 中提供了模板方法、策略、命令、职责链、状态、观察者、中介者、迭代器、访问者、备忘录、解释器等 11 种行为型模式。 2. 根据作用范围来分根据模式是主要用于类上还是主要用于对象上来分，这种方式可分为类模式和对象模式两种。 类模式：用于处理类与子类之间的关系，这些关系通过继承来建立，是静态的，在编译时刻便确定下来了。GoF中的工厂方法、（类）适配器、模板方法、解释器属于该模式。 对象模式：用于处理对象之间的关系，这些关系可以通过组合或聚合来实现，在运行时刻是可以变化的，更具动态性。GoF 中除了以上 4 种，其他的都是对象模式。 表 1 介绍了这 23 种设计模式的分类。 范围\\目的 创建型模式 结构型模式 行为型模式 类模式 工厂方法 (类）适配器 模板方法、解释器 对象模式 单例 原型 抽象工厂 建造者 代理 (对象）适配器 桥接 装饰 外观 享元 组合 策略 命令 职责链 状态 观察者 中介者 迭代器 访问者 备忘录 3. GoF的23种设计模式的功能前面说明了 GoF 的 23 种设计模式的分类，现在对各个模式的功能进行介绍。 单例（Singleton）模式：某个类只能生成一个实例，该类提供了一个全局访问点供外部获取该实例，其拓展是有限多例模式。 原型（Prototype）模式：将一个对象作为原型，通过对其进行复制而克隆出多个和原型类似的新实例。 工厂方法（Factory Method）模式：定义一个用于创建产品的接口，由子类决定生产什么产品。 抽象工厂（AbstractFactory）模式：提供一个创建产品族的接口，其每个子类可以生产一系列相关的产品。 建造者（Builder）模式：将一个复杂对象分解成多个相对简单的部分，然后根据不同需要分别创建它们，最后构建成该复杂对象。 代理（Proxy）模式：为某对象提供一种代理以控制对该对象的访问。即客户端通过代理间接地访问该对象，从而限制、增强或修改该对象的一些特性。 适配器（Adapter）模式：将一个类的接口转换成客户希望的另外一个接口，使得原本由于接口不兼容而不能一起工作的那些类能一起工作。 桥接（Bridge）模式：将抽象与实现分离，使它们可以独立变化。它是用组合关系代替继承关系来实现，从而降低了抽象和实现这两个可变维度的耦合度。 装饰（Decorator）模式：动态的给对象增加一些职责，即增加其额外的功能。 外观（Facade）模式：为多个复杂的子系统提供一个一致的接口，使这些子系统更加容易被访问。 享元（Flyweight）模式：运用共享技术来有效地支持大量细粒度对象的复用。 组合（Composite）模式：将对象组合成树状层次结构，使用户对单个对象和组合对象具有一致的访问性。 模板方法（TemplateMethod）模式：定义一个操作中的算法骨架，而将算法的一些步骤延迟到子类中，使得子类可以不改变该算法结构的情况下重定义该算法的某些特定步骤。 策略（Strategy）模式：定义了一系列算法，并将每个算法封装起来，使它们可以相互替换，且算法的改变不会影响使用算法的客户。 命令（Command）模式：将一个请求封装为一个对象，使发出请求的责任和执行请求的责任分割开。 职责链（Chain of Responsibility）模式：把请求从链中的一个对象传到下一个对象，直到请求被响应为止。通过这种方式去除对象之间的耦合。 状态（State）模式：允许一个对象在其内部状态发生改变时改变其行为能力。 观察者（Observer）模式：多个对象间存在一对多关系，当一个对象发生改变时，把这种改变通知给其他多个对象，从而影响其他对象的行为。 中介者（Mediator）模式：定义一个中介对象来简化原有对象之间的交互关系，降低系统中对象间的耦合度，使原有对象之间不必相互了解。 迭代器（Iterator）模式：提供一种方法来顺序访问聚合对象中的一系列数据，而不暴露聚合对象的内部表示。 访问者（Visitor）模式：在不改变集合元素的前提下，为一个集合中的每个元素提供多种访问方式，即每个元素有多个访问者对象访问。 备忘录（Memento）模式：在不破坏封装性的前提下，获取并保存一个对象的内部状态，以便以后恢复它。 解释器（Interpreter）模式：提供如何定义语言的文法，以及对语言句子的解释方法，即解释器。 参考 http://c.biancheng.net/view/1320.html","link":"/design-pattern/"},{"title":"dubbo的简介与实际开发","text":"今天，我们再次来看dubbo3的一些东西，从头开始复习介绍一下吧 1. 什么是服务发现服务发现，即消费端自动发现服务地址列表的能力，是微服务框架需要具备的关键能力，借助于自动化的服务发现，微服务之间可以在无需感知对端部署位置与 IP 地址的情况下实现通信。 实现服务发现的方式有很多种，Dubbo 提供的是一种 Client-Based 的服务发现机制，通常还需要部署额外的第三方注册中心组件来协调服务发现过程，如常用的 Nacos、Consul、Zookeeper 等，Dubbo 自身也提供了对多种注册中心组件的对接，用户可以灵活选择。 2. 什么是RPC通信协议RPC是远程过程调用（Remote Procedure Call）的缩写形式。SAP系统RPC调用的原理其实很简单，有一些类似于三层构架的C/S系统，第三方的客户程序通过接口调用SAP内部的标准或自定义函数，获得函数返回的数据进行处理后显示或打印。 java rmi 1.1提供","link":"/dubbo-introduce/"},{"title":"Log4j2日志详解与配置1","text":"Log日志在我们的开发过程中非常常见，好的日志记录能够帮助开发者快速定位问题，解决问题，现在让我们记录一下开发过程中的常用Log4j2框架日志文件配置以及参数详解 常用的日志配置&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;!-- log4j2配置文件与log4j(1.x版本)有很大不同 --&gt;&lt;!-- status 日志级别, TRACE &lt; DEBUG &lt; INFO &lt; WARN &lt; ERROR &lt; FATAL --&gt;&lt;!-- monitorInterval 每隔300秒重新读取配置文件, 可以不重启应用的情况下修改配置 --&gt;&lt;Configuration status=&quot;DEBUG&quot; monitorInterval=&quot;300&quot;&gt; &lt;Properties&gt; &lt;property name=&quot;LOG_PATH&quot;&gt;/app/test/logs&lt;/property&gt; &lt;Property name=&quot;LOG_LAYOUT&quot;&gt;%-5p [%d{yyyy-MM-dd HH:mm:ss}] %t [%c:%L] - %m%n&lt;/Property&gt; &lt;/Properties&gt; &lt;Appenders&gt; &lt;!-- 控制台 --&gt; &lt;Console name=&quot;console&quot; target=&quot;SYSTEM_OUT&quot;&gt; &lt;encoder&gt; &lt;!--字符编码--&gt; &lt;charset&gt;UTF-8&lt;/charset&gt;&lt;!--此处设置字符集--&gt; &lt;/encoder&gt; &lt;PatternLayout pattern=&quot;${LOG_LAYOUT}&quot; /&gt; &lt;/Console&gt; &lt;RollingFile name=&quot;file_root&quot; fileName=&quot;${LOG_PATH}/root.log&quot; filePattern=&quot;${LOG_PATH}/$${date:yyyy-MM-dd}/root-%d{yyyy-MM-dd}-%i.log&quot;&gt; &lt;PatternLayout&gt; &lt;Pattern&gt;${LOG_LAYOUT}&lt;/Pattern&gt; &lt;/PatternLayout&gt; &lt;Policies&gt; &lt;TimeBasedTriggeringPolicy /&gt; &lt;!-- 指定当文件体积大于size指定的值时, 触发Rolling --&gt; &lt;SizeBasedTriggeringPolicy size=&quot;10 MB&quot; /&gt; &lt;/Policies&gt; &lt;DefaultRolloverStrategy max=&quot;20&quot; /&gt; &lt;/RollingFile&gt; &lt;RollingFile name=&quot;file_springframework&quot; fileName=&quot;${LOG_PATH}/springframework.log&quot; filePattern=&quot;${LOG_PATH}/$${date:yyyy-MM-dd}/springframework-%d{yyyy-MM-dd}-%i.log&quot;&gt; &lt;PatternLayout&gt; &lt;Pattern&gt;${LOG_LAYOUT}&lt;/Pattern&gt; &lt;/PatternLayout&gt; &lt;Policies&gt; &lt;TimeBasedTriggeringPolicy /&gt; &lt;SizeBasedTriggeringPolicy size=&quot;10 MB&quot; /&gt; &lt;/Policies&gt; &lt;DefaultRolloverStrategy max=&quot;20&quot; /&gt; &lt;/RollingFile&gt; &lt;RollingFile name=&quot;file_project&quot; fileName=&quot;${LOG_PATH}/project.log&quot; filePattern=&quot;${LOG_PATH}/$${date:yyyy-MM-dd}/project-%d{yyyy-MM-dd}-%i.log&quot;&gt; &lt;PatternLayout&gt; &lt;Pattern&gt;${LOG_LAYOUT}&lt;/Pattern&gt; &lt;/PatternLayout&gt; &lt;Policies&gt; &lt;TimeBasedTriggeringPolicy /&gt; &lt;SizeBasedTriggeringPolicy size=&quot;10 MB&quot; /&gt; &lt;/Policies&gt; &lt;DefaultRolloverStrategy max=&quot;20&quot; /&gt; &lt;/RollingFile&gt; &lt;/Appenders&gt; &lt;Loggers&gt; &lt;Logger name=&quot;com.test&quot; level=&quot;DEBUG&quot; additivity=&quot;false&quot;&gt; &lt;AppenderRef ref=&quot;file_project&quot; /&gt; &lt;AppenderRef ref=&quot;console&quot; /&gt; &lt;/Logger&gt; &lt;Logger name=&quot;org.springframework&quot; level=&quot;ERROR&quot; additivity=&quot;false&quot;&gt; &lt;AppenderRef ref=&quot;file_springframework&quot; /&gt; &lt;AppenderRef ref=&quot;console&quot; /&gt; &lt;/Logger&gt; &lt;Root level=&quot;ERROR&quot;&gt; &lt;AppenderRef ref=&quot;file_root&quot; /&gt; &lt;AppenderRef ref=&quot;console&quot; /&gt; &lt;/Root&gt; &lt;/Loggers&gt;&lt;/Configuration&gt; 配置文件参数解读 Attribute Name Description advertiser (Optional) The Advertiser plugin name which will be used to advertise individual FileAppender or SocketAppender configurations. The only Advertiser plugin provided is ‘multicastdns”. 广告商插件名称，将用于广告各个FileAppender或SocketAppender配置。提供的唯一广告商插件是“ multicastdns”。 dest Either “err” for stderr, “out” for stdout, a file path, or a URL. monitorInterval The minimum amount of time, in seconds, that must elapse before the file configuration is checked for changes. 每隔多少秒检查配置文件 name The name of the configuration.配置项名称 packages A comma separated list of package names to search for plugins. Plugins are only loaded once per classloader so changing this value may not have any effect upon reconfiguration. schema Identifies the location for the classloader to located the XML Schema to use to validate the configuration. Only valid when strict is set to true. If not set no schema validation will take place.标识类加载器的位置，该位置用于定位XML模式以用于验证配置。仅在strict设置为true时有效。如果未设置，则不会进行任何模式验证。 shutdownHook Specifies whether or not Log4j should automatically shutdown when the JVM shuts down. The shutdown hook is enabled by default but may be disabled by setting this attribute to “disable” shutdownTimeout Specifies how many milliseconds appenders and background tasks will get to shutdown when the JVM shuts down. Default is zero which mean that each appender uses its default timeout, and don’t wait for background tasks. Not all appenders will honor this, it is a hint and not an absolute guarantee that the shutdown procedure will not take longer. Setting this too low increase the risk of losing outstanding log events not yet written to the final destination. See LoggerContext.stop(long, java.util.concurrent.TimeUnit). (Not used if shutdownHook is set to “disable”.) status The level of internal Log4j events that should be logged to the console. Valid values for this attribute are “trace”, “debug”, “info”, “warn”, “error” and “fatal”. Log4j will log details about initialization, rollover and other internal actions to the status logger. Setting status=”trace” is one of the first tools available to you if you need to troubleshoot log4j.(Alternatively, setting system property log4j2.debug will also print internal Log4j2 logging to the console, including internal logging that took place before the configuration file was found.) log4j2进行初始化的，输出到控制台的日志级别 strict Enables the use of the strict XML format. Not supported in JSON configurations.启用严格XML格式的使用。JSON配置中不支持。 verbose Enables diagnostic information while loading plugins. 同步日志和异步日志。具体的文章请参考https://www.cnblogs.com/yeyang/p/7944906.html 总结： 日志输出方式 sync 同步打印日志，日志输出与业务逻辑在同一线程内，当日志输出完毕，才能进行后续业务逻辑操作 Async Appender 异步打印日志，内部采用ArrayBlockingQueue，对每个AsyncAppender创建一个线程用于处理日志输出。 Async Logger 异步打印日志，采用了高性能并发框架Disruptor，创建一个线程用于处理日志输出。 RollingRandomAccessFile和RollingFile区别参考：https://blog.csdn.net/qq_33200504/article/details/78413438 启动web应用后发现输出的日志有很大一段时间不更新或者是日志文件一直是0Kb，由于需求的问题，我们需要调整日志及时输出，然后网上搜了很多大神让我把immediateFlush属性改为true，但是发现还是没有用。最后将RollingRandomAccessFile标签改成RollingFile问题解决了。然后我百度搜了RollingRandomAccessFile和RollingFile区别然后一直没有找到，最后一个外国大佬帖子上解释到（ RandomAccessFileAppender is always buffered, while FileAppender provides a config switch (bufferedIO). Both have an “immediateFlush” config option in case you want to be sure your messages are on disk (e.g. audit logging). Finally, the default buffer size for RandomAccessFileAppender is larger: 2561024 bytes vs 81024 bytes for FileAppender (both appenders buffer size can be set in configuration). ）。大概意思是说log4j2的RollingRandomAccessFile 默认日志文件写入策略为异步刷盘，引出一个缓冲区（buffer）的概念，RollingRandomAccessFile 会将日志信息先写入到缓冲区，然后缓冲区满后刷到磁盘，并清空缓冲区，默认缓冲区的大小在8-256kb，具体大小需要自己设置。","link":"/log4j2-conf-1/"},{"title":"class文件解读","text":"今天研究一手class文件的二进制构造，以提高一下自己钻研问题的深度。 参考文章地址：https://www.cnblogs.com/signheart/p/14441383.html 准备一段java代码public class CooperTestCache { private static final Cache&lt;String, String&gt; cache = CacheBuilder.newBuilder().maximumSize(100).concurrencyLevel(20).expireAfterWrite(25, TimeUnit.MINUTES).build(); public void test() { cache.put(&quot;1&quot;, &quot;2&quot;); cache.put(&quot;2&quot;, &quot;3&quot;); cache.put(&quot;3&quot;, &quot;4&quot;); } public static void main(String[] args) throws ExecutionException { CooperTestCache.cache.put(&quot;1&quot;,&quot;2&quot;); CooperTestCache.cache.put(&quot;2&quot;,&quot;4&quot;); CooperTestCache.cache.put(&quot;3&quot;,&quot;5&quot;); CooperTestCache.cache.put(&quot;4&quot;,&quot;6&quot;); System.out.println(CooperTestCache.cache.get(&quot;88&quot;,()-&gt;{ System.out.println(&quot;我执行了&quot;); return null; })); }} 查看.class 文件将代码编译后用如下命令打开。 hexdump CooperTestCache.class 得到二进制信息如下 0000000 ca fe ba be 00 00 00 34 00 83 0a 00 1c 00 35 090000010 00 1b 00 36 08 00 37 08 00 38 0b 00 39 00 3a 080000020 00 3b 08 00 3c 08 00 3d 08 00 3e 09 00 3f 00 400000030 08 00 41 12 00 00 00 47 0b 00 39 00 48 07 00 490000040 0a 00 4a 00 4b 08 00 4c 0a 00 4d 00 4e 05 00 000000050 00 00 00 00 00 64 0a 00 4d 00 4f 0a 00 4d 00 500000060 05 00 00 00 00 00 00 00 19 09 00 51 00 52 0a 000000070 4d 00 53 0a 00 4d 00 54 07 00 55 07 00 56 01 000000080 05 63 61 63 68 65 01 00 1f 4c 63 6f 6d 2f 67 6f0000090 6f 67 6c 65 2f 63 6f 6d 6d 6f 6e 2f 63 61 63 6800000a0 65 2f 43 61 63 68 65 3b 01 00 09 53 69 67 6e 6100000b0 74 75 72 65 01 00 45 4c 63 6f 6d 2f 67 6f 6f 6700000c0 6c 65 2f 63 6f 6d 6d 6f 6e 2f 63 61 63 68 65 2f00000d0 43 61 63 68 65 3c 4c 6a 61 76 61 2f 6c 61 6e 6700000e0 2f 53 74 72 69 6e 67 3b 4c 6a 61 76 61 2f 6c 6100000f0 6e 67 2f 53 74 72 69 6e 67 3b 3e 3b 01 00 06 3c0000100 69 6e 69 74 3e 01 00 03 28 29 56 01 00 04 43 6f0000110 64 65 01 00 0f 4c 69 6e 65 4e 75 6d 62 65 72 540000120 61 62 6c 65 01 00 12 4c 6f 63 61 6c 56 61 72 690000130 61 62 6c 65 54 61 62 6c 65 01 00 04 74 68 69 730000140 01 00 35 4c 63 6f 6d 2f 6d 72 68 79 2f 73 70 720000150 69 6e 67 66 72 61 6d 65 77 6f 72 6b 2f 62 65 610000160 6e 73 2f 74 65 73 74 2f 43 6f 6f 70 65 72 54 650000170 73 74 43 61 63 68 65 3b 01 00 04 74 65 73 74 010000180 00 04 6d 61 69 6e 01 00 16 28 5b 4c 6a 61 76 610000190 2f 6c 61 6e 67 2f 53 74 72 69 6e 67 3b 29 56 0100001a0 00 04 61 72 67 73 01 00 13 5b 4c 6a 61 76 61 2f00001b0 6c 61 6e 67 2f 53 74 72 69 6e 67 3b 01 00 0a 4500001c0 78 63 65 70 74 69 6f 6e 73 07 00 57 01 00 0d 6c00001d0 61 6d 62 64 61 24 6d 61 69 6e 24 30 01 00 14 2800001e0 29 4c 6a 61 76 61 2f 6c 61 6e 67 2f 53 74 72 6900001f0 6e 67 3b 07 00 58 01 00 08 3c 63 6c 69 6e 69 740000200 3e 01 00 0a 53 6f 75 72 63 65 46 69 6c 65 01 000000210 14 43 6f 6f 70 65 72 54 65 73 74 43 61 63 68 650000220 2e 6a 61 76 61 0c 00 21 00 22 0c 00 1d 00 1e 010000230 00 01 31 01 00 01 32 07 00 59 0c 00 5a 00 5b 010000240 00 01 33 01 00 01 34 01 00 01 35 01 00 01 36 070000250 00 5c 0c 00 5d 00 5e 01 00 02 38 38 01 00 10 420000260 6f 6f 74 73 74 72 61 70 4d 65 74 68 6f 64 73 0f0000270 06 00 5f 10 00 60 0f 06 00 61 10 00 30 0c 00 620000280 00 63 0c 00 64 00 65 01 00 10 6a 61 76 61 2f 6c0000290 61 6e 67 2f 53 74 72 69 6e 67 07 00 66 0c 00 6700002a0 00 68 01 00 0c e6 88 91 e6 89 a7 e8 a1 8c e4 ba00002b0 86 07 00 69 0c 00 6a 00 6b 0c 00 6c 00 6d 0c 0000002c0 6e 00 6f 07 00 70 0c 00 71 00 72 0c 00 73 00 7400002d0 0c 00 75 00 76 01 00 33 63 6f 6d 2f 6d 72 68 7900002e0 2f 73 70 72 69 6e 67 66 72 61 6d 65 77 6f 72 6b00002f0 2f 62 65 61 6e 73 2f 74 65 73 74 2f 43 6f 6f 700000300 65 72 54 65 73 74 43 61 63 68 65 01 00 10 6a 610000310 76 61 2f 6c 61 6e 67 2f 4f 62 6a 65 63 74 01 000000320 27 6a 61 76 61 2f 75 74 69 6c 2f 63 6f 6e 63 750000330 72 72 65 6e 74 2f 45 78 65 63 75 74 69 6f 6e 450000340 78 63 65 70 74 69 6f 6e 01 00 13 6a 61 76 61 2f0000350 6c 61 6e 67 2f 45 78 63 65 70 74 69 6f 6e 01 000000360 1d 63 6f 6d 2f 67 6f 6f 67 6c 65 2f 63 6f 6d 6d0000370 6f 6e 2f 63 61 63 68 65 2f 43 61 63 68 65 01 000000380 03 70 75 74 01 00 27 28 4c 6a 61 76 61 2f 6c 610000390 6e 67 2f 4f 62 6a 65 63 74 3b 4c 6a 61 76 61 2f00003a0 6c 61 6e 67 2f 4f 62 6a 65 63 74 3b 29 56 01 0000003b0 10 6a 61 76 61 2f 6c 61 6e 67 2f 53 79 73 74 6500003c0 6d 01 00 03 6f 75 74 01 00 15 4c 6a 61 76 61 2f00003d0 69 6f 2f 50 72 69 6e 74 53 74 72 65 61 6d 3b 0a00003e0 00 77 00 78 01 00 14 28 29 4c 6a 61 76 61 2f 6c00003f0 61 6e 67 2f 4f 62 6a 65 63 74 3b 0a 00 1b 00 790000400 01 00 04 63 61 6c 6c 01 00 21 28 29 4c 6a 61 760000410 61 2f 75 74 69 6c 2f 63 6f 6e 63 75 72 72 65 6e0000420 74 2f 43 61 6c 6c 61 62 6c 65 3b 01 00 03 67 650000430 74 01 00 45 28 4c 6a 61 76 61 2f 6c 61 6e 67 2f0000440 4f 62 6a 65 63 74 3b 4c 6a 61 76 61 2f 75 74 690000450 6c 2f 63 6f 6e 63 75 72 72 65 6e 74 2f 43 61 6c0000460 6c 61 62 6c 65 3b 29 4c 6a 61 76 61 2f 6c 61 6e0000470 67 2f 4f 62 6a 65 63 74 3b 01 00 13 6a 61 76 610000480 2f 69 6f 2f 50 72 69 6e 74 53 74 72 65 61 6d 010000490 00 07 70 72 69 6e 74 6c 6e 01 00 15 28 4c 6a 6100004a0 76 61 2f 6c 61 6e 67 2f 53 74 72 69 6e 67 3b 2900004b0 56 01 00 24 63 6f 6d 2f 67 6f 6f 67 6c 65 2f 6300004c0 6f 6d 6d 6f 6e 2f 63 61 63 68 65 2f 43 61 63 6800004d0 65 42 75 69 6c 64 65 72 01 00 0a 6e 65 77 42 7500004e0 69 6c 64 65 72 01 00 28 28 29 4c 63 6f 6d 2f 6700004f0 6f 6f 67 6c 65 2f 63 6f 6d 6d 6f 6e 2f 63 61 630000500 68 65 2f 43 61 63 68 65 42 75 69 6c 64 65 72 3b0000510 01 00 0b 6d 61 78 69 6d 75 6d 53 69 7a 65 01 000000520 29 28 4a 29 4c 63 6f 6d 2f 67 6f 6f 67 6c 65 2f0000530 63 6f 6d 6d 6f 6e 2f 63 61 63 68 65 2f 43 61 630000540 68 65 42 75 69 6c 64 65 72 3b 01 00 10 63 6f 6e0000550 63 75 72 72 65 6e 63 79 4c 65 76 65 6c 01 00 290000560 28 49 29 4c 63 6f 6d 2f 67 6f 6f 67 6c 65 2f 630000570 6f 6d 6d 6f 6e 2f 63 61 63 68 65 2f 43 61 63 680000580 65 42 75 69 6c 64 65 72 3b 01 00 1d 6a 61 76 610000590 2f 75 74 69 6c 2f 63 6f 6e 63 75 72 72 65 6e 7400005a0 2f 54 69 6d 65 55 6e 69 74 01 00 07 4d 49 4e 5500005b0 54 45 53 01 00 1f 4c 6a 61 76 61 2f 75 74 69 6c00005c0 2f 63 6f 6e 63 75 72 72 65 6e 74 2f 54 69 6d 6500005d0 55 6e 69 74 3b 01 00 10 65 78 70 69 72 65 41 6600005e0 74 65 72 57 72 69 74 65 01 00 48 28 4a 4c 6a 6100005f0 76 61 2f 75 74 69 6c 2f 63 6f 6e 63 75 72 72 650000600 6e 74 2f 54 69 6d 65 55 6e 69 74 3b 29 4c 63 6f0000610 6d 2f 67 6f 6f 67 6c 65 2f 63 6f 6d 6d 6f 6e 2f0000620 63 61 63 68 65 2f 43 61 63 68 65 42 75 69 6c 640000630 65 72 3b 01 00 05 62 75 69 6c 64 01 00 21 28 290000640 4c 63 6f 6d 2f 67 6f 6f 67 6c 65 2f 63 6f 6d 6d0000650 6f 6e 2f 63 61 63 68 65 2f 43 61 63 68 65 3b 070000660 00 7a 0c 00 7b 00 7f 0c 00 2f 00 30 01 00 22 6a0000670 61 76 61 2f 6c 61 6e 67 2f 69 6e 76 6f 6b 65 2f0000680 4c 61 6d 62 64 61 4d 65 74 61 66 61 63 74 6f 720000690 79 01 00 0b 6d 65 74 61 66 61 63 74 6f 72 79 0700006a0 00 81 01 00 06 4c 6f 6f 6b 75 70 01 00 0c 49 6e00006b0 6e 65 72 43 6c 61 73 73 65 73 01 00 cc 28 4c 6a00006c0 61 76 61 2f 6c 61 6e 67 2f 69 6e 76 6f 6b 65 2f00006d0 4d 65 74 68 6f 64 48 61 6e 64 6c 65 73 24 4c 6f00006e0 6f 6b 75 70 3b 4c 6a 61 76 61 2f 6c 61 6e 67 2f00006f0 53 74 72 69 6e 67 3b 4c 6a 61 76 61 2f 6c 61 6e0000700 67 2f 69 6e 76 6f 6b 65 2f 4d 65 74 68 6f 64 540000710 79 70 65 3b 4c 6a 61 76 61 2f 6c 61 6e 67 2f 690000720 6e 76 6f 6b 65 2f 4d 65 74 68 6f 64 54 79 70 650000730 3b 4c 6a 61 76 61 2f 6c 61 6e 67 2f 69 6e 76 6f0000740 6b 65 2f 4d 65 74 68 6f 64 48 61 6e 64 6c 65 3b0000750 4c 6a 61 76 61 2f 6c 61 6e 67 2f 69 6e 76 6f 6b0000760 65 2f 4d 65 74 68 6f 64 54 79 70 65 3b 29 4c 6a0000770 61 76 61 2f 6c 61 6e 67 2f 69 6e 76 6f 6b 65 2f0000780 43 61 6c 6c 53 69 74 65 3b 07 00 82 01 00 25 6a0000790 61 76 61 2f 6c 61 6e 67 2f 69 6e 76 6f 6b 65 2f00007a0 4d 65 74 68 6f 64 48 61 6e 64 6c 65 73 24 4c 6f00007b0 6f 6b 75 70 01 00 1e 6a 61 76 61 2f 6c 61 6e 6700007c0 2f 69 6e 76 6f 6b 65 2f 4d 65 74 68 6f 64 48 6100007d0 6e 64 6c 65 73 00 21 00 1b 00 1c 00 00 00 01 0000007e0 1a 00 1d 00 1e 00 01 00 1f 00 00 00 02 00 20 0000007f0 05 00 01 00 21 00 22 00 01 00 23 00 00 00 2f 000000800 01 00 01 00 00 00 05 2a b7 00 01 b1 00 00 00 020000810 00 24 00 00 00 06 00 01 00 00 00 0e 00 25 00 000000820 00 0c 00 01 00 00 00 05 00 26 00 27 00 00 00 010000830 00 28 00 22 00 01 00 23 00 00 00 5b 00 03 00 010000840 00 00 00 25 b2 00 02 12 03 12 04 b9 00 05 03 000000850 b2 00 02 12 04 12 06 b9 00 05 03 00 b2 00 02 120000860 06 12 07 b9 00 05 03 00 b1 00 00 00 02 00 24 000000870 00 00 12 00 04 00 00 00 14 00 0c 00 15 00 18 000000880 16 00 24 00 17 00 25 00 00 00 0c 00 01 00 00 000000890 25 00 26 00 27 00 00 00 09 00 29 00 2a 00 02 0000008a0 23 00 00 00 87 00 04 00 01 00 00 00 49 b2 00 0200008b0 12 03 12 04 b9 00 05 03 00 b2 00 02 12 04 12 0700008c0 b9 00 05 03 00 b2 00 02 12 06 12 08 b9 00 05 0300008d0 00 b2 00 02 12 07 12 09 b9 00 05 03 00 b2 00 0a00008e0 b2 00 02 12 0b ba 00 0c 00 00 b9 00 0d 03 00 c000008f0 00 0e b6 00 0f b1 00 00 00 02 00 24 00 00 00 1a0000900 00 06 00 00 00 1a 00 0c 00 1b 00 18 00 1c 00 240000910 00 1d 00 30 00 1e 00 48 00 22 00 25 00 00 00 0c0000920 00 01 00 00 00 49 00 2b 00 2c 00 00 00 2d 00 000000930 00 04 00 01 00 2e 10 0a 00 2f 00 30 00 02 00 230000940 00 00 00 26 00 02 00 00 00 00 00 0a b2 00 0a 120000950 10 b6 00 0f 01 b0 00 00 00 01 00 24 00 00 00 0a0000960 00 02 00 00 00 1f 00 08 00 20 00 2d 00 00 00 040000970 00 01 00 31 00 08 00 32 00 22 00 01 00 23 00 000000980 00 36 00 04 00 00 00 00 00 1e b8 00 11 14 00 120000990 b6 00 14 10 14 b6 00 15 14 00 16 b2 00 18 b6 0000009a0 19 b6 00 1a b3 00 02 b1 00 00 00 01 00 24 00 0000009b0 00 06 00 01 00 00 00 10 00 03 00 33 00 00 00 0200009c0 00 34 00 7e 00 00 00 0a 00 01 00 7c 00 80 00 7d00009d0 00 19 00 42 00 00 00 0c 00 01 00 43 00 03 00 4400009e0 00 45 00 46 00009e4","link":"/java-class/"},{"title":"java内存区域与内存溢出异常","text":"Java运行时的数据区域Java虚拟机在执行Java程序的过程中会把他所管理的内存划分为不同的数据区域，这些区域各有各的作用，也各有各的声明周期 虚拟机栈、本地方法区、程序计数器一定不会有垃圾。 程序计数器一块较小的内存空间，是当前线程所执行的字节码的行号指示器。在jvm中，字节码解释器通过改变这个程序计数器的值，来控制程序的分支、跳转、异常处理、线程恢复等功能 Java虚拟机栈虚拟机栈描述的是Java方法执行的线程内存模型：每个方法被执行的时候，Java虚拟机都会同步创建一个栈帧，用于存储局部变量表，操作数栈、动态连接，方法出入口等信息。每一个方法被调用直至执行完毕的过程，就对应着一个栈帧在虚拟机栈中从入栈到出栈的过程。 局部变量表： ==存放了编译器可知的各种Java虚拟机基本数据类型（boolean，byte，char，short，int，float，long，double）、对象引用和returnAddress类型。== 方法区被所有的线程共享，所有的字段和方法字节码、以及一些特殊的方法、如构造函数、接口代码也在此定义，简单说，所有定义的方法的信息都在此处，此区域属于共享空间。 ==静态变量、常量、类信息（构造方法、接口定义）、运行时的常量池存在方法区中，但是实际变量存在堆方法中，与方法区无关== static final Class模板、常量池 堆（Heap）一个jvm只有一个JVM，堆内存的大小是可以调节的 堆内存中要分为三个地方 新生区 养老区 永久区 垃圾回收主要是在伊甸园区和养老区 在jdk8以后，永久存储区改了一个名字（元空间）； 新生区 类诞生和成长甚至死亡的地方 伊甸园区，所有的对象都是在伊甸园区new出来的 幸存区（0，1） 真相：经过研究，有90的的类都存活不到养老区 永久区这个区域常驻内存，用来存储JDK自带的Class对象，Interface元数据，存储的java运行时的一些内存和类信息，这个区域不存在垃圾回收，关闭虚拟机会释放这个区域的内存。 jdk1.6之前：永久代，常量池在方法区中 Jdk1.7 :去永久代，常量池在堆中 Jdk1.8 ：元空间，常量池在元空间中 GC GC两种类型：轻GC， 重GC GC算法 标记清除法 标记压缩法 引用计数法 复制算法 类加载器==class 是抽象的，用new 关键字出来的才是具体的类。== 作用加载class文件 分类 虚拟机自带的加载器 启动类（根）加载器 bootstrap class loader c++编写，主要针对于rt.jar 拓展类加载器 extension class loader 主要针对于ext底下的包 应用程序加载器 app 主要针对于用户路径下的包 双亲委派机制（双亲委派模型在jdk1.2的时候被引入）如果一个类加载器收到了类加载的请求，首先他不会自己去尝试加载这个类，而是把这个请求委派给父类加载器去完成，每一个层次的类加载器都是如此，因此所有的加载请求最终都应该传送到最顶层的类加载器中。只有当父类加载器反馈自己无法完成这个类加载请求时，子加载器才会尝试自己去调用类加载。 作用： 1、防止重复加载同一个.class。通过委托去向上面问一问，加载过了，就不用再加载一遍。保证数据安全。 2、保证核心.class不能被篡改。通过委托方式，不会去篡改核心.clas，即使篡改也不会去加载，即使加载也不会是同一个.class对象了。不同的加载器加载同一个.class也不是同一个Class对象。这样保证了Class执行安全。 Native 关键字、方法区凡是带有native关键字的，说明java的作用范围达不到了，会去调用底层c语言的库。 会调用本地方法栈 调用本地方法接口（JNI） JNI作用：拓展java使用，融合不同语言为java所用 历史：java诞生之初 C C++ 横行，想要立足 必须调用C和C++的方法 三种java虚拟机 HotSpot （sun公司） BEA JRockit IBM J9 Vm JVM垃圾什么是垃圾凡是在jvm中不被引用的对象均是垃圾 如何判定垃圾，常见的算法 \u0019\u0015计数法 弊端：无法找出相互引用的 根可达法 根： 垃圾清除常用的算法 标记清除 算法比较简单，会进行两边扫描，有碎片 复制算法（copy） 空间浪费，移动复制对象，需要调整对象引用 标记压缩 栈上分配","link":"/jvm-model/"},{"title":"java类加载的过程","text":"今天来学习一下从class到jvm类加载初始化的过程 第一步：将class文件load到内存中，其中包含以下的知识点 双亲委派 类加载器 protected Class&lt;?&gt; loadClass(String name, boolean resolve) throws ClassNotFoundException { synchronized (getClassLoadingLock(name)) { // First, check if the class has already been loaded Class&lt;?&gt; c = findLoadedClass(name); if (c == null) { long t0 = System.nanoTime(); try { if (parent != null) { c = parent.loadClass(name, false); } else { c = findBootstrapClassOrNull(name); } } catch (ClassNotFoundException e) { // ClassNotFoundException thrown if class not found // from the non-null parent class loader } if (c == null) { // If still not found, then invoke findClass in order // to find the class. long t1 = System.nanoTime(); c = findClass(name); // this is the defining class loader; record the stats sun.misc.PerfCounter.getParentDelegationTime().addTime(t1 - t0); sun.misc.PerfCounter.getFindClassTime().addElapsedTimeFrom(t1); sun.misc.PerfCounter.getFindClasses().increment(); } } if (resolve) { resolveClass(c); } return c; } } 自定义classloader（重写findclass） 将文件转换为字节 public class CustomerClassLoader extends ClassLoader { @Override protected Class&lt;?&gt; findClass(String name) throws ClassNotFoundException { File file = new File(&quot;/Users/cooper/Projects/Cooper/StudyPlan/mrhy-grow-up/mrhy-jvm/target/classes/&quot;, name.replaceAll(&quot;.&quot;, &quot;/&quot;).concat(&quot;.class&quot;)); try { FileInputStream fileInputStream = new FileInputStream(file); ByteArrayOutputStream byteArrayOutputStream = new ByteArrayOutputStream(); int b = 0; while ((b = fileInputStream.read()) != 0) { byteArrayOutputStream.write(b); } byte[] bytes = byteArrayOutputStream.toByteArray(); byteArrayOutputStream.close(); fileInputStream.close(); return defineClass(name, bytes, 0, bytes.length); } catch (Exception e) { e.printStackTrace(); } return super.findClass(name); } public static void main(String[] args) throws Exception { CustomerClassLoader customerClassLoader=new CustomerClassLoader(); Class&lt;?&gt; aClass = customerClassLoader.loadClass(&quot;com.mrhy.jvm.test.Hello&quot;); Hello hello = (Hello)aClass.newInstance(); hello.sayHello(); System.out.println(hello.getClass().getClassLoader()); System.out.println(hello.getClass().getClassLoader().getClass().getClassLoader()); }} 破坏双亲委派机制（重写classloader） 第二步：linking verification 验证文件是否符合jvm规范 preparation 为静态变量赋值 resolution 解析（resolve） 将类、方法、属性等符号引用解析为直接引用，常量池中的各种符号引用解析为指针。 第三步。Initialingzing","link":"/jvm-class/"},{"title":"java编译器和懒加载","text":"针对java是一种编译性语言还是解释性语言做出解释 针对java的懒加载做出解释 解释器bytecode intepreter JIT Just In- Time compiler 混合模式 混合使用解释器+热点代码编译 起始阶段采用解释执行 热点代码检测 多次被调用的方法（方法计数器：监测方法的执行频率） 多次被调用的循环（循环计数器：检测循环执行频率） 进行编译 可以在启动中设置解释器的模式 -Xmixed 默认为混合模式，开始解释执行，启动速度较快，对热点代码实行检测和编译 -Xint 使用解释模式，启动很快，执行稍慢 -Xcomp 使用纯编译模式，启动很慢，执行很快。 懒加载 JVM规范并没有规定何时加载 但是严格规定了什么时候必须初始化 new getstatic putstatic invokestatic指令，访问final变量除外 java.lang.reflect对类进行反射调用时 初始化子类时，父类首先初始化 虚拟机启动时，被执行的主类必须初始化 动态语言支持 java.lang.invoke.MethodHandle解析的结果为REF_getstatic REF_putstatic REF_invokestatic的方法句柄时，该类必须初始化。","link":"/jvm-jit/"},{"title":"jvm常用的参数含义以及调优","text":"记录java开发过程中常用的jvm参数设置，用以学习和工作中。 jvm性能调优Xmn Xms Xmx Xss有什么区别Xmn、Xms、Xmx、Xss都是JVM对内存的配置参数，我们可以根据不同需要区修改这些参数，以达到运行程序的最好效果。 -Xms 堆内存的最小大小，默认为物理内存的1/64 -Xmx 堆内存的最大大小，默认为物理内存的1/4 -Xmn 堆内新生代的大小。通过这个值也可以得到老生代的大小：-Xmx减去-Xmn -Xss 设置每个线程可使用的内存大小，即栈的大小。在相同物理内存下，减小这个值能生成更多的线程，当然操作系统对一个进程内的线程数还是有限制的，不能无限生成。线程栈的大小是个双刃剑，如果设置过小，可能会出现栈溢出，特别是在该线程内有递归、大的循环时出现溢出的可能性更大，如果该值设置过大，就有影响到创建栈的数量，如果是多线程的应用，就会出现内存溢出的错误。 jvm配置 堆设置 -Xms:初始堆大小 默认为物理内存的1/64 -Xmx:最大堆大小 默认为物理内存的1/4 -Xmn:新生代大小 -XX:NewRatio:设置新生代和老年代的比值。如：为3，表示年轻代与老年代比值为1：3 -XX:SurvivorRatio:新生代中Eden区与两个Survivor区的比值。注意Survivor区有两个。如：为3，表示Eden：Survivor=3：2，一个Survivor区占整个新生代的1/5 -XX:MaxTenuringThreshold:设置转入老年代的存活次数。如果是0，则直接跳过新生代进入老年代 -XX:PermSize、**-XX:MaxPermSize**:分别设置永久代最小大小与最大大小（Java8以前） -XX:MetaspaceSize、**-XX:MaxMetaspaceSize**:分别设置元空间最小大小与最大大小（Java8以后） 收集器设置 -XX:+UseSerialGC:设置串行收集器 -XX:+UseParallelGC:设置并行收集器 -XX:+UseParalledlOldGC:设置并行老年代收集器 -XX:+UseConcMarkSweepGC:设置并发收集器 垃圾回收统计信息 -XX:+PrintGC -XX:+PrintGCDetails -XX:+PrintGCTimeStamps -Xloggc:filename 并行收集器设置 -XX:ParallelGCThreads=n:设置并行收集器收集时使用的CPU数。并行收集线程数。 -XX:MaxGCPauseMillis=n:设置并行收集最大暂停时间 -XX:GCTimeRatio=n:设置垃圾回收时间占程序运行时间的百分比。公式为1/(1+n) 并发收集器设 置 -XX:+CMSIncrementalMode:设置为增量模式。适用于单CPU情况。 -XX:ParallelGCThreads=n:设置并发收集器新生代收集方式为并行收集时，使用的CPU数。并行收集线程数。 jvm调优工具JPofiler 分析dump内存文件，快速定位问题泄露 获取堆中的数据 获取大的对象","link":"/jvm-good/"},{"title":"Springboot启动源码分析","text":"今天抽时间研究一下SpringBoot的启动过程，加深一下对springBoot的学习，啥也不说，开始整活 通常我们去写一个简单的springboot应用的时候，都是如下起步 @SpringBootApplicationpublic class OfficeSpringApplication { public static void main(String[] args) { ConfigurableApplicationContext run = SpringApplication.run(OfficeSpringApplication.class, args); }} 现在我们着重看一下SpringApplication.run的流程。 // 入口跳转public static ConfigurableApplicationContext run(Class&lt;?&gt; primarySource, String... args) { return run(new Class&lt;?&gt;[] { primarySource }, args); } /** * Static helper that can be used to run a {@link SpringApplication} from the * specified sources using default settings and user supplied arguments. * @param primarySources the primary sources to load * @param args the application arguments (usually passed from a Java main method) * @return the running {@link ApplicationContext} */ public static ConfigurableApplicationContext run(Class&lt;?&gt;[] primarySources, String[] args) { return new SpringApplication(primarySources).run(args); } 构建了一个SpringApplication的对象，primarySources是构造SpringApplication类的参数，指的要加载数据的主要来源。 构造方法解读/** * Create a new {@link SpringApplication} instance. The application context will load * beans from the specified primary sources (see {@link SpringApplication class-level} * documentation for details). The instance can be customized before calling * {@link #run(String...)}. * @param primarySources the primary bean sources * @see #run(Class, String[]) * @see #SpringApplication(ResourceLoader, Class...) * @see #setSources(Set) */public SpringApplication(Class&lt;?&gt;... primarySources) { this(null, primarySources);}public SpringApplication(ResourceLoader resourceLoader, Class&lt;?&gt;... primarySources) { this.resourceLoader = resourceLoader; Assert.notNull(primarySources, &quot;PrimarySources must not be null&quot;); this.primarySources = new LinkedHashSet&lt;&gt;(Arrays.asList(primarySources)); this.webApplicationType = WebApplicationType.deduceFromClasspath(); this.bootstrapRegistryInitializers = new ArrayList&lt;&gt;( getSpringFactoriesInstances(BootstrapRegistryInitializer.class)); // 设置初始化 setInitializers((Collection) getSpringFactoriesInstances(ApplicationContextInitializer.class)); // 设置监听 setListeners((Collection) getSpringFactoriesInstances(ApplicationListener.class)); this.mainApplicationClass = deduceMainApplicationClass();} 查看如上的代码，首先几个变量 变量名 解释 resourceLoader 目前为空，资源加载器 primarySources 主要的来源 webApplicationType web应用的类型，debug下来是SERVLET bootstrapRegistryInitializers 不清楚，debug下来是空数组,因为没有这个 mainApplicationClass setInitializers getSpringFactoriesInstances(ApplicationContextInitializer.class))// 点进来 private &lt;T&gt; Collection&lt;T&gt; getSpringFactoriesInstances(Class&lt;T&gt; type) { return getSpringFactoriesInstances(type, new Class&lt;?&gt;[] {}); } private &lt;T&gt; Collection&lt;T&gt; getSpringFactoriesInstances(Class&lt;T&gt; type, Class&lt;?&gt;[] parameterTypes, Object... args) { ClassLoader classLoader = getClassLoader(); // Use names and ensure unique to protect against duplicates Set&lt;String&gt; names = new LinkedHashSet&lt;&gt;(SpringFactoriesLoader.loadFactoryNames(type, classLoader)); List&lt;T&gt; instances = createSpringFactoriesInstances(type, parameterTypes, classLoader, args, names); AnnotationAwareOrderComparator.sort(instances); return instances; } 研究一下SpringFactoriesLoader.loadFactoryNames(type, classLoader)已知 type是一个类，且此时value=interface org.springframework.context.ApplicationContextInitializerclassloader 是AppClassLoader 进入方法 public static List&lt;String&gt; loadFactoryNames(Class&lt;?&gt; factoryType, @Nullable ClassLoader classLoader) { ClassLoader classLoaderToUse = classLoader; if (classLoaderToUse == null) { classLoaderToUse = SpringFactoriesLoader.class.getClassLoader(); } String factoryTypeName = factoryType.getName(); return loadSpringFactories(classLoaderToUse).getOrDefault(factoryTypeName, Collections.emptyList());} 主要逻辑有两块loadSpringFactories(classLoaderToUse)getOrDefault(factoryTypeName, Collections.emptyList())先看第一块 private static Map&lt;String, List&lt;String&gt;&gt; loadSpringFactories(ClassLoader classLoader) { Map&lt;String, List&lt;String&gt;&gt; result = cache.get(classLoader); if (result != null) { return result; } result = new HashMap&lt;&gt;(); try { Enumeration&lt;URL&gt; urls = classLoader.getResources(FACTORIES_RESOURCE_LOCATION); while (urls.hasMoreElements()) { URL url = urls.nextElement(); UrlResource resource = new UrlResource(url); Properties properties = PropertiesLoaderUtils.loadProperties(resource); for (Map.Entry&lt;?, ?&gt; entry : properties.entrySet()) { String factoryTypeName = ((String) entry.getKey()).trim(); String[] factoryImplementationNames = StringUtils.commaDelimitedListToStringArray((String) entry.getValue()); for (String factoryImplementationName : factoryImplementationNames) { result.computeIfAbsent(factoryTypeName, key -&gt; new ArrayList&lt;&gt;()) .add(factoryImplementationName.trim()); } } } // Replace all lists with unmodifiable lists containing unique elements result.replaceAll((factoryType, implementations) -&gt; implementations.stream().distinct() .collect(Collectors.collectingAndThen(Collectors.toList(), Collections::unmodifiableList))); cache.put(classLoader, result); } catch (IOException ex) { throw new IllegalArgumentException(&quot;Unable to load factories from location [&quot; + FACTORIES_RESOURCE_LOCATION + &quot;]&quot;, ex); } return result;} 已知Classloader=AppClassLoader，获取SpringFactory static final Map&lt;ClassLoader, Map&lt;String, List&lt;String&gt;&gt;&gt; cache = new ConcurrentReferenceHashMap&lt;&gt;(); 首先从缓存中拿取，如果缓存中没有则走下面的方法 读取spring.factories 扫描所有的spring.factories，最终缓存类如下 所以，综上，初始化的类有了眉目，赋值给initializers，实际上拿取的是Spring.factories中全部的写好的类 再次解读方法，目前就可以清楚 private &lt;T&gt; Collection&lt;T&gt; getSpringFactoriesInstances(Class&lt;T&gt; type, Class&lt;?&gt;[] parameterTypes, Object... args) { ClassLoader classLoader = getClassLoader(); // Use names and ensure unique to protect against duplicates Set&lt;String&gt; names = new LinkedHashSet&lt;&gt;(SpringFactoriesLoader.loadFactoryNames(type, classLoader)); // 创建instancee示例 List&lt;T&gt; instances = createSpringFactoriesInstances(type, parameterTypes, classLoader, args, names); // 排序 AnnotationAwareOrderComparator.sort(instances); return instances;} // 查看一下排序规则，主要方法如下 private int doCompare(@Nullable Object o1, @Nullable Object o2, @Nullable OrderSourceProvider sourceProvider) { boolean p1 = (o1 instanceof PriorityOrdered); boolean p2 = (o2 instanceof PriorityOrdered); if (p1 &amp;&amp; !p2) { return -1; } else if (p2 &amp;&amp; !p1) { return 1; } int i1 = getOrder(o1, sourceProvider); int i2 = getOrder(o2, sourceProvider); return Integer.compare(i1, i2); }private int getOrder(@Nullable Object obj, @Nullable OrderSourceProvider sourceProvider) { Integer order = null; if (obj != null &amp;&amp; sourceProvider != null) { Object orderSource = sourceProvider.getOrderSource(obj); if (orderSource != null) { if (orderSource.getClass().isArray()) { for (Object source : ObjectUtils.toObjectArray(orderSource)) { order = findOrder(source); if (order != null) { break; } } } else { order = findOrder(orderSource); } } } return (order != null ? order : getOrder(obj)); } protected int getOrder(@Nullable Object obj) { if (obj != null) { Integer order = findOrder(obj); if (order != null) { return order; } } return Ordered.LOWEST_PRECEDENCE; } @Override @Nullable protected Integer findOrder(Object obj) { Integer order = super.findOrder(obj); if (order != null) { return order; } return findOrderFromAnnotation(obj); } @Nullable protected Integer findOrder(Object obj) { return (obj instanceof Ordered ? ((Ordered) obj).getOrder() : null); } 整个调用链下来，可以发现，如果实现了Order，则根据Order的顺序进行排序，如果没有，则从注解中拿取 @Nullableprivate Integer findOrderFromAnnotation(Object obj) { AnnotatedElement element = (obj instanceof AnnotatedElement ? (AnnotatedElement) obj : obj.getClass()); MergedAnnotations annotations = MergedAnnotations.from(element, SearchStrategy.TYPE_HIERARCHY); Integer order = OrderUtils.getOrderFromAnnotations(element, annotations); if (order == null &amp;&amp; obj instanceof DecoratingProxy) { return findOrderFromAnnotation(((DecoratingProxy) obj).getDecoratedClass()); } return order;}private static Integer findOrder(MergedAnnotations annotations) { MergedAnnotation&lt;Order&gt; orderAnnotation = annotations.get(Order.class); if (orderAnnotation.isPresent()) { return orderAnnotation.getInt(MergedAnnotation.VALUE); } MergedAnnotation&lt;?&gt; priorityAnnotation = annotations.get(JAVAX_PRIORITY_ANNOTATION); if (priorityAnnotation.isPresent()) { return priorityAnnotation.getInt(MergedAnnotation.VALUE); } return null;} 按照从小到大的顺序进行排序 变量名 解释 initializers 目前为spring.factories中写的继承ApplicationContextInitializer中的方法，并按照order的顺序进行排序 解读listeners变量，代码逻辑和上述一致 变量名 解释 listeners 目前为spring.factories中写的继承ApplicationListener中的方法，并按照order的顺序进行排序 最后一个mainApplicationClass变量，我们看看干了啥 private Class&lt;?&gt; deduceMainApplicationClass() { try { StackTraceElement[] stackTrace = new RuntimeException().getStackTrace(); for (StackTraceElement stackTraceElement : stackTrace) { if (&quot;main&quot;.equals(stackTraceElement.getMethodName())) { return Class.forName(stackTraceElement.getClassName()); } } } catch (ClassNotFoundException ex) { // Swallow and continue } return null;} 可以看出，是获取main方法的类 综上，Springboot的构造方法主要初始化了如下几个变量 变量名 解释 resourceLoader 目前为空，资源加载器 primarySources 主要的来源 webApplicationType web应用的类型，debug下来是SERVLET bootstrapRegistryInitializers 不清楚，debug下来是空数组,因为没有这个 mainApplicationClass 主类，标识谁去加载Spring initializers 目前为spring.factories中写的继承ApplicationContextInitializer中的方法，并按照order的顺序进行排序 listeners 目前为spring.factories中写的继承ApplicationListener中的方法，并按照order的顺序进行排序 run方法解读当SpringApplication这个对象被初始化后，开始执行run方法 /** * Run the Spring application, creating and refreshing a new * {@link ApplicationContext}. * @param args the application arguments (usually passed from a Java main method) * @return a running {@link ApplicationContext} */public ConfigurableApplicationContext run(String... args) { long startTime = System.nanoTime(); DefaultBootstrapContext bootstrapContext = createBootstrapContext(); ConfigurableApplicationContext context = null; configureHeadlessProperty(); SpringApplicationRunListeners listeners = getRunListeners(args); listeners.starting(bootstrapContext, this.mainApplicationClass); try { ApplicationArguments applicationArguments = new DefaultApplicationArguments(args); ConfigurableEnvironment environment = prepareEnvironment(listeners, bootstrapContext, applicationArguments); configureIgnoreBeanInfo(environment); Banner printedBanner = printBanner(environment); context = createApplicationContext(); context.setApplicationStartup(this.applicationStartup); prepareContext(bootstrapContext, context, environment, listeners, applicationArguments, printedBanner); refreshContext(context); afterRefresh(context, applicationArguments); Duration timeTakenToStartup = Duration.ofNanos(System.nanoTime() - startTime); if (this.logStartupInfo) { new StartupInfoLogger(this.mainApplicationClass).logStarted(getApplicationLog(), timeTakenToStartup); } listeners.started(context, timeTakenToStartup); callRunners(context, applicationArguments); } catch (Throwable ex) { handleRunFailure(context, ex, listeners); throw new IllegalStateException(ex); } try { Duration timeTakenToReady = Duration.ofNanos(System.nanoTime() - startTime); listeners.ready(context, timeTakenToReady); } catch (Throwable ex) { handleRunFailure(context, ex, null); throw new IllegalStateException(ex); } return context;} 第一个变量DefaultBootstrapContext bootstrapContext = createBootstrapContext();private DefaultBootstrapContext createBootstrapContext() { DefaultBootstrapContext bootstrapContext = new DefaultBootstrapContext(); this.bootstrapRegistryInitializers.forEach((initializer) -&gt; initializer.initialize(bootstrapContext)); return bootstrapContext;} 上面我们提到bootstrapRegistryInitializers 是空数组，所以目前来看，只是new了一个对象 configureHeadlessPropertyconfigureHeadlessProperty();SYSTEM_PROPERTY_JAVA_AWT_HEADLESS// 设置环境变量，目前作用不详，目前为trueprivate boolean headless = true;private void configureHeadlessProperty() { System.setProperty(SYSTEM_PROPERTY_JAVA_AWT_HEADLESS, System.getProperty(SYSTEM_PROPERTY_JAVA_AWT_HEADLESS, Boolean.toString(this.headless))); } getRunListenersgetRunListeners(args)private SpringApplicationRunListeners getRunListeners(String[] args) { Class&lt;?&gt;[] types = new Class&lt;?&gt;[] { SpringApplication.class, String[].class }; return new SpringApplicationRunListeners(logger, getSpringFactoriesInstances(SpringApplicationRunListener.class, types, this, args), this.applicationStartup); }private &lt;T&gt; Collection&lt;T&gt; getSpringFactoriesInstances(Class&lt;T&gt; type, Class&lt;?&gt;[] parameterTypes, Object... args) { ClassLoader classLoader = getClassLoader(); // Use names and ensure unique to protect against duplicates Set&lt;String&gt; names = new LinkedHashSet&lt;&gt;(SpringFactoriesLoader.loadFactoryNames(type, classLoader)); List&lt;T&gt; instances = createSpringFactoriesInstances(type, parameterTypes, classLoader, args, names); AnnotationAwareOrderComparator.sort(instances); return instances; } getSpringFactoriesInstances 方法咱们分析过，这里是获取继承SpringApplicationRunListener接口的实例，并按照实例的order进行排序 那看一下 new SpringApplicationRunListeners 这个构造方法 SpringApplicationRunListeners(Log log, Collection&lt;? extends SpringApplicationRunListener&gt; listeners, ApplicationStartup applicationStartup) { this.log = log; this.listeners = new ArrayList&lt;&gt;(listeners); this.applicationStartup = applicationStartup;} 变量名 解释 log 目前为空 listeners 只有一个eventPublishingRunListener applicationStartup 走的默认，默认为 ApplicationStartup.DEFAULT DefaultApplicationStartup listeners.starting(bootstrapContext, this.mainApplicationClass) todo 作用;监听者开始干活，分析一下代码干了什么 void starting(ConfigurableBootstrapContext bootstrapContext, Class&lt;?&gt; mainApplicationClass) { doWithListeners(&quot;spring.boot.application.starting&quot;, (listener) -&gt; listener.starting(bootstrapContext), (step) -&gt; { if (mainApplicationClass != null) { step.tag(&quot;mainApplicationClass&quot;, mainApplicationClass.getName()); } });}private void doWithListeners(String stepName, Consumer&lt;SpringApplicationRunListener&gt; listenerAction, Consumer&lt;StartupStep&gt; stepAction) { StartupStep step = this.applicationStartup.start(stepName); this.listeners.forEach(listenerAction); if (stepAction != null) { stepAction.accept(step); } step.end();} 此处可以复习一下consumer的知识，具体参考文章可以如下： https://cloud.tencent.com/developer/article/1488128 第一个参数此处只要是调用doWithListeners方法，实现的功能是，调用 DefaultApplicationStartup 的start方法，如下 private static final DefaultStartupStep DEFAULT_STARTUP_STEP = new DefaultStartupStep();public DefaultStartupStep start(String name) { return DEFAULT_STARTUP_STEP; } this.listeners.forEach(listenerAction)作用是，eventPublishingRunListener 调用starting方法 private final SimpleApplicationEventMulticaster initialMulticaster; public EventPublishingRunListener(SpringApplication application, String[] args) { this.application = application; this.args = args; this.initialMulticaster = new SimpleApplicationEventMulticaster(); for (ApplicationListener&lt;?&gt; listener : application.getListeners()) { this.initialMulticaster.addApplicationListener(listener); } } public void starting(ConfigurableBootstrapContext bootstrapContext) { this.initialMulticaster .multicastEvent(new ApplicationStartingEvent(bootstrapContext, this.application, this.args)); } public void multicastEvent(ApplicationEvent event) { multicastEvent(event, resolveDefaultEventType(event)); }// 此处的event类型是ApplicationStartingEventpublic void multicastEvent(final ApplicationEvent event, @Nullable ResolvableType eventType) { ResolvableType type = (eventType != null ? eventType : resolveDefaultEventType(event)); Executor executor = getTaskExecutor(); for (ApplicationListener&lt;?&gt; listener : getApplicationListeners(event, type)) { if (executor != null) { executor.execute(() -&gt; invokeListener(listener, event)); } else { invokeListener(listener, event); } } } 查看startting，变量只有一个initialMulticaster，查看变量如下 解析multicastEvent 方法 public void multicastEvent(final ApplicationEvent event, @Nullable ResolvableType eventType) { ResolvableType type = (eventType != null ? eventType : resolveDefaultEventType(event)); // 获取线程池 Executor executor = getTaskExecutor(); // 获取监听者 for (ApplicationListener&lt;?&gt; listener : getApplicationListeners(event, type)) { if (executor != null) { executor.execute(() -&gt; invokeListener(listener, event)); } else { invokeListener(listener, event); } }} 获取监听者 目前三个listener进行invoke，本质上是调用各自的onApplicationEvent方法,以LoggingApplicationListener为例 @Override public void onApplicationEvent(ApplicationEvent event) { // 此处的event全都是 ApplicationStartingEvent if (event instanceof ApplicationStartingEvent) { onApplicationStartingEvent((ApplicationStartingEvent) event); } else if (event instanceof ApplicationEnvironmentPreparedEvent) { onApplicationEnvironmentPreparedEvent((ApplicationEnvironmentPreparedEvent) event); } else if (event instanceof ApplicationPreparedEvent) { onApplicationPreparedEvent((ApplicationPreparedEvent) event); } else if (event instanceof ContextClosedEvent &amp;&amp; ((ContextClosedEvent) event).getApplicationContext().getParent() == null) { onContextClosedEvent(); } else if (event instanceof ApplicationFailedEvent) { onApplicationFailedEvent(); } }private void onApplicationStartingEvent(ApplicationStartingEvent event) { this.loggingSystem = LoggingSystem.get(event.getSpringApplication().getClassLoader()); this.loggingSystem.beforeInitialize(); }// 可以理解为对loggingSystem进行了赋值LoggingApplicationListener @Override public void beforeInitialize() { LoggerContext loggerContext = getLoggerContext(); if (isAlreadyInitialized(loggerContext)) { return; } super.beforeInitialize(); loggerContext.getConfiguration().addFilter(FILTER); } 第二个参数void starting(ConfigurableBootstrapContext bootstrapContext, Class&lt;?&gt; mainApplicationClass) { doWithListeners(&quot;spring.boot.application.starting&quot;, (listener) -&gt; listener.starting(bootstrapContext), (step) -&gt; { if (mainApplicationClass != null) { step.tag(&quot;mainApplicationClass&quot;, mainApplicationClass.getName()); } }); } private void doWithListeners(String stepName, Consumer&lt;SpringApplicationRunListener&gt; listenerAction, Consumer&lt;StartupStep&gt; stepAction) { StartupStep step = this.applicationStartup.start(stepName); this.listeners.forEach(listenerAction); if (stepAction != null) { stepAction.accept(step); } step.end(); } 以DefaultApplicationStartup为例 @Override public StartupStep tag(String key, String value) { return this; }public void end() { } 可以理解为 啥也没做，那起比较重要作用的就是listenerAction prepareEnvironment在初始化前面有一行代码 /**提供对用于运行SpringApplication的参数的访问。自从：1.3.0作者：菲利普·韦伯**/ApplicationArguments applicationArguments = new DefaultApplicationArguments(args); public DefaultApplicationArguments(String... args) { Assert.notNull(args, &quot;Args must not be null&quot;); this.source = new Source(args); this.args = args; } 获取一个ApplicationArguments对象，此时对象为 接下来让我们看prepareEnvironment(listeners, bootstrapContext, applicationArguments); 首先是看三个入参 listeners 只有一个eventPublishingRunListener bootstrapContext DefaultBootstrapContext applicationArguments 暂无别的 private ConfigurableEnvironment prepareEnvironment(SpringApplicationRunListeners listeners, DefaultBootstrapContext bootstrapContext, ApplicationArguments applicationArguments) { // Create and configure the environment ConfigurableEnvironment environment = getOrCreateEnvironment(); configureEnvironment(environment, applicationArguments.getSourceArgs()); ConfigurationPropertySources.attach(environment); listeners.environmentPrepared(bootstrapContext, environment); DefaultPropertiesPropertySource.moveToEnd(environment); Assert.state(!environment.containsProperty(&quot;spring.main.environment-prefix&quot;), &quot;Environment prefix cannot be set via properties.&quot;); bindToSpringApplication(environment); if (!this.isCustomEnvironment) { environment = convertEnvironment(environment); } ConfigurationPropertySources.attach(environment); return environment;} getOrCreateEnvironment();private ConfigurableEnvironment getOrCreateEnvironment() { if (this.environment != null) { return this.environment; } switch (this.webApplicationType) { case SERVLET: return new ApplicationServletEnvironment(); case REACTIVE: return new ApplicationReactiveWebEnvironment(); default: return new ApplicationEnvironment(); }} 返回一个新的ApplicationServletEnvironment() configureEnvironment(environment, applicationArguments.getSourceArgs());protected void configureEnvironment(ConfigurableEnvironment environment, String[] args) { if (this.addConversionService) { // 默认为true environment.setConversionService(new ApplicationConversionService()); } configurePropertySources(environment, args); configureProfiles(environment, args);} /** 在此应用程序的环境中添加、删除或重新排序任何PropertySource 。 参数： environment - 此应用程序的环境 args – 传递给run方法的参数 也可以看看： configureEnvironment(ConfigurableEnvironment, String[]) 将args 给到环境中，进行加载 **/protected void configurePropertySources(ConfigurableEnvironment environment, String[] args) { MutablePropertySources sources = environment.getPropertySources(); if (!CollectionUtils.isEmpty(this.defaultProperties)) { DefaultPropertiesPropertySource.addOrMerge(this.defaultProperties, sources); } if (this.addCommandLineProperties &amp;&amp; args.length &gt; 0) { String name = CommandLinePropertySource.COMMAND_LINE_PROPERTY_SOURCE_NAME; if (sources.contains(name)) { PropertySource&lt;?&gt; source = sources.get(name); CompositePropertySource composite = new CompositePropertySource(name); composite.addPropertySource( new SimpleCommandLinePropertySource(&quot;springApplicationCommandLineArgs&quot;, args)); composite.addPropertySource(source); sources.replace(name, composite); } else { sources.addFirst(new SimpleCommandLinePropertySource(args)); } }}protected void configureProfiles(ConfigurableEnvironment environment, String[] args) {} listeners.environmentPrepared(bootstrapContext, environment);分析一下，liseners还是只有EventPublishingRunListener void environmentPrepared(ConfigurableBootstrapContext bootstrapContext, ConfigurableEnvironment environment) { doWithListeners(&quot;spring.boot.application.environment-prepared&quot;, (listener) -&gt; listener.environmentPrepared(bootstrapContext, environment));} 又是熟悉的doWithListners的方法 这次是执行environmentPrepared public void environmentPrepared(ConfigurableBootstrapContext bootstrapContext, ConfigurableEnvironment environment) { this.initialMulticaster.multicastEvent( new ApplicationEnvironmentPreparedEvent(bootstrapContext, this.application, this.args, environment));} 研究一下getApplicationListeners(event, type) protected Collection&lt;ApplicationListener&lt;?&gt;&gt; getApplicationListeners( ApplicationEvent event, ResolvableType eventType) { Object source = event.getSource(); Class&lt;?&gt; sourceType = (source != null ? source.getClass() : null); ListenerCacheKey cacheKey = new ListenerCacheKey(eventType, sourceType); // Potential new retriever to populate CachedListenerRetriever newRetriever = null; // Quick check for existing entry on ConcurrentHashMap CachedListenerRetriever existingRetriever = this.retrieverCache.get(cacheKey); if (existingRetriever == null) { // Caching a new ListenerRetriever if possible if (this.beanClassLoader == null || (ClassUtils.isCacheSafe(event.getClass(), this.beanClassLoader) &amp;&amp; (sourceType == null || ClassUtils.isCacheSafe(sourceType, this.beanClassLoader)))) { newRetriever = new CachedListenerRetriever(); existingRetriever = this.retrieverCache.putIfAbsent(cacheKey, newRetriever); if (existingRetriever != null) { newRetriever = null; // no need to populate it in retrieveApplicationListeners } } } if (existingRetriever != null) { Collection&lt;ApplicationListener&lt;?&gt;&gt; result = existingRetriever.getApplicationListeners(); if (result != null) { return result; } // If result is null, the existing retriever is not fully populated yet by another thread. // Proceed like caching wasn't possible for this current local attempt. } return retrieveApplicationListeners(eventType, sourceType, newRetriever);}","link":"/SpringBootApplication/"},{"title":"Springboot使用SpringRetry","text":"今天开始整活，使用springRetry，根据why哥的公众号推荐，今天我们也来研究一下。具体文章地址为： https://mp.weixin.qq.com/s/XkKKBWnpAIbMxtLYRY4o-Q 功能spring-retry顾名思义，是一个用来重试的组件，在现实中，我们调第三方服务，碰到有异常的时候会有重试机制，代码为 package com.mrhy.middleware.retry;import lombok.extern.slf4j.Slf4j;import org.springframework.stereotype.Component;import java.util.concurrent.TimeoutException;/** * @author cooper * @description 平常的代码 * @date 2022/1/25 2:24 PM */@Slf4j@Componentpublic class CommonRetry { private static final Integer FINAL_RETRY_COUNT = 3; public void callChannel() throws Exception { int retryCount = 0; for (int i = 0; i &lt;= FINAL_RETRY_COUNT; i++) { if (retryCount &gt;= FINAL_RETRY_COUNT) { log.error(&quot;重试次数超过上线，&quot;); throw new Exception(&quot;连接超时&quot;); } try { queryOrder(); break; } catch (TimeoutException e) { retryCount++; log.error(&quot;第{}次查询接口失败,要重试&quot;, retryCount); } } } private void queryOrder() throws TimeoutException { throw new TimeoutException(&quot;查询订单超时&quot;); }} 调用这个接口结果为 上述的功能代码看起来一点都不优雅，且容易在for循环和失败次数上面徘徊，于是我们就引入了spring-retry这个组件 组件组件的官网地址为： https://github.com/spring-projects/spring-retry 引入组件 &lt;dependency&gt; &lt;groupId&gt;org.springframework.retry&lt;/groupId&gt; &lt;artifactId&gt;spring-retry&lt;/artifactId&gt; &lt;/dependency&gt; // 依赖aop，所以需要引入aop的组件 &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-aop&lt;/artifactId&gt; &lt;exclusions&gt; &lt;exclusion&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-logging&lt;/artifactId&gt; &lt;/exclusion&gt; &lt;/exclusions&gt; &lt;/dependency&gt; DEMO@Component@Slf4jpublic class SpringRetry { @Retryable public void call() throws Exception { throw new Exception(&quot;我出错了&quot;); } @Recover public void recover(Exception e) { log.info(&quot;没有获取到call，抛出异常&quot;); }} 从上面的代码中可以看到，spring-retry默认是重试3次。 源码解析首先我们找到RetryTemplate这个类，对第324行打断点，并重新观察调用栈","link":"/spring-retry/"},{"title":"Springboot默认线程池的理解","text":"本篇文章主要去记录对于springboot中默认线程池的研究与理解，正好借此机会复习一下如何查看springboot源码 花5分钟搭建一个springboot的项目 开始写代码 package com.mrhy.springdemo.component;import org.slf4j.Logger;import org.slf4j.LoggerFactory;import org.springframework.scheduling.annotation.Async;import org.springframework.stereotype.Component;/** * @description: * @author: cooper * @date: 2021/7/9 12:44 下午 */@Componentpublic class Test { private final Logger logger = LoggerFactory.getLogger(this.getClass().getName()); @Async public void testAsync(){ logger.info(&quot;猜猜我是谁{}&quot;,Thread.currentThread().getName()); }} package com.mrhy.springdemo.controller;import com.mrhy.springdemo.component.Test;import org.slf4j.Logger;import org.slf4j.LoggerFactory;import org.springframework.beans.factory.annotation.Autowired;import org.springframework.web.bind.annotation.GetMapping;import org.springframework.web.bind.annotation.RequestMapping;import org.springframework.web.bind.annotation.RestController;/** * @author cooper * @description * @date 2021/9/6 8:38 下午 */@RequestMapping(&quot;/async&quot;)@RestControllerpublic class AsyncController { private final Logger logger = LoggerFactory.getLogger(this.getClass().getName()); @Autowired Test test; @GetMapping(&quot;/test&quot;) public void test() { logger.info(&quot;我是主线程{}&quot;, Thread.currentThread().getName()); test.testAsync(); }} 调用 curl http://localhost:8080/async/test 查看结果 async源码探究 首先研究一下注解 import java.lang.annotation.Documented;import java.lang.annotation.ElementType;import java.lang.annotation.Retention;import java.lang.annotation.RetentionPolicy;import java.lang.annotation.Target;/** * Annotation that marks a method as a candidate for &lt;i&gt;asynchronous&lt;/i&gt; execution. * Can also be used at the type level, in which case all of the type's methods are * considered as asynchronous. Note, however, that {@code @Async} is not supported * on methods declared within a * {@link org.springframework.context.annotation.Configuration @Configuration} class. * * &lt;p&gt;In terms of target method signatures, any parameter types are supported. * However, the return type is constrained to either {@code void} or * {@link java.util.concurrent.Future}. In the latter case, you may declare the * more specific {@link org.springframework.util.concurrent.ListenableFuture} or * {@link java.util.concurrent.CompletableFuture} types which allow for richer * interaction with the asynchronous task and for immediate composition with * further processing steps. * * &lt;p&gt;A {@code Future} handle returned from the proxy will be an actual asynchronous * {@code Future} that can be used to track the result of the asynchronous method * execution. However, since the target method needs to implement the same signature, * it will have to return a temporary {@code Future} handle that just passes a value * through: e.g. Spring's {@link AsyncResult}, EJB 3.1's {@link javax.ejb.AsyncResult}, * or {@link java.util.concurrent.CompletableFuture#completedFuture(Object)}. * * @author Juergen Hoeller * @author Chris Beams * @since 3.0 * @see AnnotationAsyncExecutionInterceptor * @see AsyncAnnotationAdvisor */翻译：这个注解标志着方式是异步执行的。 也可以在类型级别使用，在这种情况下，所有类型的方法都被视为异步。 但是请注意，这@Async不支持方法内声明@Configuration类。在目标方法签名方面，支持任何参数类型。 但是，返回类型被限制为void或java.util.concurrent.Future 。 在后一种情况下，您可以声明更具体的org.springframework.util.concurrent.ListenableFuture或java.util.concurrent.CompletableFuture类型，它们允许与异步任务进行更丰富的交互，并允许与进一步处理步骤的立即组合。从代理返回的Future句柄将是一个实际的异步Future ，可用于跟踪异步方法执行的结果。 然而，由于目标方法需要实现相同的签名，它必须返回一个临时的Future句柄，它只是传递一个值：例如 Spring 的AsyncResult 、EJB 3.1 的javax.ejb.AsyncResult或java.util.concurrent.CompletableFuture.completedFuture(Object) @Target({ElementType.TYPE, ElementType.METHOD})@Retention(RetentionPolicy.RUNTIME)@Documentedpublic @interface Async { /** * A qualifier value for the specified asynchronous operation(s). * &lt;p&gt;May be used to determine the target executor to be used when executing * the asynchronous operation(s), matching the qualifier value (or the bean * name) of a specific {@link java.util.concurrent.Executor Executor} or * {@link org.springframework.core.task.TaskExecutor TaskExecutor} * bean definition. * &lt;p&gt;When specified on a class-level {@code @Async} annotation, indicates that the * given executor should be used for all methods within the class. Method-level use * of {@code Async#value} always overrides any value set at the class level. * @since 3.1.2 */ 指定异步操作的限定符值。可用于确定执行异步操作时要使用的目标执行程序，匹配特定Executor或TaskExecutor bean 定义的限定符值（或 bean 名称）。当在类级别@Async注释上指定时，表示给定的执行程序应该用于类中的所有方法。 Async#value方法级别使用始终覆盖在类级别设置的任何值。 String value() default &quot;&quot;;} @Async可以是方法，可以是类（类中全部方法均为异步） @Async不能和@Configuration同时修饰 返回值必须为void或者Futrue对象。 如果方法中和类中同时有这个注解，方法上的优先级高 追根 点击.value 注解，发现其源码只有一出引用，其他均为注释（良心方法）QAQ 我们点击第4个可以进入一个方法 /** * Return the qualifier or bean name of the executor to be used when executing the * given method, specified via {@link Async#value} at the method or declaring * class level. If {@code @Async} is specified at both the method and class level, the * method's {@code #value} takes precedence (even if empty string, indicating that * the default executor should be used preferentially). * @param method the method to inspect for executor qualifier metadata * @return the qualifier if specified, otherwise empty string indicating that the * {@linkplain #setExecutor(Executor) default executor} should be used * @see #determineAsyncExecutor(Method) */翻译：返回要执行的bean的名称，方法的优先级高于类中的优先级 @Override @Nullable protected String getExecutorQualifier(Method method) { // Maintainer's note: changes made here should also be made in // AnnotationAsyncExecutionAspect#getExecutorQualifier Async async = AnnotatedElementUtils.findMergedAnnotation(method, Async.class); if (async == null) { async = AnnotatedElementUtils.findMergedAnnotation(method.getDeclaringClass(), Async.class); } return (async != null ? async.value() : null); } 上述代码的意思是：先获取方法上的注释，如果没有则获取类上的注释。 项目启动后第一次调用会走这个方法，让我们看一下 跟着方法看他的上层调用的地方，来到了如下的方法 ==org.springframework.aop.interceptor.AsyncExecutionAspectSupport== 看这个qualifier是否为空，如果不为空，找上面的线程池，如果没有，则获取默认的。 当代码执行到第177行的时候，就已经获取默认的线程池了，线程池的属性配置如下 核心线程：8最大线程数:Interger.MAX_VALUE队列长度:Interger.MAX_VALUE空闲线程存活时间:60s 根据以上属性我们有理由判断出： ==如果我们用默认线程池，当异步线程太多的时候，完全可能存在oom的问题== 我们反回来说上面的方法。 那重点就是172行 ==targetExecutor = this.defaultExecutor.get();== 问题来了 defaultExecutor是什么 是怎么get到默认线程池的 看我标红的位置： 1. Implements BeanFactoryAware setBeanFactory 获取bean工厂，然后可以拿到很多信息。 实现了BeanFactoryAware接口的bean，可以直接通过beanfactory来访问spring的容器，当该bean被容器创建以后，会有一个相应的beanfactory的实例引用，该 接口有一个方法void setBeanFactory(BeanFactory beanFactory)方法通过这个方法的参数创建它的BeanFactory实例，实现了BeanFactoryAware接口，就可以让Bean拥有访问Spring容器的能力。 @Overridepublic void setBeanFactory(BeanFactory beanFactory) { this.beanFactory = beanFactory;} SingleTonSupplierdefaultExecutor 看看这个地方是什么时候赋值的 public AsyncExecutionAspectSupport(@Nullable Executor defaultExecutor) { this.defaultExecutor = new SingletonSupplier&lt;&gt;(defaultExecutor, () -&gt; getDefaultExecutor(this.beanFactory)); this.exceptionHandler = SingletonSupplier.of(SimpleAsyncUncaughtExceptionHandler::new);} 首先看new SingletonSupplier&lt;&gt;(defaultExecutor, () -&gt; getDefaultExecutor(this.beanFactory)); 看构造函数 /** * Build a {@code SingletonSupplier} with the given singleton instance * and a default supplier for the case when the instance is {@code null}. * @param instance the singleton instance (potentially {@code null}) * @param defaultSupplier the default supplier as a fallback */public SingletonSupplier(@Nullable T instance, Supplier&lt;? extends T&gt; defaultSupplier) { this.instanceSupplier = null; this.defaultSupplier = defaultSupplier; this.singletonInstance = instance;} 查看get方法 /** * Get the shared singleton instance for this supplier. * @return the singleton instance (or {@code null} if none) */@Override@Nullablepublic T get() { T instance = this.singletonInstance; if (instance == null) { synchronized (this) { instance = this.singletonInstance; if (instance == null) { if (this.instanceSupplier != null) { instance = this.instanceSupplier.get(); } if (instance == null &amp;&amp; this.defaultSupplier != null) { instance = this.defaultSupplier.get(); } this.singletonInstance = instance; } } } return instance;} 标准的单例模式写法， 当第一次调用的时候，也是会调用Supplier的get，看一下这个get方法 package java.util.function;/** * Represents a supplier of results. * * &lt;p&gt;There is no requirement that a new or distinct result be returned each * time the supplier is invoked. * * &lt;p&gt;This is a &lt;a href=&quot;package-summary.html&quot;&gt;functional interface&lt;/a&gt; * whose functional method is {@link #get()}. * * @param &lt;T&gt; the type of results supplied by this supplier * * @since 1.8 代表结果的提供者。没有要求每次调用供应商时都返回一个新的或不同的结果。这是一个函数式接口，其函数式方法是get() 。 */@FunctionalInterfacepublic interface Supplier&lt;T&gt; { /** * Gets a result. * * @return a result */ T get();} 理解： 1.supplier是个接口，有一个get()方法 2.语法 ： Supplier&lt;TestSupplier&gt; sup= TestSupplier::new; 3.每次调用get()方法时都会调用构造方法创建一个新对象。 这个地方就是获取一个ThreadPoolTaskExecutor对象 锁定getDefaultExecutor /** * Retrieve or build a default executor for this advice instance. * An executor returned from here will be cached for further use. * &lt;p&gt;The default implementation searches for a unique {@link TaskExecutor} bean * in the context, or for an {@link Executor} bean named &quot;taskExecutor&quot; otherwise. * If neither of the two is resolvable, this implementation will return {@code null}. * @param beanFactory the BeanFactory to use for a default executor lookup * @return the default executor, or {@code null} if none available * @since 4.2.6 * @see #findQualifiedExecutor(BeanFactory, String) * @see #DEFAULT_TASK_EXECUTOR_BEAN_NAME */@Nullableprotected Executor getDefaultExecutor(@Nullable BeanFactory beanFactory) { if (beanFactory != null) { try { // Search for TaskExecutor bean... not plain Executor since that would // match with ScheduledExecutorService as well, which is unusable for // our purposes here. TaskExecutor is more clearly designed for it. return beanFactory.getBean(TaskExecutor.class); } catch (NoUniqueBeanDefinitionException ex) { logger.debug(&quot;Could not find unique TaskExecutor bean&quot;, ex); try { return beanFactory.getBean(DEFAULT_TASK_EXECUTOR_BEAN_NAME, Executor.class); } catch (NoSuchBeanDefinitionException ex2) { if (logger.isInfoEnabled()) { logger.info(&quot;More than one TaskExecutor bean found within the context, and none is named &quot; + &quot;'taskExecutor'. Mark one of them as primary or name it 'taskExecutor' (possibly &quot; + &quot;as an alias) in order to use it for async processing: &quot; + ex.getBeanNamesFound()); } } } catch (NoSuchBeanDefinitionException ex) { logger.debug(&quot;Could not find default TaskExecutor bean&quot;, ex); try { return beanFactory.getBean(DEFAULT_TASK_EXECUTOR_BEAN_NAME, Executor.class); } catch (NoSuchBeanDefinitionException ex2) { logger.info(&quot;No task executor bean found for async processing: &quot; + &quot;no bean of type TaskExecutor and no bean named 'taskExecutor' either&quot;); } // Giving up -&gt; either using local default executor or none at all... } } return null;} 在bean中回去默认线程池 获取完成后，返回，并提交任务 至此，所有的调用链我们已经理完","link":"/spring-async/"},{"title":"maven快速生成项目的脚手架","text":"在我们实际开发过程中，好多项目的模板都是固定的，都要引用相同的包，一遍一遍的复制pom文件是一个办法，但是这种方法很浪费时间和精力，现在mvn为我们提供了一个archetype功能，现在记录一下，方便使用。 创建模板项目利用idea创建一个maven的项目，里面加上一些常用的包 生成archetype 执行 mvn clean 成功之后执行第二步 mvn archetype:create-from-project 此时如果执行成功会在当前目录下生成一个target文件夹 进入archetype目录，执行以下命令 mvn install 将项目打包成jar包，并安装在本地仓库 执行如下命令 mvn archetype:crawl 看一下官网的解释 扫描仓库，并在仓库的目录下创建一个archetype-catalog.xml 的文件 打开这个文件就能看见你的这个项目的groupId artifactId 等信息 从模板创建项目 新建maven项目，add Archetype groupId ArtifactId version 等信息均由如下产生 Repository 填写 local 点击ok生成archetype 选中create from archeType ，找到你刚添加的项目，点击next，接下来就和创建普通的maven项目是一样的 点击ok，结束之后等待pom中包的引入，然后就会看到和之前一样的项目了","link":"/maven-archetype/"},{"title":"分布式锁的解决方案","text":"分布式锁解决高并发问题public void deductStock(){ synchronized (this){ int stock=Integer.parseInt(stringRedisTemplate.opsForValue().get(&quot;test&quot;)); if (stock&gt;0){ stock=stock-1; stringRedisTemplate.opsForValue().set(&quot;test&quot;,String.valueOf(stock)); System.out.println(&quot;抢购成功：库存剩余：&quot;+stock); }else{ System.out.println(&quot;库存不足，停止抢购&quot;); } } } 问题抛出：以上代码在单机器上没有问题，在多节点的部署方式上就会有问题 优化改造1（使用redis解决分布式锁的问题）public void deductStock() { String lockKey = &quot;product&quot;; try{ Boolean result = stringRedisTemplate.opsForValue().setIfAbsent(lockKey, &quot;123&quot;, 30, TimeUnit.SECONDS); if (!result) { return; } int stock = Integer.parseInt(stringRedisTemplate.opsForValue().get(&quot;test&quot;)); if (stock &gt; 0) { stock = stock - 1; stringRedisTemplate.opsForValue().set(&quot;test&quot;, String.valueOf(stock)); System.out.println(&quot;抢购成功：库存剩余：&quot; + stock); } else { System.out.println(&quot;库存不足，停止抢购&quot;); } }finally { stringRedisTemplate.delete(lockKey); }} 上述代码存在问题：当线程1进来获取锁之后执行业务逻辑代码，线程2进来result为false，直接return，但是我们的代码中有finally，也就是return之前会先执行finally里面的代码，导致线程2将线程1里面的锁删除，因此，线程3进来，还是会存在高并发的问题。 优化改造2public void deductStock() { String lockKey = &quot;product&quot;; String uuid=UUID.randomUUID().toString(); try{ Boolean result = stringRedisTemplate.opsForValue().setIfAbsent(lockKey, uuid, 30, TimeUnit.SECONDS); if (!result) { return; } int stock = Integer.parseInt(stringRedisTemplate.opsForValue().get(&quot;test&quot;)); if (stock &gt; 0) { stock = stock - 1; stringRedisTemplate.opsForValue().set(&quot;test&quot;, String.valueOf(stock)); System.out.println(&quot;抢购成功：库存剩余：&quot; + stock); } else { System.out.println(&quot;库存不足，停止抢购&quot;); } }finally { if (uuid.equals(stringRedisTemplate.opsForValue().get(lockKey))){ stringRedisTemplate.delete(lockKey); } } } 每一个线程的value有自己的值，最后删除的时候只删除自己线程里面的key，不干涉其他线程 上述代码太长，引入redission 轻松解决 RedissionSpring引入redission&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-data-redis&lt;/artifactId&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.redisson&lt;/groupId&gt; &lt;artifactId&gt;redisson&lt;/artifactId&gt; &lt;version&gt;3.13.4&lt;/version&gt;&lt;/dependency&gt; 注入bean@Log4j2@Componentpublic class OtherBean { @Bean public Redisson redisson(){ log.info(&quot;我是redission的bean，我被加载了&quot;);// 此为单机模式 Config config=new Config(); config.useSingleServer().setAddress(&quot;redis://localhost:6379&quot;).setDatabase(0); return (Redisson)Redisson.create(config); }} 使用@Autowiredprivate Redisson redisson;public void redisTest(){ String lockKey = &quot;product&quot;; final RLock redissonLock = redisson.getLock(lockKey); try{ redissonLock.lock(); int stock = Integer.parseInt(stringRedisTemplate.opsForValue().get(&quot;test&quot;)); if (stock &gt; 0) { stock = stock - 1; stringRedisTemplate.opsForValue().set(&quot;test&quot;, String.valueOf(stock)); System.out.println(&quot;抢购成功：库存剩余：&quot; + stock); } else { System.out.println(&quot;库存不足，停止抢购&quot;); } }finally { redissonLock.unlock(); } 对比上面的代码，发现代码量减少很多，","link":"/redission/"},{"title":"spring-cloud-gateway的使用","text":"","link":"/undefined/"},{"title":"微服务中的防止雪崩的解决方案","text":"一、什么是雪崩效应在微服务的架构中，高并发的场景下，系统如过存在 A、B、C三个服务，并且 存在A服务调用B服务，B服务C服务。若C服务突然宕机或者其他因素，无法对B提供服务，这时候，B对C服务大量的调用。由于C无法提供服务，B又不断连接C，B与C的连接总是等待资源自动关闭连接，导致资源无法及时释放，最终B由于资源耗费也挂了，同理，B也导致A也挂了。（雪崩时，没有一个是无辜的，哈哈！） 二、雪落而不雪崩的解决方案隔离我们知道计算资源都是有限的，cpu，内存，队列，线程池都是资源，他们都是限定的资源数，如果不进行隔离，一个服务的调用可能要消耗很多的线程资源，把其他服务的资源都给占用了，那么可能出现应为一个服务的问题连带效应造成其他服务不能进行访问。 熔断如果说房子里面安装了电路熔断器，当你使用超大功率的电路时，有熔断设配帮你保护不至于出问题的时候把问题扩大化。 降级如果说系统后题无法提供足够的支撑能力，那么需要一个降级能力，保护系统不会被进一步恶化，而且可以对用户提供比较友好的柔性方案，例如告知用户暂时无法访问，请在一段时候后重试等等。 限流让大流量的访问冲进去我们的服务时，我们需要一定的限流措施，比方说我们规则一定时间内只允许一定的访问数从我们的资源过，如果再大的化系统会出现问题，那么就需要限流保护。 超时三、容错容错的三个核心思想 保证自己不被上游服务压垮 保证自己不被下游服务拖垮 保证自己不受外界环境影响 ## 四、解决方案（Sentinel） Sentinel规则1. 流控规则流量控制，其原理是监控流量的QPS（每秒查询率）或者并发线程数等指标1.1流控模式 直接（默认） 关联（当关联的资源达到限流条件时，开启限流（适用于资源让步）） 链路（当从某个接口的过来的资源达到限流时，开启限流，有点类似于针对来源配置项，区别在针对来源配置项针对上级微服务，链路流控针对于上级接口，也就是粒度最细）如果要使链路规则生效，则版本需要2.1.1之后，并且增加配置spring.cloud.sentinel.web-context-unify=false 1.2 流控效果 快速失败 warm up（预热） 排队等待 2. 降级规则降级规则指的就是在满足什么条件的时候对服务进行降级 慢调用比例 RT：平均响应时间 时间窗口：降级持续时间 当平均响应时间 大于 输入的值的时候，进入预降级状态，如果1秒内持续进入的请求响应时间依然大于输入的值，则进入降级状态，降级时间为时间窗口输入的值 异常比例 当资源的每秒异常总数占总量的比例超过阈值后，就会进入降级状态 异常数 当1分钟之内的异常数大于异常数，则进入降级状态，注意 时间窗口需要大于60秒3. 热点规则热点参数流控规则是一种更细粒度的规则，它允许将规则具体细化到具体的参数上 4. 授权规则自定义授权规则,继承 RequestOriginParse,自定义授权。import com.alibaba.csp.sentinel.adapter.spring.webmvc.callback.RequestOriginParser;import org.apache.commons.lang3.StringUtils;import org.springframework.context.annotation.Configuration;import javax.servlet.http.HttpServletRequest;/** * 自定义授权规则 * * @author mrhy * @date 2020/10/3 11:25 下午 * Copyright (C), 2018-2020 */@Configurationpublic class MyRequestOriginParser implements RequestOriginParser { @Override public String parseOrigin(HttpServletRequest request) {// 判断来源或者指定带有的规则 String origin = request.getHeader(&quot;origin&quot;); if (StringUtils.isEmpty(origin)){ throw new RuntimeException(&quot;origin is not empty&quot;); } return origin; }} 5. 系统规则（不推荐） 自定义异常界面继承UrlBlockHandler（Spring cloud alibaba 2.2.0 realease 之前） 继承 BlockExceptionHandler（Spring cloud alibaba 2.2.0 realease 之后） import com.alibaba.csp.sentinel.adapter.spring.webmvc.callback.BlockExceptionHandler;import com.alibaba.csp.sentinel.slots.block.BlockException;import com.alibaba.csp.sentinel.slots.block.authority.AuthorityException;import com.alibaba.csp.sentinel.slots.block.degrade.DegradeException;import com.alibaba.csp.sentinel.slots.block.flow.FlowException;import com.alibaba.csp.sentinel.slots.block.flow.param.ParamFlowException;import com.alibaba.csp.sentinel.slots.system.SystemBlockException;import com.alibaba.fastjson.JSON;import com.mrhy.wisdomcommon.common.ObjectResponse;import com.mrhy.wisdomcommon.common.OperationFlag;import org.springframework.context.annotation.Configuration;import org.springframework.http.MediaType;import javax.print.attribute.standard.Media;import javax.servlet.http.HttpServletRequest;import javax.servlet.http.HttpServletResponse;/** * 自定义sentinel界面 * * @author mrhy * @date 2020/10/3 11:34 下午 * Copyright (C), 2018-2020 */@Configurationpublic class ExceptionHandlerPage implements BlockExceptionHandler { @Override public void handle(HttpServletRequest httpServletRequest, HttpServletResponse httpServletResponse, BlockException e) throws Exception { ObjectResponse response = null; if (e instanceof FlowException) { response = new ObjectResponse(OperationFlag.INTERFACE_LIMITING.getReturnCode(), OperationFlag.INTERFACE_LIMITING.getDescription()); } else if (e instanceof DegradeException) { response = new ObjectResponse(OperationFlag.SERVICE_DEGRADATION.getReturnCode(), OperationFlag.SERVICE_DEGRADATION.getDescription()); } else if (e instanceof ParamFlowException) { response = new ObjectResponse(OperationFlag.HOT_PARAMETER_LIMITING.getReturnCode(), OperationFlag.HOT_PARAMETER_LIMITING.getDescription()); } else if (e instanceof SystemBlockException) { response = new ObjectResponse(OperationFlag.TRIGGER_SYSTEM_PROTECT_ROLE.getReturnCode(), OperationFlag.TRIGGER_SYSTEM_PROTECT_ROLE.getDescription()); } else if (e instanceof AuthorityException) { response = new ObjectResponse(OperationFlag.AUTHORIZATION_RULES.getReturnCode(), OperationFlag.AUTHORIZATION_RULES.getDescription()); } httpServletResponse.setCharacterEncoding(&quot;utf-8&quot;); httpServletResponse.setContentType(MediaType.APPLICATION_JSON_VALUE); httpServletResponse.getWriter().write(JSON.toJSONString(response)); }} 状态码 import lombok.AllArgsConstructor;import lombok.Getter;/** * 操作符 * * @author mrhy */@AllArgsConstructor@Getterpublic enum OperationFlag { /** * 成功 */ SUCCESS(0, &quot;操作成功&quot;), /** * 失败 */ FAIL(-1, &quot;操作失败&quot;), /** * 参数违法 */ ILLEGAL_ARGUMENT(-2, &quot;参数违法&quot;), /** * 未登录 */ NOT_LOGIN(401, &quot;未登录&quot;), /** * 服务降级 */ SERVICE_DEGRADATION(100, &quot;服务降级&quot;), /** * 接口限流 */ INTERFACE_LIMITING(101, &quot;接口限流&quot;), /** * 热电参数限流 */ HOT_PARAMETER_LIMITING(102, &quot;热点参数限流&quot;), /** * 触发系统保护规则 */ TRIGGER_SYSTEM_PROTECT_ROLE(103, &quot;触发系统保护规则&quot;), /** * 授权规则不通过 */ AUTHORIZATION_RULES(104, &quot;授权规则不通过&quot;), ; private final Integer returnCode; private final String description;} /** * 响应实体类 * @author mrhy */public class ObjectResponse { private Integer returnCode; private String description; private Object result; public ObjectResponse() { this.returnCode = OperationFlag.SUCCESS.getReturnCode(); this.description = OperationFlag.SUCCESS.getDescription(); } public ObjectResponse(Integer returnCode) { this(returnCode, (String) null); } public ObjectResponse(Object result) { this.returnCode = OperationFlag.SUCCESS.getReturnCode(); this.description = OperationFlag.SUCCESS.getDescription(); this.result = result; } public ObjectResponse(Integer returnCode, String description) { this(returnCode, description, (Object) null); } public ObjectResponse(Integer returnCode, String description, Object result) { this.returnCode = returnCode; this.description = description; this.result = result; } public Integer getReturnCode() { return returnCode; } public void setReturnCode(Integer returnCode) { this.returnCode = returnCode; } public String getDescription() { return description; } public void setDescription(String description) { this.description = description; } public Object getResult() { return result; } public void setResult(Object result) { this.result = result; }} 将配置同步到nacos引入依赖 &lt;dependency&gt; &lt;groupId&gt;com.alibaba.csp&lt;/groupId&gt; &lt;artifactId&gt;sentinel-datasource-nacos&lt;/artifactId&gt; &lt;/dependency&gt; 配置application ## ds指的数据源，可以随便写## server-addr 指的是nacos地址spring.cloud.sentinel.datasource.ds.nacos.server-addr=localhost:8848 ## 对应配置中的data-idspring.cloud.sentinel.datasource.ds.nacos.data-id=sentinel-wisdom-web## 对应配置中的group-idspring.cloud.sentinel.datasource.ds.nacos.group-id=DEFAULT_GROUP## 对应文件类型spring.cloud.sentinel.datasource.ds.nacos.data-type=json## 降级规则spring.cloud.sentinel.datasource.ds.nacos.rule-type=flow 配置nacos（配置内容） [ { &quot;resource&quot;: &quot;test&quot;, // 资源名称 &quot;limitApp&quot;: &quot;default&quot;, //来源应用 &quot;grade&quot;: 1,//阈值类型 0 线程数 1QPS &quot;count&quot;: 5,// 单机阈值 &quot;strategy&quot;: 0,// 流控模式 0 直接 1 关联 2 链路 &quot;controlBehavior&quot;: 0,// 流控效果 0 快速失败 1 warmup 2 排队等待 &quot;clusterMode&quot;: false // 是否是集群模式 }] 参考网站sentinel github 网站 将配置持久化到本地编写配置类 import com.alibaba.csp.sentinel.command.handler.ModifyParamFlowRulesCommandHandler;import com.alibaba.csp.sentinel.datasource.*;import com.alibaba.csp.sentinel.init.InitFunc;import com.alibaba.csp.sentinel.slots.block.authority.AuthorityRule;import com.alibaba.csp.sentinel.slots.block.authority.AuthorityRuleManager;import com.alibaba.csp.sentinel.slots.block.degrade.DegradeRule;import com.alibaba.csp.sentinel.slots.block.degrade.DegradeRuleManager;import com.alibaba.csp.sentinel.slots.block.flow.FlowRule;import com.alibaba.csp.sentinel.slots.block.flow.FlowRuleManager;import com.alibaba.csp.sentinel.slots.block.flow.param.ParamFlowRule;import com.alibaba.csp.sentinel.slots.block.flow.param.ParamFlowRuleManager;import com.alibaba.csp.sentinel.slots.system.SystemRule;import com.alibaba.csp.sentinel.slots.system.SystemRuleManager;import com.alibaba.csp.sentinel.transport.util.WritableDataSourceRegistry;import com.alibaba.fastjson.JSON;import com.alibaba.fastjson.TypeReference;import org.springframework.beans.factory.annotation.Value;import org.springframework.context.annotation.Configuration;import java.io.File;import java.io.IOException;import java.util.List;/** * sentinel 配置持久化到本地 * * @author mrhy * @date 2020/10/4 10:17 上午 * Copyright (C), 2018-2020 */@Configurationpublic class FilePersistence implements InitFunc { private final String APPLICATION_NAME = &quot;wisdom-web&quot;; @Override public void init() throws Exception { String ruleDir = System.getProperty(&quot;user.home&quot;) + &quot;/sentinel-rules/&quot; + APPLICATION_NAME; String flowRulePath = ruleDir + &quot;/flow-rule.json&quot;; String degradeRulePath = ruleDir + &quot;/degrade-rule.json&quot;; String systemRulePath = ruleDir + &quot;/system-rule.json&quot;; String authorityRulePath = ruleDir + &quot;/authority-rule.json&quot;; String paramFlowRulePath = ruleDir + &quot;/param-flow-rule.json&quot;; this.mkdirIfNotExits(ruleDir); this.createFileIfNotExits(flowRulePath); this.createFileIfNotExits(degradeRulePath); this.createFileIfNotExits(systemRulePath); this.createFileIfNotExits(authorityRulePath); this.createFileIfNotExits(paramFlowRulePath); // 注册一个可读数据源，用来定时读取本地的json文件，更新到规则缓存中 // 流控规则 ReadableDataSource&lt;String, List&lt;FlowRule&gt;&gt; flowRuleRDS = new FileRefreshableDataSource&lt;&gt;(flowRulePath, flowRuleListParser); // 将可读数据源注册至FlowRuleManager // 这样当规则文件发生变化时，就会更新规则到内存 FlowRuleManager.register2Property(flowRuleRDS.getProperty()); WritableDataSource&lt;List&lt;FlowRule&gt;&gt; flowRuleWDS = new FileWritableDataSource&lt;&gt;( flowRulePath, this::encodeJson ); // 将可写数据源注册至transport模块的WritableDataSourceRegistry中 // 这样收到控制台推送的规则时，Sentinel会先更新到内存，然后将规则写入到文件中 WritableDataSourceRegistry.registerFlowDataSource(flowRuleWDS); // 降级规则 ReadableDataSource&lt;String, List&lt;DegradeRule&gt;&gt; degradeRuleRDS = new FileRefreshableDataSource&lt;&gt;( degradeRulePath, degradeRuleListParser ); DegradeRuleManager.register2Property(degradeRuleRDS.getProperty()); WritableDataSource&lt;List&lt;DegradeRule&gt;&gt; degradeRuleWDS = new FileWritableDataSource&lt;&gt;( degradeRulePath, this::encodeJson ); WritableDataSourceRegistry.registerDegradeDataSource(degradeRuleWDS); // 系统规则 ReadableDataSource&lt;String, List&lt;SystemRule&gt;&gt; systemRuleRDS = new FileRefreshableDataSource&lt;&gt;( systemRulePath, systemRuleListParser ); SystemRuleManager.register2Property(systemRuleRDS.getProperty()); WritableDataSource&lt;List&lt;SystemRule&gt;&gt; systemRuleWDS = new FileWritableDataSource&lt;&gt;( systemRulePath, this::encodeJson ); WritableDataSourceRegistry.registerSystemDataSource(systemRuleWDS); // 授权规则 ReadableDataSource&lt;String, List&lt;AuthorityRule&gt;&gt; authorityRuleRDS = new FileRefreshableDataSource&lt;&gt;( authorityRulePath, authorityRuleListParser ); AuthorityRuleManager.register2Property(authorityRuleRDS.getProperty()); WritableDataSource&lt;List&lt;AuthorityRule&gt;&gt; authorityRuleWDS = new FileWritableDataSource&lt;&gt;( authorityRulePath, this::encodeJson ); WritableDataSourceRegistry.registerAuthorityDataSource(authorityRuleWDS); // 热点参数规则 ReadableDataSource&lt;String, List&lt;ParamFlowRule&gt;&gt; paramFlowRuleRDS = new FileRefreshableDataSource&lt;&gt;( paramFlowRulePath, paramFlowRuleListParser ); ParamFlowRuleManager.register2Property(paramFlowRuleRDS.getProperty()); WritableDataSource&lt;List&lt;ParamFlowRule&gt;&gt; paramFlowRuleWDS = new FileWritableDataSource&lt;&gt;( paramFlowRulePath, this::encodeJson ); ModifyParamFlowRulesCommandHandler.setWritableDataSource(paramFlowRuleWDS); } private Converter&lt;String, List&lt;FlowRule&gt;&gt; flowRuleListParser = source -&gt; JSON.parseObject( source, new TypeReference&lt;List&lt;FlowRule&gt;&gt;() { } ); private Converter&lt;String, List&lt;DegradeRule&gt;&gt; degradeRuleListParser = source -&gt; JSON.parseObject( source, new TypeReference&lt;List&lt;DegradeRule&gt;&gt;() { } ); private Converter&lt;String, List&lt;SystemRule&gt;&gt; systemRuleListParser = source -&gt; JSON.parseObject( source, new TypeReference&lt;List&lt;SystemRule&gt;&gt;() { } ); private Converter&lt;String, List&lt;AuthorityRule&gt;&gt; authorityRuleListParser = source -&gt; JSON.parseObject( source, new TypeReference&lt;List&lt;AuthorityRule&gt;&gt;() { } ); private Converter&lt;String, List&lt;ParamFlowRule&gt;&gt; paramFlowRuleListParser = source -&gt; JSON.parseObject( source, new TypeReference&lt;List&lt;ParamFlowRule&gt;&gt;() { } ); private void mkdirIfNotExits(String filePath) throws IOException { File file = new File(filePath); if (!file.exists()) { file.mkdirs(); } } private void createFileIfNotExits(String filePath) throws IOException { File file = new File(filePath); if (!file.exists()) { file.createNewFile(); } } private &lt;T&gt; String encodeJson(T t) { return JSON.toJSONString(t); }} 在resource 下 新建这个目录 META-INF.services 在这个目录下新建文件 com.alibaba.csp.sentinel.init.InitFunc 添加内容 上面写的java文件的路径 copy reference","link":"/undefined/"},{"title":"各种IO","text":"","link":"/IO*/"},{"title":"日常小记录","text":"spring想要运行jar包里面的main方法 java -classpath jar包名.jar 包名.类名 比如 java -classpath mrhy-tk-mybatis-1.0-SNAPSHOT.jar com.mrhy.tkmybatis.controller.TestController 111 222","link":"/dailyRecord/"},{"title":"线程池","text":"博客搭的好，笔记不能少 线程池的种类先整理这4种吧 newCachedThreadPool创建一个可缓存线程池，如果线程池长度超过处理需要，可灵活回收空闲线程，若无可回收，则新建线程。newFixedThreadPool 创建一个定长线程池，可控制线程最大并发数，超出的线程会在队列中等待。newScheduledThreadPool 创建一个定长线程池，支持定时及周期性任务执行。newSingleThreadExecutor 创建一个单线程化的线程池，它只会用唯一的工作线程来执行任务，保证所有任务按照指定顺序(FIFO, LILO)执行。 newCachedThreadPool工作线程的创建数量几乎没有限制(其实也有限制的,数目为Interger. MAX_VALUE), 这样可灵活的往线程池中添加线程。 如果长时间没有往线程池中提交任务，即如果工作线程空闲了指定的时间(默认为1分钟)，则该工作线程将自动终止。终止后，如果你又提交了新的任务，则线程池重新创建一个工作线程。 在使用CachedThreadPool时，一定要注意控制任务的数量，否则，由于大量线程同时运行，很有会造成系统OOM。 有着时间间隔的提交 public static void main(String[] args) throws InterruptedException { ExecutorService executorService = Executors.newCachedThreadPool(); for (int i = 0; i &lt; 10; i++) { executorService.submit(new Runnable() { @Override public void run() { System.out.println(Thread.currentThread().getName()+&quot;输出了&quot;); } }); TimeUnit.SECONDS.sleep(2); } } 无时间间隔的提交 通过对比发现，newCachedThreadPool会在短时间内创建大量的线程。这也是他的弊端 newFixedThreadPool创建定长的线程池，最大的线程数量可以通过参数指定 ExecutorService executorService = Executors.newFixedThreadPool(5); 通过代码发现，与缓存线程池相比，定长线程池在短时间内不会创建大量的线程。 newScheduledThreadPool定时线程池，从源码角度看，他实际上也是一个普通的线程池，但是他的阻塞队列和别的线程池不同 ① public static ScheduledExecutorService newScheduledThreadPool(int corePoolSize) { return new ScheduledThreadPoolExecutor(corePoolSize); } ② public ScheduledThreadPoolExecutor(int corePoolSize) { super(corePoolSize, Integer.MAX_VALUE, 0, NANOSECONDS, new DelayedWorkQueue());} ③点击super public ThreadPoolExecutor(int corePoolSize, int maximumPoolSize, long keepAliveTime, TimeUnit unit, BlockingQueue&lt;Runnable&gt; workQueue) { this(corePoolSize, maximumPoolSize, keepAliveTime, unit, workQueue, Executors.defaultThreadFactory(), defaultHandler); } 来到了线程7板斧，后面的就不细说 那我们研究一下DelayWorkQueue 参考：https://blog.csdn.net/nobody_1/article/details/99684009 延迟N秒执行 public static void main(String[] args) { ScheduledExecutorService executorService = Executors.newScheduledThreadPool(5); executorService.schedule(new Runnable() { @Override public void run() { System.out.println(Thread.currentThread().getName() + &quot;输出了&quot;); } }, 5, TimeUnit.SECONDS); executorService.shutdown(); } 按照固定频率执行 public static void main(String[] args) { ScheduledExecutorService executorService = Executors.newScheduledThreadPool(5); executorService.scheduleAtFixedRate(new Runnable() { @Override public void run() { System.out.println(Thread.currentThread().getName() + &quot;输出了&quot;); } }, 5, 2,TimeUnit.SECONDS); } 延迟5秒，然后按照2秒1次执行 按照固定频率执行2 public static void main(String[] args) { ScheduledExecutorService executorService = Executors.newScheduledThreadPool(5); executorService.scheduleWithFixedDelay(new Runnable() { @Override public void run() { System.out.println(Thread.currentThread().getName() + &quot;输出了&quot;); } }, 5, 2,TimeUnit.SECONDS); } scheduleAtFixedRate与scheduleWithFixedDelay的区别是，延时时间计算的方式不同 scheduleAtFixedRate 是以任务开始的时间为时间起点来计时，时间到就执行第二次任务，与任务执行所花费的时间无关；而 scheduleWithFixedDelay 是以任务执行结束的时间点作为计时的开始。如下所示 图片来源：https://www.cnblogs.com/i-code/p/13917733.html newSingleThreadExecutor创建一个单一线程的线程池，保障fifo public static void main(String[] args) { ExecutorService executorService = Executors.newSingleThreadExecutor(); executorService.submit(new Runnable() { @Override public void run() { System.out.println(Thread.currentThread().getName() + &quot;输出了&quot;); } }); } 线程池的7大参数public ThreadPoolExecutor(int corePoolSize, int maximumPoolSize, long keepAliveTime, TimeUnit unit, BlockingQueue&lt;Runnable&gt; workQueue, ThreadFactory threadFactory, RejectedExecutionHandler handler) { if (corePoolSize &lt; 0 || maximumPoolSize &lt;= 0 || maximumPoolSize &lt; corePoolSize || keepAliveTime &lt; 0) throw new IllegalArgumentException(); if (workQueue == null || threadFactory == null || handler == null) throw new NullPointerException(); this.acc = System.getSecurityManager() == null ? null : AccessController.getContext(); this.corePoolSize = corePoolSize; this.maximumPoolSize = maximumPoolSize; this.workQueue = workQueue; this.keepAliveTime = unit.toNanos(keepAliveTime); this.threadFactory = threadFactory; this.handler = handler; } corePoolSize:核心线程数 maximumPoolSize：最大线程数 keepAliveTime：存活时间 keepAliveTime是线程池中空闲线程等待工作的超时时间。当线程池中线程数量大于corePoolSize（核心线程数量）或设置了allowCoreThreadTimeOut（是否允许空闲核心线程超时）时，线程会根据keepAliveTime的值进行活性检查，一旦超时便销毁线程。 unit：单位 workQueue：队列 threadFactory：线程工厂，在里面可以自定义线程名字 handler：拒绝策略 线程池的4种拒绝策略AbortPolicy：丢弃任务，抛出RejectedExecutionHandler异常 DiscardPolicy：丢弃，但不抛出异常 DiscardOldestPolicy：丢弃一个最老的 CallerRunsPolicy：在主线程开启一个线程，执行方法","link":"/threadPool/"},{"title":"java计算两个时间之间相差月份","text":"记录一下java计算两个时间相差的月份 public static Integer getDifMonth(Date startDate, Date endDate){ Calendar start = Calendar.getInstance(); Calendar end = Calendar.getInstance(); start.setTime(startDate); end.setTime(endDate); int result = end.get(Calendar.MONTH) - start.get(Calendar.MONTH); int month = (end.get(Calendar.YEAR) - start.get(Calendar.YEAR)) * 12; return Math.abs(month + result); }","link":"/java-utils-difftime/"},{"title":"笛卡尔积工具类","text":"public void descartes(List&lt;List&lt;JointRuleAlertDataModel&gt;&gt; dimensionValue, List&lt;List&lt;JointRuleAlertDataModel&gt;&gt; result, int layer, List&lt;JointRuleAlertDataModel&gt; currentList) { if (layer &lt; dimensionValue.size() - 1) { if (dimensionValue.get(layer).size() == 0) { descartes(dimensionValue, result, layer + 1, currentList); } else { for (int i = 0; i &lt; dimensionValue.get(layer).size(); i++) { List&lt;JointRuleAlertDataModel&gt; list = new ArrayList&lt;&gt;(currentList); list.add(dimensionValue.get(layer).get(i)); descartes(dimensionValue, result, layer + 1, list); } } } else if (layer == dimensionValue.size() - 1) { if (dimensionValue.get(layer).size() == 0) { result.add(currentList); } else { for (int i = 0; i &lt; dimensionValue.get(layer).size(); i++) { List&lt;JointRuleAlertDataModel&gt; list = new ArrayList&lt;&gt;(currentList); list.add(dimensionValue.get(layer).get(i)); result.add(list); } } } }","link":"/descartes/"},{"title":"java递归生成菜单","text":"记录java递归生成菜单 List&lt;Menu&gt;menuList=dao.selectMenu();List&lt;Menu&gt;newMenu=buildMenuTree(menuList,0);private List&lt;Menu&gt; buildMenuTree(List&lt;Menu&gt; menuList, Integer pid) { List&lt;Menu&gt; treeList = new ArrayList&lt;&gt;(); menuList.forEach(menu -&gt; { if (menu.getParentId().equals(pid)) { menu.setChildrens(buildMenuTree(menuList, menu.getId())); treeList.add(menu); } }); return treeList; }","link":"/java-utils-menu/"},{"title":"ReentrantLock","text":"先贴一下源码 public class ReentrantLock implements Lock, java.io.Serializable { private static final long serialVersionUID = 7373984872572414699L; /** Synchronizer providing all implementation mechanics */ private final Sync sync; /** * Base of synchronization control for this lock. Subclassed * into fair and nonfair versions below. Uses AQS state to * represent the number of holds on the lock. */ abstract static class Sync extends AbstractQueuedSynchronizer { private static final long serialVersionUID = -5179523762034025860L; /** * Performs {@link Lock#lock}. The main reason for subclassing * is to allow fast path for nonfair version. */ abstract void lock(); /** * Performs non-fair tryLock. tryAcquire is implemented in * subclasses, but both need nonfair try for trylock method. */ final boolean nonfairTryAcquire(int acquires) { final Thread current = Thread.currentThread(); int c = getState(); if (c == 0) { if (compareAndSetState(0, acquires)) { setExclusiveOwnerThread(current); return true; } } else if (current == getExclusiveOwnerThread()) { int nextc = c + acquires; if (nextc &lt; 0) // overflow throw new Error(&quot;Maximum lock count exceeded&quot;); setState(nextc); return true; } return false; } protected final boolean tryRelease(int releases) { int c = getState() - releases; if (Thread.currentThread() != getExclusiveOwnerThread()) throw new IllegalMonitorStateException(); boolean free = false; if (c == 0) { free = true; setExclusiveOwnerThread(null); } setState(c); return free; } protected final boolean isHeldExclusively() { // While we must in general read state before owner, // we don't need to do so to check if current thread is owner return getExclusiveOwnerThread() == Thread.currentThread(); } final ConditionObject newCondition() { return new ConditionObject(); } // Methods relayed from outer class final Thread getOwner() { return getState() == 0 ? null : getExclusiveOwnerThread(); } final int getHoldCount() { return isHeldExclusively() ? getState() : 0; } final boolean isLocked() { return getState() != 0; } /** * Reconstitutes the instance from a stream (that is, deserializes it). */ private void readObject(java.io.ObjectInputStream s) throws java.io.IOException, ClassNotFoundException { s.defaultReadObject(); setState(0); // reset to unlocked state } } /** * Sync object for non-fair locks */ static final class NonfairSync extends Sync { private static final long serialVersionUID = 7316153563782823691L; /** * Performs lock. Try immediate barge, backing up to normal * acquire on failure. */ final void lock() { if (compareAndSetState(0, 1)) setExclusiveOwnerThread(Thread.currentThread()); else acquire(1); } protected final boolean tryAcquire(int acquires) { return nonfairTryAcquire(acquires); } } /** * Sync object for fair locks */ static final class FairSync extends Sync { private static final long serialVersionUID = -3000897897090466540L; final void lock() { acquire(1); } /** * Fair version of tryAcquire. Don't grant access unless * recursive call or no waiters or is first. */ protected final boolean tryAcquire(int acquires) { final Thread current = Thread.currentThread(); int c = getState(); if (c == 0) { if (!hasQueuedPredecessors() &amp;&amp; compareAndSetState(0, acquires)) { setExclusiveOwnerThread(current); return true; } } else if (current == getExclusiveOwnerThread()) { int nextc = c + acquires; if (nextc &lt; 0) throw new Error(&quot;Maximum lock count exceeded&quot;); setState(nextc); return true; } return false; } } /** * Creates an instance of {@code ReentrantLock}. * This is equivalent to using {@code ReentrantLock(false)}. */ public ReentrantLock() { sync = new NonfairSync(); } /** * Creates an instance of {@code ReentrantLock} with the * given fairness policy. * * @param fair {@code true} if this lock should use a fair ordering policy */ public ReentrantLock(boolean fair) { sync = fair ? new FairSync() : new NonfairSync(); } /** * Acquires the lock. * * &lt;p&gt;Acquires the lock if it is not held by another thread and returns * immediately, setting the lock hold count to one. * * &lt;p&gt;If the current thread already holds the lock then the hold * count is incremented by one and the method returns immediately. * * &lt;p&gt;If the lock is held by another thread then the * current thread becomes disabled for thread scheduling * purposes and lies dormant until the lock has been acquired, * at which time the lock hold count is set to one. */ public void lock() { sync.lock(); } /** * Acquires the lock unless the current thread is * {@linkplain Thread#interrupt interrupted}. * * &lt;p&gt;Acquires the lock if it is not held by another thread and returns * immediately, setting the lock hold count to one. * * &lt;p&gt;If the current thread already holds this lock then the hold count * is incremented by one and the method returns immediately. * * &lt;p&gt;If the lock is held by another thread then the * current thread becomes disabled for thread scheduling * purposes and lies dormant until one of two things happens: * * &lt;ul&gt; * * &lt;li&gt;The lock is acquired by the current thread; or * * &lt;li&gt;Some other thread {@linkplain Thread#interrupt interrupts} the * current thread. * * &lt;/ul&gt; * * &lt;p&gt;If the lock is acquired by the current thread then the lock hold * count is set to one. * * &lt;p&gt;If the current thread: * * &lt;ul&gt; * * &lt;li&gt;has its interrupted status set on entry to this method; or * * &lt;li&gt;is {@linkplain Thread#interrupt interrupted} while acquiring * the lock, * * &lt;/ul&gt; * * then {@link InterruptedException} is thrown and the current thread's * interrupted status is cleared. * * &lt;p&gt;In this implementation, as this method is an explicit * interruption point, preference is given to responding to the * interrupt over normal or reentrant acquisition of the lock. * * @throws InterruptedException if the current thread is interrupted */ public void lockInterruptibly() throws InterruptedException { sync.acquireInterruptibly(1); } /** * Acquires the lock only if it is not held by another thread at the time * of invocation. * * &lt;p&gt;Acquires the lock if it is not held by another thread and * returns immediately with the value {@code true}, setting the * lock hold count to one. Even when this lock has been set to use a * fair ordering policy, a call to {@code tryLock()} &lt;em&gt;will&lt;/em&gt; * immediately acquire the lock if it is available, whether or not * other threads are currently waiting for the lock. * This &amp;quot;barging&amp;quot; behavior can be useful in certain * circumstances, even though it breaks fairness. If you want to honor * the fairness setting for this lock, then use * {@link #tryLock(long, TimeUnit) tryLock(0, TimeUnit.SECONDS) } * which is almost equivalent (it also detects interruption). * * &lt;p&gt;If the current thread already holds this lock then the hold * count is incremented by one and the method returns {@code true}. * * &lt;p&gt;If the lock is held by another thread then this method will return * immediately with the value {@code false}. * * @return {@code true} if the lock was free and was acquired by the * current thread, or the lock was already held by the current * thread; and {@code false} otherwise */ public boolean tryLock() { return sync.nonfairTryAcquire(1); } /** * Acquires the lock if it is not held by another thread within the given * waiting time and the current thread has not been * {@linkplain Thread#interrupt interrupted}. * * &lt;p&gt;Acquires the lock if it is not held by another thread and returns * immediately with the value {@code true}, setting the lock hold count * to one. If this lock has been set to use a fair ordering policy then * an available lock &lt;em&gt;will not&lt;/em&gt; be acquired if any other threads * are waiting for the lock. This is in contrast to the {@link #tryLock()} * method. If you want a timed {@code tryLock} that does permit barging on * a fair lock then combine the timed and un-timed forms together: * * &lt;pre&gt; {@code * if (lock.tryLock() || * lock.tryLock(timeout, unit)) { * ... * }}&lt;/pre&gt; * * &lt;p&gt;If the current thread * already holds this lock then the hold count is incremented by one and * the method returns {@code true}. * * &lt;p&gt;If the lock is held by another thread then the * current thread becomes disabled for thread scheduling * purposes and lies dormant until one of three things happens: * * &lt;ul&gt; * * &lt;li&gt;The lock is acquired by the current thread; or * * &lt;li&gt;Some other thread {@linkplain Thread#interrupt interrupts} * the current thread; or * * &lt;li&gt;The specified waiting time elapses * * &lt;/ul&gt; * * &lt;p&gt;If the lock is acquired then the value {@code true} is returned and * the lock hold count is set to one. * * &lt;p&gt;If the current thread: * * &lt;ul&gt; * * &lt;li&gt;has its interrupted status set on entry to this method; or * * &lt;li&gt;is {@linkplain Thread#interrupt interrupted} while * acquiring the lock, * * &lt;/ul&gt; * then {@link InterruptedException} is thrown and the current thread's * interrupted status is cleared. * * &lt;p&gt;If the specified waiting time elapses then the value {@code false} * is returned. If the time is less than or equal to zero, the method * will not wait at all. * * &lt;p&gt;In this implementation, as this method is an explicit * interruption point, preference is given to responding to the * interrupt over normal or reentrant acquisition of the lock, and * over reporting the elapse of the waiting time. * * @param timeout the time to wait for the lock * @param unit the time unit of the timeout argument * @return {@code true} if the lock was free and was acquired by the * current thread, or the lock was already held by the current * thread; and {@code false} if the waiting time elapsed before * the lock could be acquired * @throws InterruptedException if the current thread is interrupted * @throws NullPointerException if the time unit is null */ public boolean tryLock(long timeout, TimeUnit unit) throws InterruptedException { return sync.tryAcquireNanos(1, unit.toNanos(timeout)); } /** * Attempts to release this lock. * * &lt;p&gt;If the current thread is the holder of this lock then the hold * count is decremented. If the hold count is now zero then the lock * is released. If the current thread is not the holder of this * lock then {@link IllegalMonitorStateException} is thrown. * * @throws IllegalMonitorStateException if the current thread does not * hold this lock */ public void unlock() { sync.release(1); } /** * Returns a {@link Condition} instance for use with this * {@link Lock} instance. * * &lt;p&gt;The returned {@link Condition} instance supports the same * usages as do the {@link Object} monitor methods ({@link * Object#wait() wait}, {@link Object#notify notify}, and {@link * Object#notifyAll notifyAll}) when used with the built-in * monitor lock. * * &lt;ul&gt; * * &lt;li&gt;If this lock is not held when any of the {@link Condition} * {@linkplain Condition#await() waiting} or {@linkplain * Condition#signal signalling} methods are called, then an {@link * IllegalMonitorStateException} is thrown. * * &lt;li&gt;When the condition {@linkplain Condition#await() waiting} * methods are called the lock is released and, before they * return, the lock is reacquired and the lock hold count restored * to what it was when the method was called. * * &lt;li&gt;If a thread is {@linkplain Thread#interrupt interrupted} * while waiting then the wait will terminate, an {@link * InterruptedException} will be thrown, and the thread's * interrupted status will be cleared. * * &lt;li&gt; Waiting threads are signalled in FIFO order. * * &lt;li&gt;The ordering of lock reacquisition for threads returning * from waiting methods is the same as for threads initially * acquiring the lock, which is in the default case not specified, * but for &lt;em&gt;fair&lt;/em&gt; locks favors those threads that have been * waiting the longest. * * &lt;/ul&gt; * * @return the Condition object */ public Condition newCondition() { return sync.newCondition(); } /** * Queries the number of holds on this lock by the current thread. * * &lt;p&gt;A thread has a hold on a lock for each lock action that is not * matched by an unlock action. * * &lt;p&gt;The hold count information is typically only used for testing and * debugging purposes. For example, if a certain section of code should * not be entered with the lock already held then we can assert that * fact: * * &lt;pre&gt; {@code * class X { * ReentrantLock lock = new ReentrantLock(); * // ... * public void m() { * assert lock.getHoldCount() == 0; * lock.lock(); * try { * // ... method body * } finally { * lock.unlock(); * } * } * }}&lt;/pre&gt; * * @return the number of holds on this lock by the current thread, * or zero if this lock is not held by the current thread */ public int getHoldCount() { return sync.getHoldCount(); } /** * Queries if this lock is held by the current thread. * * &lt;p&gt;Analogous to the {@link Thread#holdsLock(Object)} method for * built-in monitor locks, this method is typically used for * debugging and testing. For example, a method that should only be * called while a lock is held can assert that this is the case: * * &lt;pre&gt; {@code * class X { * ReentrantLock lock = new ReentrantLock(); * // ... * * public void m() { * assert lock.isHeldByCurrentThread(); * // ... method body * } * }}&lt;/pre&gt; * * &lt;p&gt;It can also be used to ensure that a reentrant lock is used * in a non-reentrant manner, for example: * * &lt;pre&gt; {@code * class X { * ReentrantLock lock = new ReentrantLock(); * // ... * * public void m() { * assert !lock.isHeldByCurrentThread(); * lock.lock(); * try { * // ... method body * } finally { * lock.unlock(); * } * } * }}&lt;/pre&gt; * * @return {@code true} if current thread holds this lock and * {@code false} otherwise */ public boolean isHeldByCurrentThread() { return sync.isHeldExclusively(); } /** * Queries if this lock is held by any thread. This method is * designed for use in monitoring of the system state, * not for synchronization control. * * @return {@code true} if any thread holds this lock and * {@code false} otherwise */ public boolean isLocked() { return sync.isLocked(); } /** * Returns {@code true} if this lock has fairness set true. * * @return {@code true} if this lock has fairness set true */ public final boolean isFair() { return sync instanceof FairSync; } /** * Returns the thread that currently owns this lock, or * {@code null} if not owned. When this method is called by a * thread that is not the owner, the return value reflects a * best-effort approximation of current lock status. For example, * the owner may be momentarily {@code null} even if there are * threads trying to acquire the lock but have not yet done so. * This method is designed to facilitate construction of * subclasses that provide more extensive lock monitoring * facilities. * * @return the owner, or {@code null} if not owned */ protected Thread getOwner() { return sync.getOwner(); } /** * Queries whether any threads are waiting to acquire this lock. Note that * because cancellations may occur at any time, a {@code true} * return does not guarantee that any other thread will ever * acquire this lock. This method is designed primarily for use in * monitoring of the system state. * * @return {@code true} if there may be other threads waiting to * acquire the lock */ public final boolean hasQueuedThreads() { return sync.hasQueuedThreads(); } /** * Queries whether the given thread is waiting to acquire this * lock. Note that because cancellations may occur at any time, a * {@code true} return does not guarantee that this thread * will ever acquire this lock. This method is designed primarily for use * in monitoring of the system state. * * @param thread the thread * @return {@code true} if the given thread is queued waiting for this lock * @throws NullPointerException if the thread is null */ public final boolean hasQueuedThread(Thread thread) { return sync.isQueued(thread); } /** * Returns an estimate of the number of threads waiting to * acquire this lock. The value is only an estimate because the number of * threads may change dynamically while this method traverses * internal data structures. This method is designed for use in * monitoring of the system state, not for synchronization * control. * * @return the estimated number of threads waiting for this lock */ public final int getQueueLength() { return sync.getQueueLength(); } /** * Returns a collection containing threads that may be waiting to * acquire this lock. Because the actual set of threads may change * dynamically while constructing this result, the returned * collection is only a best-effort estimate. The elements of the * returned collection are in no particular order. This method is * designed to facilitate construction of subclasses that provide * more extensive monitoring facilities. * * @return the collection of threads */ protected Collection&lt;Thread&gt; getQueuedThreads() { return sync.getQueuedThreads(); } /** * Queries whether any threads are waiting on the given condition * associated with this lock. Note that because timeouts and * interrupts may occur at any time, a {@code true} return does * not guarantee that a future {@code signal} will awaken any * threads. This method is designed primarily for use in * monitoring of the system state. * * @param condition the condition * @return {@code true} if there are any waiting threads * @throws IllegalMonitorStateException if this lock is not held * @throws IllegalArgumentException if the given condition is * not associated with this lock * @throws NullPointerException if the condition is null */ public boolean hasWaiters(Condition condition) { if (condition == null) throw new NullPointerException(); if (!(condition instanceof AbstractQueuedSynchronizer.ConditionObject)) throw new IllegalArgumentException(&quot;not owner&quot;); return sync.hasWaiters((AbstractQueuedSynchronizer.ConditionObject)condition); } /** * Returns an estimate of the number of threads waiting on the * given condition associated with this lock. Note that because * timeouts and interrupts may occur at any time, the estimate * serves only as an upper bound on the actual number of waiters. * This method is designed for use in monitoring of the system * state, not for synchronization control. * * @param condition the condition * @return the estimated number of waiting threads * @throws IllegalMonitorStateException if this lock is not held * @throws IllegalArgumentException if the given condition is * not associated with this lock * @throws NullPointerException if the condition is null */ public int getWaitQueueLength(Condition condition) { if (condition == null) throw new NullPointerException(); if (!(condition instanceof AbstractQueuedSynchronizer.ConditionObject)) throw new IllegalArgumentException(&quot;not owner&quot;); return sync.getWaitQueueLength((AbstractQueuedSynchronizer.ConditionObject)condition); } /** * Returns a collection containing those threads that may be * waiting on the given condition associated with this lock. * Because the actual set of threads may change dynamically while * constructing this result, the returned collection is only a * best-effort estimate. The elements of the returned collection * are in no particular order. This method is designed to * facilitate construction of subclasses that provide more * extensive condition monitoring facilities. * * @param condition the condition * @return the collection of threads * @throws IllegalMonitorStateException if this lock is not held * @throws IllegalArgumentException if the given condition is * not associated with this lock * @throws NullPointerException if the condition is null */ protected Collection&lt;Thread&gt; getWaitingThreads(Condition condition) { if (condition == null) throw new NullPointerException(); if (!(condition instanceof AbstractQueuedSynchronizer.ConditionObject)) throw new IllegalArgumentException(&quot;not owner&quot;); return sync.getWaitingThreads((AbstractQueuedSynchronizer.ConditionObject)condition); } /** * Returns a string identifying this lock, as well as its lock state. * The state, in brackets, includes either the String {@code &quot;Unlocked&quot;} * or the String {@code &quot;Locked by&quot;} followed by the * {@linkplain Thread#getName name} of the owning thread. * * @return a string identifying this lock, as well as its lock state */ public String toString() { Thread o = sync.getOwner(); return super.toString() + ((o == null) ? &quot;[Unlocked]&quot; : &quot;[Locked by thread &quot; + o.getName() + &quot;]&quot;); }} 贴一下结构图，然后逐一分析","link":"/java-sourcecode-reentrantlock/"},{"title":"hashMap","text":"抽时间静下心来读一下hashMap的源码，从大佬前辈们的代码中获取灵感，充实自己。gogogo HashMap 初始化通常我们用hashMap的时候会new一个hashMap的对象，那new的过程中发生了什么呢 // 初始化 HashMap&lt;String, String&gt; stringStringHashMap = new HashMap&lt;&gt;(); 走起，点击进来 /** * Constructs an empty &lt;tt&gt;HashMap&lt;/tt&gt; with the default initial capacity * (16) and the default load factor (0.75). */ public HashMap() { this.loadFactor = DEFAULT_LOAD_FACTOR; // all other fields defaulted } 如上所示，HashMap的无参构造方法就只有一行，默认加载因子为0.75，此处没有指定默认的容量，我们往上翻常量 /** * The default initial capacity - MUST be a power of two. */ static final int DEFAULT_INITIAL_CAPACITY = 1 &lt;&lt; 4; // aka 16 /** * The maximum capacity, used if a higher value is implicitly specified * by either of the constructors with arguments. * MUST be a power of two &lt;= 1&lt;&lt;30. */ static final int MAXIMUM_CAPACITY = 1 &lt;&lt; 30; /** * The load factor used when none specified in constructor. */ static final float DEFAULT_LOAD_FACTOR = 0.75f; /** * The bin count threshold for using a tree rather than list for a * bin. Bins are converted to trees when adding an element to a * bin with at least this many nodes. The value must be greater * than 2 and should be at least 8 to mesh with assumptions in * tree removal about conversion back to plain bins upon * shrinkage. */ static final int TREEIFY_THRESHOLD = 8; /** * The bin count threshold for untreeifying a (split) bin during a * resize operation. Should be less than TREEIFY_THRESHOLD, and at * most 6 to mesh with shrinkage detection under removal. */ static final int UNTREEIFY_THRESHOLD = 6; /** * The smallest table capacity for which bins may be treeified. * (Otherwise the table is resized if too many nodes in a bin.) * Should be at least 4 * TREEIFY_THRESHOLD to avoid conflicts * between resizing and treeification thresholds. */ static final int MIN_TREEIFY_CAPACITY = 64; 可以看到 默认初始容量为16 ，再理解一下其他参数 // 最大容量 static final int MAXIMUM_CAPACITY = 1 &lt;&lt; 30;// 默认加载因子static final float DEFAULT_LOAD_FACTOR = 0.75f;// 链表转为红黑树的临界值static final int TREEIFY_THRESHOLD = 8;// 红黑树再次转为链表的临界值static final int UNTREEIFY_THRESHOLD = 6;// 链表转换为红黑树的最小数组的长度，如果不够这个长度则扩容 static final int MIN_TREEIFY_CAPACITY = 64; 从上面 我们再引出一个面试中常问的问题 HashMap的默认初始化加载因子的作用//默认加载因子是threadhold 的决定因素之一，而threadhold是HashMap的table 进行扩容的临界点，举个简单的例子,如果table 初始容量大小为是16，加载因子为0.75，则当数组长度为16*0.75=12的时候，然后下一个put的时候将进行扩容操作。关键代码如下：662-663 HashMap默认加载因子为什么是0.75加载因子是表示Hsah表中元素的填满的程度。 加载因子越大,填满的元素越多,空间利用率越高，但冲突的机会加大了。 反之,加载因子越小,填满的元素越少,冲突的机会减小,但空间浪费多了。 冲突的机会越大,则查找的成本越高。反之,查找的成本越小。 因此,必须在 “冲突的机会”与”空间利用率”之间寻找一种平衡与折衷。 /* Because TreeNodes are about twice the size of regular nodes, we * use them only when bins contain enough nodes to warrant use * (see TREEIFY_THRESHOLD). And when they become too small (due to * removal or resizing) they are converted back to plain bins. In * usages with well-distributed user hashCodes, tree bins are * rarely used. Ideally, under random hashCodes, the frequency of * nodes in bins follows a Poisson distribution * (http://en.wikipedia.org/wiki/Poisson_distribution) with a * parameter of about 0.5 on average for the default resizing * threshold of 0.75, although with a large variance because of * resizing granularity. Ignoring variance, the expected * occurrences of list size k are (exp(-0.5) * pow(0.5, k) / * factorial(k)). The first values are: * * 0: 0.60653066 * 1: 0.30326533 * 2: 0.07581633 * 3: 0.01263606 * 4: 0.00157952 * 5: 0.00015795 * 6: 0.00001316 * 7: 0.00000094 * 8: 0.00000006 * more: less than 1 in ten million * 简单翻译一下就是在理想情况下,使用随机哈希码,节点出现的频率在hash桶中遵循泊松分布，同时给出了桶中元素个数和概率的对照表。 从上面的表中可以看到当桶中元素到达8个的时候，概率已经变得非常小，也就是说用0.75作为加载因子，每个碰撞位置的链表长度超过８个是几乎不可能的。 HashMap中hash函数的实现过程贴一段String中hash函数实现 public int hashCode() { int h = hash; if (h == 0 &amp;&amp; value.length &gt; 0) { char val[] = value; for (int i = 0; i &lt; value.length; i++) { h = 31 * h + val[i]; } hash = h; } return h; } 将string 转为char数组，取元素在ASCII码中的位置相加，然后依次进行遍历加和（描述不是很准确，看代码吧） 我们在看一下hashMap中的函数，还是以放入的key为String类型为例。代码如下 static final int hash(Object key) { int h; return (key == null) ? 0 : (h = key.hashCode()) ^ (h &gt;&gt;&gt; 16);} 在原有的基础上，对重新对hash进行优化，得出的hashCode 的低16位与高16位进行==异或== 异或 异域的概念是相同为0不同为1.如果两个数值异或后的值相同，异或前可能不同。比如二进制：0010^0001=0011 而0000^0011=0011。 异或的注意点 1.自己与自己异或结果为0， 2.异或满足 交换律。a^b=b^a，即异或满足交换律。 hashMap 中获取hash值异或的好处是什么 保证低16位与高16位只要有一位进行变动，就会对hash值产生影响，减少了hash冲突的可能性。 hashMap中put方法分析当我们输入如下代码，跟着断点看hashMap到底进行了如何操作 HashMap&lt;String, String&gt; stringStringHashMap = new HashMap&lt;&gt;();stringStringHashMap.put(&quot;1&quot;,&quot;1&quot;); 一步一步走 public V put(K key, V value) { return putVal(hash(key), key, value, false, true); }// 此处计算key的hash值，上面已经分析过了，不再过多赘述 看putVal方法 final V putVal(int hash, K key, V value, boolean onlyIfAbsent, boolean evict) { Node&lt;K,V&gt;[] tab; Node&lt;K,V&gt; p; int n, i; if ((tab = table) == null || (n = tab.length) == 0) // 第一次放入元素的时候，就会进入此分支，进行resize操作。 n = (tab = resize()).length; if ((p = tab[i = (n - 1) &amp; hash]) == null) // 判断 （当前容器的大小为 -1）与 hashCode相与，判断当前位置是否有元素。 // 引出一个知识点，为什么hashMap的容量一定要设置成2的倍数。 tab[i] = newNode(hash, key, value, null); else { Node&lt;K,V&gt; e; K k; if (p.hash == hash &amp;&amp; ((k = p.key) == key || (key != null &amp;&amp; key.equals(k)))) e = p; else if (p instanceof TreeNode) e = ((TreeNode&lt;K,V&gt;)p).putTreeVal(this, tab, hash, key, value); else { for (int binCount = 0; ; ++binCount) { if ((e = p.next) == null) { p.next = newNode(hash, key, value, null); if (binCount &gt;= TREEIFY_THRESHOLD - 1) // -1 for 1st treeifyBin(tab, hash); break; } if (e.hash == hash &amp;&amp; ((k = e.key) == key || (key != null &amp;&amp; key.equals(k)))) break; p = e; } } if (e != null) { // existing mapping for key V oldValue = e.value; if (!onlyIfAbsent || oldValue == null) e.value = value; afterNodeAccess(e); return oldValue; } } ++modCount; if (++size &gt; threshold) resize(); afterNodeInsertion(evict); return null;} 当我们放入第一个元素时， 因为table为null，所以走第一个if分支，进行resize()操作进行扩容。 final Node&lt;K,V&gt;[] resize() { Node&lt;K,V&gt;[] oldTab = table; // 第一次放入 oldCap为0 int oldCap = (oldTab == null) ? 0 : oldTab.length; // oldThr 也为0； int oldThr = threshold; int newCap, newThr = 0; if (oldCap &gt; 0) { if (oldCap &gt;= MAXIMUM_CAPACITY) { threshold = Integer.MAX_VALUE; return oldTab; } else if ((newCap = oldCap &lt;&lt; 1) &lt; MAXIMUM_CAPACITY &amp;&amp; oldCap &gt;= DEFAULT_INITIAL_CAPACITY) newThr = oldThr &lt;&lt; 1; // double threshold } else if (oldThr &gt; 0) // initial capacity was placed in threshold newCap = oldThr; else { // zero initial threshold signifies using defaults // 第一次放入走else 这个分支，将容量和扩容临界值变为默认值 16 和12 newCap = DEFAULT_INITIAL_CAPACITY; newThr = (int)(DEFAULT_LOAD_FACTOR * DEFAULT_INITIAL_CAPACITY); } if (newThr == 0) { float ft = (float)newCap * loadFactor; newThr = (newCap &lt; MAXIMUM_CAPACITY &amp;&amp; ft &lt; (float)MAXIMUM_CAPACITY ? (int)ft : Integer.MAX_VALUE); } threshold = newThr; @SuppressWarnings({&quot;rawtypes&quot;,&quot;unchecked&quot;}) // 第一次初始化 newCap为16，此处创建一个新的数组结构 Node&lt;K,V&gt;[] newTab = (Node&lt;K,V&gt;[])new Node[newCap]; table = newTab; // 第一次初始化oldTab为null，此时直接返回newTable if (oldTab != null) { for (int j = 0; j &lt; oldCap; ++j) { Node&lt;K,V&gt; e; if ((e = oldTab[j]) != null) { oldTab[j] = null; if (e.next == null) newTab[e.hash &amp; (newCap - 1)] = e; else if (e instanceof TreeNode) ((TreeNode&lt;K,V&gt;)e).split(this, newTab, j, oldCap); else { // preserve order Node&lt;K,V&gt; loHead = null, loTail = null; Node&lt;K,V&gt; hiHead = null, hiTail = null; Node&lt;K,V&gt; next; do { next = e.next; if ((e.hash &amp; oldCap) == 0) { if (loTail == null) loHead = e; else loTail.next = e; loTail = e; } else { if (hiTail == null) hiHead = e; else hiTail.next = e; hiTail = e; } } while ((e = next) != null); if (loTail != null) { loTail.next = null; newTab[j] = loHead; } if (hiTail != null) { hiTail.next = null; newTab[j + oldCap] = hiHead; } } } } } return newTab; } 紧接着我们放入第二个元素,还是看putVal方法 final V putVal(int hash, K key, V value, boolean onlyIfAbsent, boolean evict) { Node&lt;K,V&gt;[] tab; Node&lt;K,V&gt; p; int n, i; // 此时不为空，则不走这个if if ((tab = table) == null || (n = tab.length) == 0) n = (tab = resize()).length; // 如果不同，则 tab【i】的位置放入一个新的node节点 if ((p = tab[i = (n - 1) &amp; hash]) == null) tab[i] = newNode(hash, key, value, null); else { // 如果新放入的key与之前某个位置的key产生了冲突 Node&lt;K,V&gt; e; K k; // 判断当前的hash值是否和心如的一样，并且key是否和key一样，如果一样则认为是相同的，将p赋值给e if (p.hash == hash &amp;&amp; ((k = p.key) == key || (key != null &amp;&amp; key.equals(k)))) e = p; else if (p instanceof TreeNode) // 否则，查看这个p节点是否是红黑树的类型 e = ((TreeNode&lt;K,V&gt;)p).putTreeVal(this, tab, hash, key, value); else { // 如果不是红黑树，则插入 for (int binCount = 0; ; ++binCount) { // 此处可以看出，下个节点放在p的后面，也就是尾插法 if ((e = p.next) == null) { p.next = newNode(hash, key, value, null); // 如果当前节点的数量大于等于要树话的临界值 if (binCount &gt;= TREEIFY_THRESHOLD - 1) // -1 for 1st treeifyBin(tab, hash); break; } // 同样去判断是否相等 if (e.hash == hash &amp;&amp; ((k = e.key) == key || (key != null &amp;&amp; key.equals(k)))) break; // 将e 赋值给p p = e; } } if (e != null) { // existing mapping for key V oldValue = e.value; if (!onlyIfAbsent || oldValue == null) e.value = value; afterNodeAccess(e); return oldValue; } } ++modCount; if (++size &gt; threshold) resize(); afterNodeInsertion(evict); return null; } hashMap的容量为什么一定要设置成2的倍数 我们可以看到新元素的位置是由（n-1）&amp;hash码确定的 比如n为15（非2的倍数）减去1 为14 在进行与运算的时候，1110 &amp; hash hash 如果是 1110 和1111 计算出来的结果是一样的，这样就增大了hash冲突的概率，所以不提倡。如果n是2的倍数则可以很好的避免这种情况。 hashMap 链表转换为红黑树的实现final void treeifyBin(Node&lt;K,V&gt;[] tab, int hash) { int n, index; Node&lt;K,V&gt; e; // 如果tab是null 或者tab数组的长度小于最小树化的时候，这时候不是转换成红黑树，而是扩容 if (tab == null || (n = tab.length) &lt; MIN_TREEIFY_CAPACITY) resize(); else if ((e = tab[index = (n - 1) &amp; hash]) != null) { TreeNode&lt;K,V&gt; hd = null, tl = null; do { // replacementTreeNode的代码我也贴一下，实际上就是返回一个trueNode节点 TreeNode&lt;K,V&gt; p = replacementTreeNode(e, null); if (tl == null) hd = p; else { p.prev = tl; tl.next = p; } tl = p; } while ((e = e.next) != null); if ((tab[index] = hd) != null) hd.treeify(tab); } } // For treeifyBin TreeNode&lt;K,V&gt; replacementTreeNode(Node&lt;K,V&gt; p, Node&lt;K,V&gt; next) { return new TreeNode&lt;&gt;(p.hash, p.key, p.value, next); } hashMap 的get方法实现 public V get(Object key) { Node&lt;K,V&gt; e; return (e = getNode(hash(key), key)) == null ? null : e.value; }final Node&lt;K,V&gt; getNode(int hash, Object key) { Node&lt;K,V&gt;[] tab; Node&lt;K,V&gt; first, e; int n; K k; // 如果是数组是空，直接返回null，如果不是则进行如下逻辑 if ((tab = table) != null &amp;&amp; (n = tab.length) &gt; 0 &amp;&amp; (first = tab[(n - 1) &amp; hash]) != null) { // 如果是首个节点的hash 和 key都是相等的，则返回首个节点 if (first.hash == hash &amp;&amp; // always check first node ((k = first.key) == key || (key != null &amp;&amp; key.equals(k)))) return first; if ((e = first.next) != null) { // 如果不是，并且节点是treeNode类型，则根据红黑树去找 if (first instanceof TreeNode) return ((TreeNode&lt;K,V&gt;)first).getTreeNode(hash, key); do { // 否则遍历链表 if (e.hash == hash &amp;&amp; ((k = e.key) == key || (key != null &amp;&amp; key.equals(k)))) return e; } while ((e = e.next) != null); } } return null; } hashMap 的remove方法实现public V remove(Object key) { Node&lt;K,V&gt; e; return (e = removeNode(hash(key), key, null, false, true)) == null ? null : e.value; } final Node&lt;K,V&gt; removeNode(int hash, Object key, Object value, boolean matchValue, boolean movable) { Node&lt;K,V&gt;[] tab; Node&lt;K,V&gt; p; int n, index; if ((tab = table) != null &amp;&amp; (n = tab.length) &gt; 0 &amp;&amp; (p = tab[index = (n - 1) &amp; hash]) != null) { Node&lt;K,V&gt; node = null, e; K k; V v; //寻找节点 if (p.hash == hash &amp;&amp; ((k = p.key) == key || (key != null &amp;&amp; key.equals(k)))) node = p; else if ((e = p.next) != null) { if (p instanceof TreeNode) node = ((TreeNode&lt;K,V&gt;)p).getTreeNode(hash, key); else { do { if (e.hash == hash &amp;&amp; ((k = e.key) == key || (key != null &amp;&amp; key.equals(k)))) { node = e; break; } p = e; } while ((e = e.next) != null); } } //找到之后断链 if (node != null &amp;&amp; (!matchValue || (v = node.value) == value || (value != null &amp;&amp; value.equals(v)))) { if (node instanceof TreeNode) ((TreeNode&lt;K,V&gt;)node).removeTreeNode(this, tab, movable); else if (node == p) tab[index] = node.next; else p.next = node.next; ++modCount; --size; afterNodeRemoval(node); return node; } } return null; } –未完待续–hashMap树化和非树化的方法","link":"/hashmap/"},{"title":"布隆过滤器","text":"今天学习一下布隆过滤器的知识点，之前没有形成统一的笔记，今天特此记录一下。 布隆过滤器是什么布隆过滤器（英语：Bloom Filter）是1970年由布隆提出的。它实际上是一个很长的二进制向量和一系列随机映射函数。布隆过滤器可以用于检索一个元素是否在一个集合中。它的优点是空间效率和查询时间都远远超过一般的算法，缺点是有一定的误识别率和删除困难 如果想判断一个元素是不是在一个集合里，一般想到的是将集合中所有元素保存起来，然后通过比较确定。链表、树、散列表（又叫哈希表，Hash table）等等数据结构都是这种思路。但是随着集合中元素的增加，我们需要的存储空间越来越大。同时检索速度也越来越慢，上述三种结构的检索时间复杂度分别为 O(n),O(\\log n),O(1) 布隆过滤器的原理是，当一个元素被加入集合时，通过K个散列函数将这个元素映射成一个位数组中的K个点，把它们置为1。检索时，我们只要看看这些点是不是都是1就（大约）知道集合中有没有它了：如果这些点有任何一个0，则被检元素一定不在；如果都是1，则被检元素很可能在。这就是布隆过滤器的基本思想。 优点相比于其它的数据结构，布隆过滤器在空间和时间方面都有巨大的优势。布隆过滤器存储空间和插入/查询时间都是常数O(K)。另外，散列函数相互之间没有关系，方便由硬件并行实现。布隆过滤器不需要存储元素本身，在某些对保密要求非常严格的场合有优势。 布隆过滤器可以表示全集，其它任何数据结构都不能；k和m相同，使用同一组散列函数的两个布隆过滤器的交并[来源请求]运算可以使用位操作进行。 缺点但是布隆过滤器的缺点和优点一样明显。误算率是其中之一。随着存入的元素数量增加，误算率随之增加。但是如果元素数量太少，则使用散列表足矣。 另外，一般情况下不能从布隆过滤器中删除元素。我们很容易想到把位数组变成整数数组，每插入一个元素相应的计数器加1, 这样删除元素时将计数器减掉就可以了。然而要保证安全地删除元素并非如此简单。首先我们必须保证删除的元素的确在布隆过滤器里面。这一点单凭这个过滤器是无法保证的。另外计数器回绕也会造成问题。 在降低误算率方面，有不少工作，使得出现了很多布隆过滤器的变种。 布隆过滤器公式布隆过滤器的误报概率假设数组长度为m,并使用k个hash函数，n是要插入到过滤器中的元素个数，那么误报率的计算如下： 位数组的大小 如果过滤器中的元素数量已知，期望的误报率位p，那么二进制位数组大小计算公式如下： 最优哈希函数数量如果m是数组长度，n是插入的元素个数，k是hash函数的个数，k计算公式如下： 哈希函数的选择哈希函数独立且生成的数值均匀分布，并且尽可能的快。比如MurmurHash、Jenkins_hash_function。 java集成布隆过滤器&lt;dependency&gt; &lt;groupId&gt;com.google.guava&lt;/groupId&gt; &lt;artifactId&gt;guava&lt;/artifactId&gt; &lt;version&gt;31.1-jre&lt;/version&gt; &lt;/dependency&gt; 咱们采用guava工具包中的bloom过滤器 创建布隆过滤器 BloomFilter&lt;Integer&gt; bloomFilter=BloomFilter.create(Funnels.integerFunnel(), 1_000_000,0.01); 创建布隆过滤器时要传入期望处理的元素数量，及最期望的误报的概率。如上分别是1_000_000和0.01。 放置元素 bloomFilter.put(1);bloomFilter.put(2); 判断元素是否存在","link":"/bloom-filter/"},{"title":"Kafka的安装与使用","text":"最近，因为工作原因，要去学习kafka，利用闲余时间搭建了一个kafka的单机demo，现在记录一下。 什么是KafkaKafka是由Apache软件基金会开发的一个开源流处理平台，由Scala和Java编写。Kafka是一种高吞吐量的分布式发布订阅消息系统，它可以处理消费者在网站中的所有动作流数据。 它主要结合了三个功能 To publish (write) and subscribe to (read) streams of events, including continuous import/export of your data from other systems. 发布订阅事件流，包括不断的从其他系统中导入或者导出数据 To store streams of events durably and reliably for as long as you want. 持久可靠的存储事件流，存储时间由你去决定 To process streams of events as they occur or retrospectively. 处理事件流的发生或者追溯 Kafka中的术语和概念event：An event records the fact that “something happened” in the world or in your business. It is also called record or message in the documentation. When you read or write data to Kafka, you do this in the form of events. Conceptually, an event has a key, value, timestamp, and optional metadata headers. Here’s an example event: Event key: “Alice” Event value: “Made a payment of $200 to Bob” Event timestamp: “Jun. 25, 2020 at 2:06 p.m.” 一个事件记录着一个事实，即业务中发生过什么。通常在文档中，也被成为message或者记录。当你在kafka中度或者写数据的时候，你会按照这个事件的格式去做。通常来讲，一个事件通常有一个key，value，时间戳和元数据的操作头。 Producers：生产者 consumers：消费者 topic：Events are organized and durably stored in topics. Very simplified, a topic is similar to a folder in a filesystem, and the events are the files in that folder. An example topic name could be “payments”. Topics in Kafka are always multi-producer and multi-subscriber: a topic can have zero, one, or many producers that write events to it, as well as zero, one, or many consumers that subscribe to these events. Events in a topic can be read as often as needed—unlike traditional messaging systems, events are not deleted after consumption. Instead, you define for how long Kafka should retain your events through a per-topic configuration setting, after which old events will be discarded. Kafka’s performance is effectively constant with respect to data size, so storing data for a long time is perfectly fine. ==事件被有条理和持久性的被存储在topic中，简单的来讲，topic类似于文件系统中的文件夹，事件相当于文件夹中的文件。在kafka中的topics通常有多个生产者和多个订阅者。一个topic通常有0个，1个甚至更多的生产者，同样的，也会有0个，1个，甚至多个消费者去订阅这些事件。与传统的消息系统相比，事件当被消费之后不会被删除。你可以定义多久才会被删除（默认7天）== partitioned：Topics are partitioned, meaning a topic is spread over a number of “buckets” located on different Kafka brokers. This distributed placement of your data is very important for scalability because it allows client applications to both read and write the data from/to many brokers at the same time. When a new event is published to a topic, it is actually appended to one of the topic’s partitions. Events with the same event key (e.g., a customer or vehicle ID) are written to the same partition, and Kafka guarantees that any consumer of a given topic-partition will always read that partition’s events in exactly the same order as they were written. ==topics被分区化，意味着一个topic被分成了多个partition，存放在不同的kafka broker上。== 安装单机版kafka官方版本 下载并解压Kafkahttps://www.apache.org/dyn/closer.cgi?path=/kafka/2.8.0/kafka_2.13-2.8.0.tgz #解压$ tar -xzf kafka_2.13-2.8.0.tgz$ cd kafka_2.13-2.8.0 开启kafka环境NOTE: Your local environment must have Java 8+ installed. 本地必须安装java 8+； Run the following commands in order to start all services in the correct order: 开启zookeeper（下载下来的kafka中带zookeeper，或者你可以用自己的环境中的zk） # Start the ZooKeeper service# Note: Soon, ZooKeeper will no longer be required by Apache Kafka.# 不久的将来，zookeeper已经不是kafka必须的环境了$ bin/zookeeper-server-start.sh config/zookeeper.properties 打开另外一个命令窗口，运行如下命令 # Start the Kafka broker service$ bin/kafka-server-start.sh config/server.properties 当所有的环境都启动之后，你的kafka的服务就算是启动起来了。 测试创建一个topic $ bin/kafka-topics.sh --create --topic quickstart-events --bootstrap-server localhost:9092 生产者生产 $ bin/kafka-console-producer.sh --topic quickstart-events --bootstrap-server localhost:9092 打开另外一个terminal，打开启动消费者 $ bin/kafka-console-consumer.sh --topic quickstart-events --from-beginning --bootstrap-server localhost:9092 在生产者发送消息，可以看到在消费者也会收到，至此，一个单机版的kafka已经搭建完成。 Kafka配置文件详解server.properties# Licensed to the Apache Software Foundation (ASF) under one or more# contributor license agreements. See the NOTICE file distributed with# this work for additional information regarding copyright ownership.# The ASF licenses this file to You under the Apache License, Version 2.0# (the &quot;License&quot;); you may not use this file except in compliance with# the License. You may obtain a copy of the License at## http://www.apache.org/licenses/LICENSE-2.0## limitations under the License.# see kafka.server.KafkaConfig for additional details and defaultsbroker.id=0# FORMAT:# listeners = listener_name://host_name:port# EXAMPLE:# listeners = PLAINTEXT://your.host.name:9092listeners=PLAINTEXT://localhost:9092# returned from java.net.InetAddress.getCanonicalHostName().#advertised.listeners=PLAINTEXT://your.host.name:9092num.network.threads=3socket.send.buffer.bytes=102400socket.request.max.bytes=104857600############################# Log Basics ############################## A comma separated list of directories under which to store log fileslog.dirs=/Users/majiju/develop-tool/Kafka/Kafka1/logsnum.partitions=1offsets.topic.replication.factor=1transaction.state.log.replication.factor=1transaction.state.log.min.isr=1############################# Log Flush Policy ############################## There are a few important trade-offs here:# 1. Durability: Unflushed data may be lost if you are not using replication.# The number of messages to accept before forcing a flush of data to disk# The maximum amount of time a message can sit in a log before we force a flush#log.flush.interval.ms=1000# from the end of the log.# The minimum age of a log file to be eligible for deletion due to agelog.retention.hours=168log.segment.bytes=1073741824# Zookeeper connection string (see zookeeper docs for details).# This is a comma separated host:port pairs, each corresponding to a zk# server. e.g. &quot;127.0.0.1:3000,127.0.0.1:3001,127.0.0.1:3002&quot;.# You can also append an optional chroot string to the urls to specify the# root directory for all kafka znodes.zookeeper.connect=localhost:12181# Timeout in ms for connecting to zookeeperzookeeper.connection.timeout.ms=18000############################# Group Coordinator Settings ############################## The following configuration specifies the time, in milliseconds, that the GroupCoordinator will delay the initial consumer rebalance.# The rebalance will be further delayed by the value of group.initial.rebalance.delay.ms as new members join the group, up to a maximum of max.poll.interval.ms.# The default value for this is 3 seconds.# We override this to 0 here as it makes for a better out-of-the-box experience for development and testing.# However, in production environments the default value of 3 seconds is more suitable as this will help to avoid unnecessary, and potentially expensive, rebalances during application startup.group.initial.rebalance.delay.ms=0 我们来一个个的读配置文件中蓝色字体的注释 字段 解释 broker.id 当前节点的id，集群中唯一 listeners=PLAINTEXT://localhost:9092 监听地址和端口，producter、consumer连接的端口，默认9092 num.network.threads=3 broker处理消息的最大线程数，一般情况下数量为cpu核数 socket.send.buffer.bytes=102400 socket的发送缓冲区 socket.request.max.bytes=104857600 socket请求的最大字节数 log.dirs 日志的目录 num.partitions 每个topic分区的个数 offsets.topic.replication.factor 副本个数 transaction.state.log.replication.factor transaction.state.log.min.isr log.retention.hours=168 日志保留的最长的时长，默认为7天 log.segment.bytes 每个segment的大小 zookeeper.connect zk的连接地址 zookeeper.connection.timeout.ms zk连接超时时间 group.initial.rebalance.delay.ms 消费者分组协调者 producer.properties# Licensed to the Apache Software Foundation (ASF) under one or more# contributor license agreements. See the NOTICE file distributed with# this work for additional information regarding copyright ownership.# The ASF licenses this file to You under the Apache License, Version 2.0# (the &quot;License&quot;); you may not use this file except in compliance with# the License. You may obtain a copy of the License at## http://www.apache.org/licenses/LICENSE-2.0## Unless required by applicable law or agreed to in writing, software# distributed under the License is distributed on an &quot;AS IS&quot; BASIS,# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.# See the License for the specific language governing permissions and# limitations under the License.# see org.apache.kafka.clients.producer.ProducerConfig for more details############################# Producer Basics ############################## list of brokers used for bootstrapping knowledge about the rest of the cluster# format: host1:port1,host2:port2 ...bootstrap.servers=localhost:9092# specify the compression codec for all data generated: none, gzip, snappy, lz4, zstdcompression.type=none# name of the partitioner class for partitioning events; default partition spreads data randomly#partitioner.class=# the maximum amount of time the client will wait for the response of a request#request.timeout.ms=# how long `KafkaProducer.send` and `KafkaProducer.partitionsFor` will block for#max.block.ms=# the producer will wait for up to the given delay to allow other records to be sent so that the sends can be batched together#linger.ms=# the maximum size of a request in bytes#max.request.size=# the default batch size in bytes when batching multiple records sent to a partition#batch.size=# the total bytes of memory the producer can use to buffer records waiting to be sent to the server#buffer.memory= 字段 解释 bootstrap.servers kafka集群列表，用于获取metadata，不必全部指定，多个用”,”分开 compression.type 压缩类型 none，gzip，snappy，lz4，zstd partitioner.class 分区策略，默认是随机分区 request.timeout.ms 客户端等待请求响应的最大超时时间 max.block.ms 最大的阻塞时长 linger.ms 生产者将等待到给定的延迟，以允许其他记录被发送，以便发送可以批处理在一起 max.request.size 最大的请求字节数 batch.size 批处理的最大数 buffer.memory 生产者能够使用的最大空间 （一起发送给server） consumer.properties# Licensed to the Apache Software Foundation (ASF) under one or more# contributor license agreements. See the NOTICE file distributed with# this work for additional information regarding copyright ownership.# The ASF licenses this file to You under the Apache License, Version 2.0# (the &quot;License&quot;); you may not use this file except in compliance with# the License. You may obtain a copy of the License at## http://www.apache.org/licenses/LICENSE-2.0## Unless required by applicable law or agreed to in writing, software# distributed under the License is distributed on an &quot;AS IS&quot; BASIS,# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.# See the License for the specific language governing permissions and# limitations under the License.# see org.apache.kafka.clients.consumer.ConsumerConfig for more details# list of brokers used for bootstrapping knowledge about the rest of the cluster# format: host1:port1,host2:port2 ...bootstrap.servers=localhost:9092# consumer group idgroup.id=test-consumer-group# What to do when there is no initial offset in Kafka or if the current# offset does not exist any more on the server: latest, earliest, none#auto.offset.reset= 字段 描述 bootstrap.servers 连接的server的地址 group.id 分组id","link":"/kafka-install/"},{"title":"Springboot创建简单的kafka项目","text":"创建一个简单的kafka父子项目，用来熟悉一下kafka在springboot中的集成，特此记录一下 环境开发工具：idea 开发环境：maven、java1.8 电脑：macOS 创建一个父项目，现贴入pom.xml&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;project xmlns=&quot;http://maven.apache.org/POM/4.0.0&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xsi:schemaLocation=&quot;http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd&quot;&gt; &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt; &lt;groupId&gt;com.mrhy&lt;/groupId&gt; &lt;artifactId&gt;kafka-demo&lt;/artifactId&gt; &lt;packaging&gt;pom&lt;/packaging&gt; &lt;version&gt;1.0-SNAPSHOT&lt;/version&gt; &lt;modules&gt; &lt;module&gt;kafka-publisher&lt;/module&gt; &lt;module&gt;kafka-consumer&lt;/module&gt; &lt;/modules&gt; &lt;properties&gt; &lt;java.version&gt;1.8&lt;/java.version&gt; &lt;maven.compiler.source&gt;8&lt;/maven.compiler.source&gt; &lt;maven.compiler.target&gt;8&lt;/maven.compiler.target&gt; &lt;spring-boot.version&gt;2.4.5&lt;/spring-boot.version&gt; &lt;lombok.version&gt;1.18.20&lt;/lombok.version&gt; &lt;/properties&gt; &lt;dependencyManagement&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-dependencies&lt;/artifactId&gt; &lt;version&gt;${spring-boot.version}&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.projectlombok&lt;/groupId&gt; &lt;artifactId&gt;lombok&lt;/artifactId&gt; &lt;version&gt;${lombok.version}&lt;/version&gt; &lt;/dependency&gt; &lt;!-- https://mvnrepository.com/artifact/org.apache.kafka/kafka-clients --&gt;&lt;!-- &lt;dependency&gt;--&gt;&lt;!-- &lt;groupId&gt;org.apache.kafka&lt;/groupId&gt;--&gt;&lt;!-- &lt;artifactId&gt;kafka-clients&lt;/artifactId&gt;--&gt;&lt;!-- &lt;version&gt;${kafka-client.version}&lt;/version&gt;--&gt;&lt;!-- &lt;/dependency&gt;--&gt; &lt;/dependencies&gt; &lt;/dependencyManagement&gt; &lt;build&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-maven-plugin&lt;/artifactId&gt; &lt;configuration&gt; &lt;excludes&gt; &lt;exclude&gt; &lt;groupId&gt;org.projectlombok&lt;/groupId&gt; &lt;artifactId&gt;lombok&lt;/artifactId&gt; &lt;/exclude&gt; &lt;/excludes&gt; &lt;/configuration&gt; &lt;/plugin&gt; &lt;/plugins&gt; &lt;/build&gt;&lt;/project&gt; 创建生产者pom.xml如下&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;project xmlns=&quot;http://maven.apache.org/POM/4.0.0&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xsi:schemaLocation=&quot;http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd&quot;&gt; &lt;parent&gt; &lt;artifactId&gt;kafka-demo&lt;/artifactId&gt; &lt;groupId&gt;com.mrhy&lt;/groupId&gt; &lt;version&gt;1.0-SNAPSHOT&lt;/version&gt; &lt;/parent&gt; &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt; &lt;artifactId&gt;kafka-publisher&lt;/artifactId&gt; &lt;properties&gt; &lt;maven.compiler.source&gt;8&lt;/maven.compiler.source&gt; &lt;maven.compiler.target&gt;8&lt;/maven.compiler.target&gt; &lt;/properties&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-web&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.projectlombok&lt;/groupId&gt; &lt;artifactId&gt;lombok&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.kafka&lt;/groupId&gt; &lt;artifactId&gt;spring-kafka&lt;/artifactId&gt; &lt;/dependency&gt;&lt;!-- &lt;dependency&gt;--&gt;&lt;!-- &lt;groupId&gt;org.apache.kafka&lt;/groupId&gt;--&gt;&lt;!-- &lt;artifactId&gt;kafka-clients&lt;/artifactId&gt;--&gt;&lt;!-- &lt;/dependency&gt;--&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-test&lt;/artifactId&gt; &lt;scope&gt;test&lt;/scope&gt; &lt;/dependency&gt; &lt;/dependencies&gt;&lt;/project&gt; 配置application.propertiesserver.port=8081server.servlet.context-path=/publisherspring.kafka.bootstrap-servers=localhost:9092# 发生错误时，消息重发的次数spring.kafka.producer.retries=0#当有多个消息需要被发送到同一个分区时，生产者会把它们放在同一个批次里。该参数指定了一个批次可以使用的内存大小，按照字节数计算。spring.kafka.producer.batch-size=16384# 设置生产者内存缓冲区大小spring.kafka.producer.buffer-memory=33554432# key的序列化方式spring.kafka.producer.key-serializer=org.apache.kafka.common.serialization.StringSerializer# 值得序列化方式spring.kafka.producer.value-serializer=org.apache.kafka.common.serialization.StringSerializer# acks=0 ： 生产者在成功写入消息之前不会等待任何来自服务器的响应。# acks=1 ： 只要集群的首领节点收到消息，生产者就会收到一个来自服务器成功响应。# acks=all ：只有当所有参与复制的节点全部收到消息时，生产者才会收到一个来自服务器的成功响应。spring.kafka.producer.acks=1 生产者发送消息@Component@Log4j2public class MyProvider { @Autowired KafkaTemplate kafkaTemplate; public void sendMessage(String topic, Object object) { /* * 这里的 ListenableFuture 类是 spring 对 java 原生 Future 的扩展增强,是一个泛型接口,用于监听异步方法的回调 * 而对于 kafka send 方法返回值而言，这里的泛型所代表的实际类型就是 SendResult&lt;K, V&gt;,而这里 K,V 的泛型实际上 * 被用于 ProducerRecord&lt;K, V&gt; producerRecord,即生产者发送消息的 key,value 类型 */ ListenableFuture&lt;SendResult&lt;String, Object&gt;&gt; future = kafkaTemplate.send(topic, object); future.addCallback(new ListenableFutureCallback&lt;SendResult&lt;String, Object&gt;&gt;() { @Override public void onFailure(Throwable throwable) { log.info(&quot;发送消息失败:&quot; + throwable.getMessage()); } @Override public void onSuccess(SendResult&lt;String, Object&gt; sendResult) { System.out.println(&quot;发送结果:&quot; + sendResult.toString()); } }); }} 创建测试类@SpringBootTest(classes = PublisherApplication.class)@RunWith(SpringRunner.class)public class SendTest { @Autowired MyProvider myProvider; @Test public void testSendMsg(){ myProvider.sendMessage(&quot;quickstart-events&quot;,&quot;你好&quot;); }} 消费者消费指定topic的消息pom.xml&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;project xmlns=&quot;http://maven.apache.org/POM/4.0.0&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xsi:schemaLocation=&quot;http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd&quot;&gt; &lt;parent&gt; &lt;artifactId&gt;kafka-demo&lt;/artifactId&gt; &lt;groupId&gt;com.mrhy&lt;/groupId&gt; &lt;version&gt;1.0-SNAPSHOT&lt;/version&gt; &lt;/parent&gt; &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt; &lt;artifactId&gt;kafka-consumer&lt;/artifactId&gt; &lt;properties&gt; &lt;maven.compiler.source&gt;8&lt;/maven.compiler.source&gt; &lt;maven.compiler.target&gt;8&lt;/maven.compiler.target&gt; &lt;/properties&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-web&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.projectlombok&lt;/groupId&gt; &lt;artifactId&gt;lombok&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.kafka&lt;/groupId&gt; &lt;artifactId&gt;spring-kafka&lt;/artifactId&gt; &lt;/dependency&gt; &lt;/dependencies&gt;&lt;/project&gt; 配置application.propertiesserver.port=8082server.servlet.context-path=/consumerspring.kafka.bootstrap-servers=localhost:9092# 自动提交的时间间隔 在 spring boot 2.X 版本中这里采用的是值的类型为 Duration 需要符合特定的格式，如 1S,1M,2H,5Dspring.kafka.consumer.auto-commit-interval=1Sspring.kafka.consumer.auto-offset-reset=earliestspring.kafka.consumer.enable-auto-commit=truespring.kafka.consumer.key-deserializer=org.apache.kafka.common.serialization.StringDeserializerspring.kafka.consumer.value-deserializer=org.apache.kafka.common.serialization.StringDeserializerspring.kafka.listener.concurrency=5 消费者消费@Component@Log4j2public class MyConsumer { @KafkaListener(topics = {&quot;quickstart-events&quot;},groupId = &quot;defaultConsumerGroup&quot;) public void getMsg(ConsumerRecord&lt;?, ?&gt; record){ log.info(&quot;获取消息：topic:{},partition:{},value:{}&quot;,record.topic(),record.partition(),record.value()); }} 点击并打开消费者项目，此时消费者就进入监听状态，当我们执行生产者的sendMsg的测试方法的时候，可以看到消费者就会有一条消息消费 结束，收工。","link":"/spring-kafka-demo/"},{"title":"clickhouse的基本数据类型","text":"此文章记录clickhouse的基本数据类型，便于后续翻阅，查找。 一、基础类型1. 数值类型Int有符号 名称 大小 范围 普遍观念 Int8 1 -128到127 Tinyint Int16 2 -32768到32767 Smallint Int32 4 Int Int64 8 Bigint 无符号 名称 大小 范围 普遍观念 UInt8 1 Tinyint UInt16 2 Smallint UInt32 4 Int UInt64 8 Bigint Float 名称 大小（字节） 有效精度 普遍观念 Float32 4 7 Float Float64 8 16 Double Decimal要求更高的精度，需要使用定点数，其中有3种方式，Decimal32(S),Decimal(S),Decimal128(S),S代表小数的位数。 原生方式为Decimal(P,S),其中P代表精度，决定总位数（整数位数+小数位数），取值范围为138 S代表着规模，决定小数位数。取值为0P。 加减乘除规则： 加：位数以高位为主。 减：位数以高位为主。 乘：位数以 两个数的位数相加。 除：位数以高位数为主。 2. 字符串类型String字符串由String定义，==长度不限== FixedString定长字段，长度固定，用法：FixedString(N),N表示固定数据 UUID生成随机的一种数据，共32位，它的格式是8-4-4-4-12。如果一个UUID类型的字段在写入数据时没有被俘值，则会骂自己安排格式使用0填充。 3. 时间类型Clickhouse 的时间类型分为DateTime，DateTime64和Date三类。ClickHouse目前没有时间戳类，时间戳类型最高的精度是秒，也就是说，如果需要处理毫秒，微秒等大于秒分辨率的时间，则只能借助UInt类型实现。 DataTime包含时分秒信息，精确到秒，支持使用字符串类型插入 ‘2019-06-22 00:00:00’ DateTime64可以记录到亚秒，它在dateTime上增加了精度的设置。 ‘2019-06-22 00:00:00.0’ DateDate类型不包含具体的时间信息，只能精确到天，同时也支持字符串形式写入。 复合类型除了基础数据类型，ClickHouse还提供了数组、元祖、枚举和嵌套四类复合类型。 Arrayselect array(1,2) select [1,2]","link":"/clickhouse-data-type/"},{"title":"clickhouse的简介及安装","text":"一、Clickhouse的简介1 优点2 缺点3 应用场景4 核心概念1）数据分片2）列式存储3）向量化4）表5）分区6）副本7）引擎二、clickhouse 的部署三、clickhouse的基本类型与语法","link":"/clickhouse/"},{"title":"flink的简介及安装","text":"元组","link":"/flink/"},{"title":"es基础","text":"今天开始学习es，先花费一天的时间了解es的基本，剩下的慢慢去弥补知识点。 es的技术主要跟着狂神说学习的，推荐一波。 https://www.bilibili.com/video/BV17a4y1x7zq?p=2&amp;spm_id_from=pageDriver ES干什么的 两个字：搜索 聊一个人聊聊Doug Cutting生活中，可能所有人都间接用过他的作品，他是Lucene、Nutch 、Hadoop等项目的发起人。是他，把高深莫测的搜索技术形成产品，贡献给普罗大众；还是他，打造了目前在云计算和大数据领域里如日中天的Hadoop。他是某种意义上的盗火者，他就是Doug Cutting。 从实习生做起 1985年，Cutting毕业于美国斯坦福大学。他并不是一开始就决心投身IT行业的，在大学时代的头两年，Cutting学习了诸如物理、地理等常规课程。因为学费的压力，Cutting开始意识到，自己必须学习一些更加实用、有趣的技能。这样，一方面可以帮助自己还清贷款，另一方面，也是为自己未来的生活做打算。因为斯坦福大学座落在IT行业的“圣地”硅谷，所以学习软件对年轻人来说是再自然不过的事情了。 Cutting的第一份工作是在Xerox做实习生，Xerox当时的激光扫描仪上运行着三个不同的操作系统，其中的一个操作系统还没有屏幕保护程序。因此，Cutting就开始为这套系统开发屏幕保护程序。由于这套程序是基于系统底层开发的，所以 其他同事可以给这个程序添加不同的主题。这份工作给了Cutting一定的满足感，也是他最早的“平台”级的作品。 可以说，Xerox对 Cutting后来研究搜索技术起到了决定性的影响，除了短暂的在苏格兰工作的经历外，Cutting事业的起步阶段大部分都是在Xerox度过的，这段 时间让他在搜索技术的知识上有了很大提高。他花了四年的时间搞研发，这四年中，他阅读了大量的论文，同时，自己也发表了很多论文，用Cutting自己的 话说——“我的研究生是在Xerox读的。” 尽管Xerox让Cutting积累了不少技术知识，但他却认为，自己当时搞的这些研究只是纸 上谈兵，没有人试验过这些理论的可实践性。于是，他决定勇敢地迈出这一步，让搜索技术可以为更多人所用。1997年底，Cutting开始以每周两天的时间投入，在家里试着用Java把这个想法变成现实，不久之后，Lucene诞生了。作为第一个提供全文文本搜索的开源函数库，Lucene的伟大自不必多言。 Hadoop的诞生 之后，Cutting再接再厉，在 Lucene的基础上将开源的思想继续深化。2004年，Cutting和同为程序员出身的Mike Cafarella决定开发一款可以代替当时的主流搜索产品的开源搜索引擎，这个项目被命名为Nutch。在此之前，Cutting所在的公司 Architext（其主要产品为Excite搜索引擎）因没有顶住互联网经济泡沫的冲击而破产，那时的Cutting正处在Freelancer的生涯 中，所以他希望自己的项目能通过一种低开销的方式来构建网页中的大量算法。幸运的是，Google这时正好发布了一项研究报告，报告中介绍了两款 Google为支持自家的搜索引擎而开发的软件平台。这两个平台一个是GFS（Google File System），用于存储不同设备所产生的海量数据；另一个是MapReduce，它运行在GFS之上，负责分布式大规模数据。基于这两个平台，Cutting最引人瞩目的作品——Hadoop诞生了。谈到Google对他们的“帮助”，Cutting说：“我们开始设想用4~5台电脑来实现 这个项目，但在实际运行中牵涉了大量繁琐的步骤需要靠人工来完成。Google的平台让这些步骤得以自动化，为我们实现整体框架打下了良好的基础。” 说起Google，Cutting也是它成长的见证人之一，这里有一段鲜为人知的故事。早在Cutting供职于Architext期间，有两个年轻人曾去拜访这家公司，并向他们兜售自己的搜索技术，但当时他们的Demo只检索出几百万条网页，Excite的工程师们觉得他们的技术太小儿科，于是就在心里鄙 视一番，把他们给送走了。但故事并未到此结束，这两个年轻人回去之后痛定思痛，决定自己创业。于是，他们开了一家自己的搜索公司，取名为Google。这两个年轻人就是Larry Page和Sergey Brin。在Cutting看来，Google的成功主要取决于，反向排序之后再存储的设计和对自己技术的自信。 让“开源”影响世界 出于对时间成本的考虑，在从Architext离职四年后，Cutting决定结束这段Freelancer的生涯，找一家靠谱的公司，进一步完善 Hadoop的性能。他先后面试了几家公司，其中也包括IBM，但IBM似乎对他的早期项目Lucene更感兴趣，至于Hadoop则不置可否。就在此 时，Cutting接受了当时Yahoo!搜索项目负责人Raymie Stata的邀请，于2006年正式加入Yahoo!。在Yahoo!，有一支一百人的团队帮助他完善Hadoop项目，这期间开发工作进行得卓有成效。 不久之后，Yahoo!就宣布，将其旗下的搜索业务的架构迁移到Hadoop上来。两年后，Yahoo!便基于Hadoop启动了第一个应用项目 “webmap”——一个用来计算网页间链接关系的算法。Cutting的时任上司（后为Hortonworks CEO）Eric Baldeschwieler曾说：“在相同的硬件环境下，基于Hadoop的webmap的反应速度是之前系统的33倍。” 虽然 Hadoop的表现惊艳，但在当时并非所有公司都有条件使用，与此同时，用户需求却在日益增加。有些大公司（如银行、电信公司、大型零售商等）只关注于产品，却不想在技术工程和咨询服务上过多投入，它们需要一个可以帮助其解决问题的平台，这就是Cutting后来跳槽到Cloudera的初衷。从某种程度上说，Cloudera就是这么一个为那些在咨询和技术上有需求的公司提供服务的平台。它的客户大多来自于传统行业，希望通过Hadoop来处理之前只能 被直接抛弃的大规模数据。现在，除了这些传统行业之外，Yahoo!、Facebook、eBay、LinkedIn等公司都在使用Hadoop，用 Cutting的话说，他们的团队被“无形之中扩大了”。 目前，Cutting的目标是把Hadoop发展成云计算领域的RedHat。 “我从来没有想过，除了搜索引擎，Hadoop的作用还能在其他方面有所发挥，它今天所受到的关注程度，已超过了我之前的所有想象”。谈到成功，Cutting认为他的成功主要归功于两点，一是对自己工作的热情（Cutting在大学时就开始做Infrastracture类的程序，还用 Lisp为Emacs贡献过代码，他非常喜欢自己的程序被千万人使用的感觉）；二是目标不要定得过大，要踏踏实实，一步一个脚印。 文章参考：https://blog.csdn.net/weixin_34060741/article/details/85610704?utm_medium=distribute.pc_relevant_t0.none-task-blog-2%7Edefault%7EBlogCommendFromMachineLearnPai2%7Edefault-1.control&amp;depth_1-utm_source=distribute.pc_relevant_t0.none-task-blog-2%7Edefault%7EBlogCommendFromMachineLearnPai2%7Edefault-1.control ES的简介You know, for search (and analysis) 货比三家安装ES 声明：JDK1.8 最低要求 下载：https://www.elastic.co/cn/downloads/elasticsearch 目录结构➜ elasticsearch-7.14.0 lltotal 1224-rw-r--r--@ 1 cooper staff 3.8K 7 30 04:47 LICENSE.txt-rw-r--r--@ 1 cooper staff 601K 7 30 04:51 NOTICE.txt-rw-r--r--@ 1 cooper staff 2.6K 7 30 04:47 README.asciidocdrwxr-xr-x@ 25 cooper staff 800B 7 30 04:54 bindrwxr-xr-x@ 11 cooper staff 352B 8 20 22:06 configdrwxr-xr-x 3 cooper staff 96B 8 20 22:06 datadrwxr-xr-x@ 3 cooper staff 96B 7 30 04:54 jdk.appdrwxr-xr-x@ 42 cooper staff 1.3K 7 30 04:54 libdrwxr-xr-x@ 15 cooper staff 480B 8 21 00:00 logsdrwxr-xr-x@ 59 cooper staff 1.8K 7 30 04:55 modulesdrwxr-xr-x@ 2 cooper staff 64B 7 30 04:51 plugins bin启动脚本、停止脚本 config lib包 modules模块 plugins插件包 启动bin/elasticsearch ES的可视化程序head安装下载地址https://github.com/mobz/elasticsearch-head 安装npm installnpm start 连接es打开网址localhost:9100,连接es 点击连接，第一次连接的时候有跨域问题，需要去es中解决跨域问题 解决跨域问题去es的config目录下修改配置文件，修改elasticsearch.yml 添加配置项 http.cors.enabled: truehttp.cors.allow-origin: '*' 重新启动es bin/elasticsearch 点击连接，跨域问题解决 安装kibanakibana是什么Kibana 是一款免费开源的前端应用程序，其基础是 Elastic Stack，可以为 Elasticsearch 中索引的数据提供搜索和数据可视化功能。尽管人们通常将 Kibana 视作 Elastic Stack（之前称作 ELK Stack，分别表示 Elasticsearch、Logstash 和 Kibana）的制图工具，但也可将 Kibana 作为用户界面来监测和管理 Elastic Stack 集群并确保集群安全性，还可将其作为基于 Elastic Stack 所开发内置解决方案的汇集中心。Elasticsearch 社区于 2013 年开发出了 Kibana，现在 Kibana 已发展成为 Elastic Stack 的窗口，是用户和公司的一个门户。 Kibana用途Kibana 与 Elasticsearch 和更广意义上的 Elastic Stack 紧密集成，这一点使其成为支持下列场景的理想之选： 搜索、查看并可视化 Elasticsearch 中所索引的数据，并通过创建柱状图、饼状图、表格、直方图和地图对数据进行分析。仪表板视图能将这些可视化元素集中到一起，然后通过浏览器加以分享，以提供有关海量数据的实时分析视图，为下列用例提供支持： 日志处理和分析 基础设施指标和容器监测 应用程序性能监测 (APM) 地理空间数据分析和可视化 安全分析 业务分析 借助网络界面来监测和管理 Elastic Stack 实例并确保实例的安全。 针对基于 Elastic Stack 开发的内置解决方案（面向可观测性、安全和企业搜索应用程序），将其访问权限集中到一起。 下载https://www.elastic.co/cn/downloads/kibana 配置汉化去config目录下，修改kibana.yml #i18n.locale: &quot;en&quot;i18n.locale: &quot;zh-CN&quot; 启动bin/kibana ES核心概念 Es是面向文档的,关系型数据库和elasticsearch客观的对比，一切都是json！ Relation DB ElasticSearch 数据库 索引（indices） 表 types 行 documents 字段 fields 物理设计： elasticSearch在后台把每个索引分成多个分片，每个分片 集群节点索引库 类型7.x 逐渐废弃 文档（documents）分片（shard）映射（mapping）字段类型 IK分词器什么是IK分词器分词:即把一段中文或者别的划分成一个个的关键字,我们在搜索时候会把自己的信息进行分词,会把数据库中或者索引库中的数据进行分词,然后进行一个匹配操作,默认的中文分词器是将每个字看成一个词,比如”我爱技术”会被分为”我”,”爱”,”技”,”术”,这显然不符合要求,所以我们需要安装中文分词器IK来解决这个问题 IK提供了两个分词算法:ik_smart和ik_max_word 其中ik_smart为最少切分,ik_max_word为最细粒度划分 安装地址：https://github.com/medcl/elasticsearch-analysis-ik/releases 下载完毕，放到elasticSearch下面的插件即可 重启ik 实战最少切分 最细粒度切分 实战2 如果想要将姜大合并成一个词，需要==手动配置字典== 手动配置ik字典去ik分词器的config目录下，创建自己的字典 vim new.dic 修改配置文件 vim IKAnalyzer.cfg.xml 重启es RestFul操作ES 关于索引的操作 创建（更新）一个索引 创建一个索引类型 获取数据、规则 用POST实现修改 删除索引 DELETE test 关于文档的操作存放数据 获取数据 更新数据 version代表着数据更新的次数 简单搜索 复杂搜索 构建复杂查询的json 指定要查询的字段 排序用sort 分页 from：从第几个数据开始 size：返回几条数据 多条件查询 must 多条件必须都满足 should 是or的意思，代表着两个条件满足一个就行 must_not 代表着不匹配 高亮","link":"/es-common/"},{"title":"springboot集成es","text":"Springboot集成es是重点，今天跟着狂神一起学习。 官方文档https://www.elastic.co/guide/en/elasticsearch/client/index.html 第一步：找到原生的API &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-data-elasticsearch&lt;/artifactId&gt; &lt;exclusions&gt; &lt;exclusion&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-logging&lt;/artifactId&gt; &lt;/exclusion&gt; &lt;/exclusions&gt; &lt;/dependency&gt; 测试API package com.mrhy.es;import com.alibaba.fastjson.JSON;import com.mrhy.es.pojo.User;import org.elasticsearch.action.admin.indices.delete.DeleteIndexRequest;import org.elasticsearch.action.bulk.BulkRequest;import org.elasticsearch.action.bulk.BulkResponse;import org.elasticsearch.action.delete.DeleteRequest;import org.elasticsearch.action.delete.DeleteResponse;import org.elasticsearch.action.get.GetRequest;import org.elasticsearch.action.get.GetResponse;import org.elasticsearch.action.index.IndexRequest;import org.elasticsearch.action.index.IndexResponse;import org.elasticsearch.action.search.SearchRequest;import org.elasticsearch.action.search.SearchResponse;import org.elasticsearch.action.support.master.AcknowledgedResponse;import org.elasticsearch.action.update.UpdateRequest;import org.elasticsearch.action.update.UpdateResponse;import org.elasticsearch.client.RequestOptions;import org.elasticsearch.client.RestHighLevelClient;import org.elasticsearch.client.indices.CreateIndexRequest;import org.elasticsearch.client.indices.CreateIndexResponse;import org.elasticsearch.client.indices.GetIndexRequest;import org.elasticsearch.common.unit.TimeValue;import org.elasticsearch.common.xcontent.XContentType;import org.elasticsearch.index.query.QueryBuilder;import org.elasticsearch.index.query.QueryBuilders;import org.elasticsearch.search.SearchHit;import org.elasticsearch.search.builder.SearchSourceBuilder;import org.elasticsearch.search.fetch.subphase.FetchSourceContext;import org.junit.jupiter.api.Test;import org.springframework.beans.factory.annotation.Autowired;import org.springframework.boot.test.context.SpringBootTest;import java.io.IOException;import java.util.ArrayList;import java.util.Locale;import java.util.UUID;import java.util.concurrent.TimeUnit;/** * @description: * @author: cooper * @date: 2021/8/22 11:25 下午 */@SpringBootTestpublic class EsTest { @Autowired private RestHighLevelClient restHighLevelClient; public static final String INDEX = &quot;mrhy_index&quot;; /** * @description 测试创建索引 * @author cooper * @date 2021/8/23 9:41 下午 */ @Test public void createIndex() throws IOException { // 创建索引请求 CreateIndexRequest indexRequest = new CreateIndexRequest(INDEX); // 执行请求 CreateIndexResponse createIndexResponse = restHighLevelClient.indices().create(indexRequest, RequestOptions.DEFAULT); // 获取结果 System.out.println(createIndexResponse); } /** * @description 测试删除索引 * @author cooper * @date 2021/8/23 9:42 下午 */ @Test public void deleteIndex() throws IOException { DeleteIndexRequest deleteIndexRequest = new DeleteIndexRequest(INDEX); AcknowledgedResponse acknowledgedResponse = restHighLevelClient.indices().delete(deleteIndexRequest, RequestOptions.DEFAULT); System.out.println(JSON.toJSONString(acknowledgedResponse)); } /** * @description 测试获取索引 * @author cooper * @date 2021/8/23 9:50 下午 */ @Test public void existIndex() throws IOException { GetIndexRequest indicesExistsRequest = new GetIndexRequest(INDEX); boolean acknowledgedResponse = restHighLevelClient.indices().exists(indicesExistsRequest, RequestOptions.DEFAULT); System.out.println(JSON.toJSONString(acknowledgedResponse)); } /** * @description 测试创建文档 * @author cooper * @date 2021/8/29 5:11 下午 */ @Test public void testCreateDocument() throws IOException { User cooper = new User(&quot;cooper&quot;, 25); IndexRequest indexRequest = new IndexRequest(INDEX); indexRequest.id(UUID.randomUUID().toString().toLowerCase(Locale.ROOT)); indexRequest.timeout(&quot;1s&quot;); indexRequest.source(JSON.toJSONString(cooper), XContentType.JSON); IndexResponse index = restHighLevelClient.index(indexRequest, RequestOptions.DEFAULT); System.out.println(JSON.toJSONString(index)); System.out.println(index.status()); System.out.println(index.getId()); } /** * @description 测试判断和获取document * @author cooper * @date 2021/8/29 5:36 下午 */ @Test public void testExistAndGetDocument() throws IOException { GetRequest request = new GetRequest(INDEX, &quot;1&quot;); // FetchSourceContext 判断是否需要创建资源。判断是否存在的时候一定设置为false，保证效率。 request.fetchSourceContext(new FetchSourceContext(true)); boolean exists = restHighLevelClient.exists(request, RequestOptions.DEFAULT); if (exists) { GetResponse response = restHighLevelClient.get(request, RequestOptions.DEFAULT); System.out.println(response.getSourceAsString()); } } /** * @description 更新es中的文档 * @author cooper * @date 2021/8/29 5:59 下午 */ @Test public void updateDocument() throws IOException { UpdateRequest updateRequest = new UpdateRequest(INDEX, &quot;1&quot;); updateRequest.doc(JSON.toJSONString(new User(&quot;cooper&quot;, 18)), XContentType.JSON); UpdateResponse update = restHighLevelClient.update(updateRequest, RequestOptions.DEFAULT); System.out.println(JSON.toJSONString(update)); } /** * @description 删除文档 * @author cooper * @date 2021/8/29 6:04 下午 */ @Test public void testDeleteDocument() throws IOException { DeleteRequest deleteRequest = new DeleteRequest(INDEX, &quot;1&quot;); DeleteResponse delete = restHighLevelClient.delete(deleteRequest, RequestOptions.DEFAULT); System.out.println(delete.status()); } /** * @description 批量插入数据 * @author cooper * @date 2021/8/29 6:06 下午 */ @Test public void testBulkRequest() throws IOException{ BulkRequest bulkRequest = new BulkRequest(INDEX); bulkRequest.timeout(&quot;10s&quot;); ArrayList&lt;User&gt; users = new ArrayList&lt;&gt;(); users.add(new User(&quot;张三&quot;,12)); users.add(new User(&quot;李四&quot;,13)); users.add(new User(&quot;王五&quot;,14)); users.add(new User(&quot;赵六&quot;,15)); // 批处理请求 for (User user : users) { bulkRequest.add(new IndexRequest().id(UUID.randomUUID().toString().toLowerCase(Locale.ROOT)).source(JSON.toJSONString(user),XContentType.JSON)); } BulkResponse bulk = restHighLevelClient.bulk(bulkRequest, RequestOptions.DEFAULT); System.out.println(bulk.status()); } /** * @description 搜索 * SearchRequest: 搜索请求 * SearchSourceBuilder：条件构造 * QueryBuilder：查询 * HighlightBuilder：高亮 * @author cooper * @date 2021/8/29 6:17 下午 */ @Test public void testSearch() throws IOException { SearchRequest searchRequest = new SearchRequest(INDEX); // 构建搜索条件 SearchSourceBuilder sourceBuilder = new SearchSourceBuilder(); // 查询条件，我们可以用QueryBuilder来实现 QueryBuilder queryBuilder= QueryBuilders.termQuery(&quot;name&quot;,&quot;cooper&quot;); sourceBuilder.query(queryBuilder); sourceBuilder.timeout(new TimeValue(60, TimeUnit.SECONDS)); searchRequest.source(sourceBuilder); SearchResponse response = restHighLevelClient.search(searchRequest, RequestOptions.DEFAULT); for (SearchHit hit : response.getHits().getHits()) { System.out.println(hit.getSourceAsMap()); } }}","link":"/es-springboot/"},{"title":"hive简介及安装","text":"hive是基于Hadoop的一个数据仓库工具，用来进行数据提取、转化、加载，这是一种可以存储、查询和分析存储在Hadoop中的大规模数据的机制。hive数据仓库工具能将结构化的数据文件映射为一张数据库表，并提供SQL查询功能，能将SQL语句转变成MapReduce任务来执行。Hive的优点是学习成本低，可以通过类似SQL语句实现快速MapReduce统计，使MapReduce变得更加简单，而不必开发专门的MapReduce应用程序。hive十分适合对数据仓库进行统计分析。 hive 的特点 它存储架构在一个数据库中并处理数据到HDFS。 它是专为OLAP设计。 它提供SQL类型语言查询叫HiveQL或HQL。 它是熟知，快速，可扩展和可扩展的 hive的架构 单元名称 操作 用户接口/界面 User InterFaces Hive是一个数据仓库基础工具软件，可以创建用户和HDFS之间互动。用户界面，Hive支持是Hive的Web UI，Hive命令行，HiveHD洞察（在Windows服务器）。 Meta Store 元存储 Hive选择各自的数据库服务器，用以储存表，数据库，列模式或元数据表，它们的数据类型和HDFS映射。 HiveQL处理引擎 HiveQL类似于SQL的查询上Metastore模式信息。这是传统的方式进行MapReduce程序的替代品之一。相反，使用Java编写的MapReduce程序，可以编写为MapReduce工作，并处理它的查询。 执行引擎 HiveQL处理引擎和MapReduce的结合部分是由Hive执行引擎。执行引擎处理查询并产生结果和MapReduce的结果一样。它采用MapReduce方法 HDFS或者HBASE 数据存储 Hadoop的分布式文件系统或者HBASE数据存储技术是用于将数据存储到文件系统。 hive的安装（mac版本） 环境 操作系统：macOS Big Sur版本：11.2.3","link":"/bigdata-hive1/"},{"title":"curl命令详解与实战","text":"curl 是常用的命令行工具，用来请求 Web 服务器。它的名字就是客户端（client）的 URL 工具的意思。 它的功能非常强大，命令行参数多达几十种。如果熟练的话，完全可以取代 Postman 这一类的图形界面工具。 今天我总结一下，以备不时之需。 curl命令大全 不带任何参数就是get命令 curl https://www.example.com -X指定请求方式 curl -X POST https://www.example.comcurl -X GET https://www.example.com -d 参数用于发送POST请求的数据体 curl -d'login=emma＆password=123' -X POST https://google.com/login -G参数用来构造 URL 的查询字符串。 curl -G -d 'q=kitties' -d 'count=20' https://google.com/search 上面命令会发出一个 GET 请求，实际请求的 URL 为https://google.com/search?q=kitties&amp;count=20。如果省略--G，会发出一个 POST 请求。 如果数据需要 URL 编码，可以结合--data--urlencode参数。 curl -G --data-urlencode 'comment=hello world' https://www.example.com -H参数添加 HTTP 请求的标头。 curl -d '{&quot;login&quot;: &quot;emma&quot;, &quot;pass&quot;: &quot;123&quot;}' -H 'Content-Type: application/json' https://google.com/login -i参数打印出服务器回应的 HTTP 标头并打印返回信息。 curl -i https://www.example.com -I 只打印服务器回应的HTTP标头 curl -I www.baidu.com -F参数用来向服务器上传二进制文件。 curl -F 'file=@photo.png' https://google.com/profile 上面命令会给 HTTP 请求加上标头Content-Type: multipart/form-data，然后将文件photo.png作为file字段上传。 curl -F 'file=@photo.png;type=image/png' https://google.com/profile 上面的-F参数可以指定 MIME 类型 curl -F 'file=@photo.png;filename=me.png' https://google.com/profile 上面命令中，原始文件名为photo.png，但是服务器接收到的文件名为me.png -o参数将服务器的回应保存成文件，等同于wget命令 curl -o example.html https://www.example.com -O参数将服务器回应保存成文件，并将 URL 的最后部分当作文件名 curl -O https://www.example.com/foo/bar.html","link":"/curl-command/"},{"title":"docker基本命令","text":"本文将介绍一些docker中的命令与目前过程中的一些技巧,这是自己在学习过程中的一些积累，记录下来，方便查询。 Docker 命令大全帮助命令docker version --显示docker的版本号docker info --显示docker的详细信息docker &lt;命令行&gt; --help 帮助命令 docker命令行地址：https://docs.docker.com/engine/reference/run/ 镜像命令 docker images 列出镜像 ➜ ~ docker images --helpUsage: docker images [OPTIONS] [REPOSITORY[:TAG]]List imagesOptions: -a, --all Show all images (default hides intermediate images) --digests Show digests -f, --filter filter Filter output based on conditions provided --format string Pretty-print images using a Go template --no-trunc Don't truncate output -q, --quiet Only show image IDs➜ ~ docker imagesREPOSITORY TAG IMAGE ID CREATED SIZEmysql latest ab2f358b8612 4 months ago 545MBzookeeper latest ea93faa92337 4 months ago 253MBredis latest 84c5f6e03bf0 7 months ago 104MBnginx latest 7e4d58f0e5f3 7 months ago 133MB# 解释REPOSITORY：镜像的仓库源TAG：镜像标签IMAGE ID：镜像idCREATED：创建时间SIZE：镜像大小 docker search 搜索镜像 ➜ ~ docker search --helpUsage: docker search [OPTIONS] TERMSearch the Docker Hub for imagesOptions: -f, --filter filter Filter output based on conditions provided --format string Pretty-print search using a Go template --limit int Max number of search results (default 25) --no-trunc Don't truncate output➜ ~ docker search mysqlNAME DESCRIPTION STARS OFFICIAL AUTOMATEDmysql MySQL is a widely used, open-source relation… 10743 [OK]mariadb MariaDB Server is a high performing open sou… 4046 [OK]mysql/mysql-server Optimized MySQL Server Docker images. Create… 790 [OK]percona Percona Server is a fork of the MySQL relati… 532 [OK]centos/mysql-57-centos7 MySQL 5.7 SQL database server 87mysql/mysql-cluster Experimental MySQL Cluster Docker images. Cr… 81centurylink/mysql Image containing mysql. Optimized to be link… 59 [OK]bitnami/mysql Bitnami MySQL Docker Image 50 [OK]databack/mysql-backup Back up mysql databases to... anywhere! 42deitch/mysql-backup REPLACED! Please use http://hub.docker.com/r… 41 [OK]prom/mysqld-exporter 37 [OK]tutum/mysql Base docker image to run a MySQL database se… 35schickling/mysql-backup-s3 Backup MySQL to S3 (supports periodic backup… 29 [OK]linuxserver/mysql A Mysql container, brought to you by LinuxSe… 27centos/mysql-56-centos7 MySQL 5.6 SQL database server 20circleci/mysql MySQL is a widely used, open-source relation… 20mysql/mysql-router MySQL Router provides transparent routing be… 19arey/mysql-client Run a MySQL client from a docker container 17 [OK]fradelg/mysql-cron-backup MySQL/MariaDB database backup using cron tas… 12 [OK]yloeffler/mysql-backup This image runs mysqldump to backup data usi… 7 [OK]openshift/mysql-55-centos7 DEPRECATED: A Centos7 based MySQL v5.5 image… 6devilbox/mysql Retagged MySQL, MariaDB and PerconaDB offici… 3ansibleplaybookbundle/mysql-apb An APB which deploys RHSCL MySQL 2 [OK]jelastic/mysql An image of the MySQL database server mainta… 1widdpim/mysql-client Dockerized MySQL Client (5.7) including Curl… 1 [OK]➜ ~ docker search --filter=stars=3000 mysqlNAME DESCRIPTION STARS OFFICIAL AUTOMATEDmysql MySQL is a widely used, open-source relation… 10743 [OK]mariadb MariaDB Server is a high performing open sou… 4046 [OK]# 重要的可以filterdocker search --filter=stars=3000 mysql docker pull 拉取 ➜ ~ docker pull mysqlUsing default tag: latest# docker 默认拉取最新的版本号latest: Pulling from library/mysqlf7ec5a41d630: Pull complete9444bb562699: Pull complete6a4207b96940: Pull complete181cefd361ce: Pull complete8a2090759d8a: Pull complete15f235e0d7ee: Pull completed870539cd9db: Pull complete5726073179b6: Pull completeeadfac8b2520: Pull completef5936a8c3f2b: Pull completecca8ee89e625: Pull complete6c79df02586a: Pull completeDigest: sha256:6e0014cdd88092545557dee5e9eb7e1a3c84c9a14ad2418d5f2231e930967a38Status: Downloaded newer image for mysql:latestdocker.io/library/mysql:latest➜ ~ docker pull mysql:5.7# 指定版本号，拉取版本号的镜像5.7: Pulling from library/mysqlf7ec5a41d630: Already exists9444bb562699: Already exists6a4207b96940: Already exists181cefd361ce: Already exists8a2090759d8a: Already exists15f235e0d7ee: Already existsd870539cd9db: Already exists## docker采用分层的思想，把一个完整的镜像拆分，如果之前下载过某些分层，则不需要重新下载7310c448ab4f: Pull complete4a72aac2e800: Pull completeb1ab932f17c4: Pull complete1a985de740ee: Pull completeDigest: sha256:e42a18d0bd0aa746a734a49cbbcc079ccdf6681c474a238d38e79dc0884e0eccStatus: Downloaded newer image for mysql:5.7docker.io/library/mysql:5.7 docker rmi -f 删除镜像 ➜ ~ docker rmi --helpUsage: docker rmi [OPTIONS] IMAGE [IMAGE...]Remove one or more imagesOptions: -f, --force Force removal of the image --no-prune Do not delete untagged parents# 删除指定的镜像➜ ~docker rmi -f daaaaaaaa# 递归删除全部镜像➜ ~docker rmi -f $(docker images -aq) 容器命令有了镜像才能有容器 docker run 运行命令 ➜ ~ docker run --helpUsage: docker run [OPTIONS] IMAGE [COMMAND] [ARG...]Run a command in a new containerOptions: --add-host list Add a custom host-to-IP mapping (host:ip) -a, --attach list Attach to STDIN, STDOUT or STDERR --blkio-weight uint16 Block IO (relative weight), between 10 and 1000, or 0 to disable (default 0) --blkio-weight-device list Block IO weight (relative device weight) (default []) --cap-add list Add Linux capabilities --cap-drop list Drop Linux capabilities --cgroup-parent string Optional parent cgroup for the container --cgroupns string Cgroup namespace to use (host|private) 'host': Run the container in the Docker host's cgroup namespace 'private': Run the container in its own private cgroup namespace '': Use the cgroup namespace as configured by the default-cgroupns-mode option on the daemon (default) --cidfile string Write the container ID to the file --cpu-period int Limit CPU CFS (Completely Fair Scheduler) period --cpu-quota int Limit CPU CFS (Completely Fair Scheduler) quota --cpu-rt-period int Limit CPU real-time period in microseconds --cpu-rt-runtime int Limit CPU real-time runtime in microseconds -c, --cpu-shares int CPU shares (relative weight) --cpus decimal Number of CPUs --cpuset-cpus string CPUs in which to allow execution (0-3, 0,1) --cpuset-mems string MEMs in which to allow execution (0-3, 0,1) -d, --detach Run container in background and print container ID --detach-keys string Override the key sequence for detaching a container --device list Add a host device to the container --device-cgroup-rule list Add a rule to the cgroup allowed devices list --device-read-bps list Limit read rate (bytes per second) from a device (default []) --device-read-iops list Limit read rate (IO per second) from a device (default []) --device-write-bps list Limit write rate (bytes per second) to a device (default []) --device-write-iops list Limit write rate (IO per second) to a device (default []) --disable-content-trust Skip image verification (default true) --dns list Set custom DNS servers --dns-option list Set DNS options --dns-search list Set custom DNS search domains --domainname string Container NIS domain name --entrypoint string Overwrite the default ENTRYPOINT of the image -e, --env list Set environment variables --env-file list Read in a file of environment variables --expose list Expose a port or a range of ports --gpus gpu-request GPU devices to add to the container ('all' to pass all GPUs) --group-add list Add additional groups to join --health-cmd string Command to run to check health --health-interval duration Time between running the check (ms|s|m|h) (default 0s) --health-retries int Consecutive failures needed to report unhealthy --health-start-period duration Start period for the container to initialize before starting health-retries countdown (ms|s|m|h) (default 0s) --health-timeout duration Maximum time to allow one check to run (ms|s|m|h) (default 0s) --help Print usage -h, --hostname string Container host name --init Run an init inside the container that forwards signals and reaps processes -i, --interactive Keep STDIN open even if not attached --ip string IPv4 address (e.g., 172.30.100.104) --ip6 string IPv6 address (e.g., 2001:db8::33) --ipc string IPC mode to use --isolation string Container isolation technology --kernel-memory bytes Kernel memory limit -l, --label list Set meta data on a container --label-file list Read in a line delimited file of labels --link list Add link to another container --link-local-ip list Container IPv4/IPv6 link-local addresses --log-driver string Logging driver for the container --log-opt list Log driver options --mac-address string Container MAC address (e.g., 92:d0:c6:0a:29:33) -m, --memory bytes Memory limit --memory-reservation bytes Memory soft limit --memory-swap bytes Swap limit equal to memory plus swap: '-1' to enable unlimited swap --memory-swappiness int Tune container memory swappiness (0 to 100) (default -1) --mount mount Attach a filesystem mount to the container --name string Assign a name to the container --network network Connect a container to a network --network-alias list Add network-scoped alias for the container --no-healthcheck Disable any container-specified HEALTHCHECK --oom-kill-disable Disable OOM Killer --oom-score-adj int Tune host's OOM preferences (-1000 to 1000) --pid string PID namespace to use --pids-limit int Tune container pids limit (set -1 for unlimited) --platform string Set platform if server is multi-platform capable --privileged Give extended privileges to this container -p, --publish list Publish a container's port(s) to the host -P, --publish-all Publish all exposed ports to random ports --pull string Pull image before running (&quot;always&quot;|&quot;missing&quot;|&quot;never&quot;) (default &quot;missing&quot;) --read-only Mount the container's root filesystem as read only --restart string Restart policy to apply when a container exits (default &quot;no&quot;) --rm Automatically remove the container when it exits --runtime string Runtime to use for this container --security-opt list Security Options --shm-size bytes Size of /dev/shm --sig-proxy Proxy received signals to the process (default true) --stop-signal string Signal to stop a container (default &quot;SIGTERM&quot;) --stop-timeout int Timeout (in seconds) to stop a container --storage-opt list Storage driver options for the container --sysctl map Sysctl options (default map[]) --tmpfs list Mount a tmpfs directory -t, --tty Allocate a pseudo-TTY --ulimit ulimit Ulimit options (default []) -u, --user string Username or UID (format: &lt;name|uid&gt;[:&lt;group|gid&gt;]) --userns string User namespace to use --uts string UTS namespace to use -v, --volume list Bind mount a volume --volume-driver string Optional volume driver for the container --volumes-from list Mount volumes from the specified container(s) -w, --workdir string Working directory inside the container 下面我们拿几个常用的参数解释 --name 指定运行容器的名字-d 后台方式-it 使用交互方式 进入容器查看-p 指定容器的端口-P 随机指定端口 测试一下 ➜ ~ docker run --name centos1 -itd 300e315adb2fc1c9a9a26dd33a6392487db1f2a7c960332eac309413a4fa82e9d9f30b4ec3fe➜ ~ docker psCONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMESc1c9a9a26dd3 300e315adb2f &quot;/bin/bash&quot; 5 seconds ago Up 4 seconds centos1➜ ~ docker exec -it centos1 /bin/bash[root@c1c9a9a26dd3 /]# lsbin dev etc home lib lib64 lost+found media mnt opt proc root run sbin srv sys tmp usr var[root@c1c9a9a26dd3 /]# pwd/[root@c1c9a9a26dd3 /]# whereis nginxnginx: docker ps 列出运行的容器 ➜ ~ docker psCONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMESc1c9a9a26dd3 300e315adb2f &quot;/bin/bash&quot; 21 hours ago Up 21 hours centos1➜ ~ docker ps -aCONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMESc1c9a9a26dd3 300e315adb2f &quot;/bin/bash&quot; 21 hours ago Up 21 hours centos1b20cb371f580 d1165f221234 &quot;/hello&quot; 4 days ago Exited (0) 4 days ago boring_lederberg056de2c653a8 ab2f358b8612 &quot;docker-entrypoint.s…&quot; 3 months ago Exited (255) 3 months ago 0.0.0.0:3306-&gt;3306/tcp, 33060/tcp mysql5394243f1574 zookeeper:latest &quot;/docker-entrypoint.…&quot; 4 months ago Exited (255) 4 days ago 2888/tcp, 3888/tcp, 8080/tcp, 0.0.0.0:12181-&gt;2181/tcp zk154a9912b99e1 redis:latest &quot;docker-entrypoint.s…&quot; 6 months ago Exited (255) 6 months ago 0.0.0.0:36379-&gt;6379/tcp redis-slaver22ce7cec87780 redis:latest &quot;docker-entrypoint.s…&quot; 6 months ago Exited (255) 3 months ago 0.0.0.0:16379-&gt;6379/tcp redis-master51606ec8b03a redis:latest &quot;docker-entrypoint.s…&quot; 6 months ago Exited (255) 5 weeks ago 0.0.0.0:26379-&gt;6379/tcp redis-slaver14113a977a62f nginx:latest &quot;/docker-entrypoint.…&quot; 6 months ago Exited (255) 6 months ago 0.0.0.0:80-&gt;80/tcp nginx-load-balancec99db3188f9a 9b9cb95443b5 &quot;/bin/echo 'Hello wo…&quot; 6 months ago Exited (0) 6 months ago dreamy_shockley1522138f775a 9b9cb95443b5 &quot;/bin/echo 'Hello wo…&quot; 6 months ago Exited (0) 6 months ago pensive_cartwrightc82cd778a8bc 9b9cb95443b5 &quot;/bin/echo 'Hello wo…&quot; 6 months ago Exited (0) 6 months ago flamboyant_nightingale docker rm 容器id（删除容器） docker rm -f 容器iddocker rm 容器id 容器的其他命令 docker start 容器iddocker restart 重启容器iddocker stop 容器iddocker kill 容器id docker 查看日志 docker logs -tf -n 20 c1c9a9a26dd3 docker 查看元数据 ➜ Shell docker inspect centos1[ { &quot;Id&quot;: &quot;c1c9a9a26dd33a6392487db1f2a7c960332eac309413a4fa82e9d9f30b4ec3fe&quot;, &quot;Created&quot;: &quot;2021-04-14T14:41:47.9653797Z&quot;, &quot;Path&quot;: &quot;/bin/bash&quot;, &quot;Args&quot;: [], &quot;State&quot;: { &quot;Status&quot;: &quot;running&quot;, &quot;Running&quot;: true, &quot;Paused&quot;: false, &quot;Restarting&quot;: false, &quot;OOMKilled&quot;: false, &quot;Dead&quot;: false, &quot;Pid&quot;: 3193, &quot;ExitCode&quot;: 0, &quot;Error&quot;: &quot;&quot;, &quot;StartedAt&quot;: &quot;2021-04-16T09:00:21.3049137Z&quot;, &quot;FinishedAt&quot;: &quot;2021-04-16T01:39:43.8417964Z&quot; }, &quot;Image&quot;: &quot;sha256:300e315adb2f96afe5f0b2780b87f28ae95231fe3bdd1e16b9ba606307728f55&quot;, &quot;ResolvConfPath&quot;: &quot;/var/lib/docker/containers/c1c9a9a26dd33a6392487db1f2a7c960332eac309413a4fa82e9d9f30b4ec3fe/resolv.conf&quot;, &quot;HostnamePath&quot;: &quot;/var/lib/docker/containers/c1c9a9a26dd33a6392487db1f2a7c960332eac309413a4fa82e9d9f30b4ec3fe/hostname&quot;, &quot;HostsPath&quot;: &quot;/var/lib/docker/containers/c1c9a9a26dd33a6392487db1f2a7c960332eac309413a4fa82e9d9f30b4ec3fe/hosts&quot;, &quot;LogPath&quot;: &quot;/var/lib/docker/containers/c1c9a9a26dd33a6392487db1f2a7c960332eac309413a4fa82e9d9f30b4ec3fe/c1c9a9a26dd33a6392487db1f2a7c960332eac309413a4fa82e9d9f30b4ec3fe-json.log&quot;, &quot;Name&quot;: &quot;/centos1&quot;, &quot;RestartCount&quot;: 0, &quot;Driver&quot;: &quot;overlay2&quot;, &quot;Platform&quot;: &quot;linux&quot;, &quot;MountLabel&quot;: &quot;&quot;, &quot;ProcessLabel&quot;: &quot;&quot;, &quot;AppArmorProfile&quot;: &quot;&quot;, &quot;ExecIDs&quot;: null, &quot;HostConfig&quot;: { &quot;Binds&quot;: null, &quot;ContainerIDFile&quot;: &quot;&quot;, &quot;LogConfig&quot;: { &quot;Type&quot;: &quot;json-file&quot;, &quot;Config&quot;: {} }, &quot;NetworkMode&quot;: &quot;default&quot;, &quot;PortBindings&quot;: {}, &quot;RestartPolicy&quot;: { &quot;Name&quot;: &quot;no&quot;, &quot;MaximumRetryCount&quot;: 0 }, &quot;AutoRemove&quot;: false, &quot;VolumeDriver&quot;: &quot;&quot;, &quot;VolumesFrom&quot;: null, &quot;CapAdd&quot;: null, &quot;CapDrop&quot;: null, &quot;CgroupnsMode&quot;: &quot;host&quot;, &quot;Dns&quot;: [], &quot;DnsOptions&quot;: [], &quot;DnsSearch&quot;: [], &quot;ExtraHosts&quot;: null, &quot;GroupAdd&quot;: null, &quot;IpcMode&quot;: &quot;private&quot;, &quot;Cgroup&quot;: &quot;&quot;, &quot;Links&quot;: null, &quot;OomScoreAdj&quot;: 0, &quot;PidMode&quot;: &quot;&quot;, &quot;Privileged&quot;: false, &quot;PublishAllPorts&quot;: false, &quot;ReadonlyRootfs&quot;: false, &quot;SecurityOpt&quot;: null, &quot;UTSMode&quot;: &quot;&quot;, &quot;UsernsMode&quot;: &quot;&quot;, &quot;ShmSize&quot;: 67108864, &quot;Runtime&quot;: &quot;runc&quot;, &quot;ConsoleSize&quot;: [ 0, 0 ], &quot;Isolation&quot;: &quot;&quot;, &quot;CpuShares&quot;: 0, &quot;Memory&quot;: 0, &quot;NanoCpus&quot;: 0, &quot;CgroupParent&quot;: &quot;&quot;, &quot;BlkioWeight&quot;: 0, &quot;BlkioWeightDevice&quot;: [], &quot;BlkioDeviceReadBps&quot;: null, &quot;BlkioDeviceWriteBps&quot;: null, &quot;BlkioDeviceReadIOps&quot;: null, &quot;BlkioDeviceWriteIOps&quot;: null, &quot;CpuPeriod&quot;: 0, &quot;CpuQuota&quot;: 0, &quot;CpuRealtimePeriod&quot;: 0, &quot;CpuRealtimeRuntime&quot;: 0, &quot;CpusetCpus&quot;: &quot;&quot;, &quot;CpusetMems&quot;: &quot;&quot;, &quot;Devices&quot;: [], &quot;DeviceCgroupRules&quot;: null, &quot;DeviceRequests&quot;: null, &quot;KernelMemory&quot;: 0, &quot;KernelMemoryTCP&quot;: 0, &quot;MemoryReservation&quot;: 0, &quot;MemorySwap&quot;: 0, &quot;MemorySwappiness&quot;: null, &quot;OomKillDisable&quot;: false, &quot;PidsLimit&quot;: null, &quot;Ulimits&quot;: null, &quot;CpuCount&quot;: 0, &quot;CpuPercent&quot;: 0, &quot;IOMaximumIOps&quot;: 0, &quot;IOMaximumBandwidth&quot;: 0, &quot;MaskedPaths&quot;: [ &quot;/proc/asound&quot;, &quot;/proc/acpi&quot;, &quot;/proc/kcore&quot;, &quot;/proc/keys&quot;, &quot;/proc/latency_stats&quot;, &quot;/proc/timer_list&quot;, &quot;/proc/timer_stats&quot;, &quot;/proc/sched_debug&quot;, &quot;/proc/scsi&quot;, &quot;/sys/firmware&quot; ], &quot;ReadonlyPaths&quot;: [ &quot;/proc/bus&quot;, &quot;/proc/fs&quot;, &quot;/proc/irq&quot;, &quot;/proc/sys&quot;, &quot;/proc/sysrq-trigger&quot; ] }, &quot;GraphDriver&quot;: { &quot;Data&quot;: { &quot;LowerDir&quot;: &quot;/var/lib/docker/overlay2/274eb8c16a6e5e6ac1a68fb60adc3835a4afa20ed7882daac3b0fd928ab5d405-init/diff:/var/lib/docker/overlay2/7963fffc28e92ac68cf658653fffa8d5c6104941d91985b5ebda6a0c78cea3fd/diff&quot;, &quot;MergedDir&quot;: &quot;/var/lib/docker/overlay2/274eb8c16a6e5e6ac1a68fb60adc3835a4afa20ed7882daac3b0fd928ab5d405/merged&quot;, &quot;UpperDir&quot;: &quot;/var/lib/docker/overlay2/274eb8c16a6e5e6ac1a68fb60adc3835a4afa20ed7882daac3b0fd928ab5d405/diff&quot;, &quot;WorkDir&quot;: &quot;/var/lib/docker/overlay2/274eb8c16a6e5e6ac1a68fb60adc3835a4afa20ed7882daac3b0fd928ab5d405/work&quot; }, &quot;Name&quot;: &quot;overlay2&quot; }, &quot;Mounts&quot;: [], &quot;Config&quot;: { &quot;Hostname&quot;: &quot;c1c9a9a26dd3&quot;, &quot;Domainname&quot;: &quot;&quot;, &quot;User&quot;: &quot;&quot;, &quot;AttachStdin&quot;: false, &quot;AttachStdout&quot;: false, &quot;AttachStderr&quot;: false, &quot;Tty&quot;: true, &quot;OpenStdin&quot;: true, &quot;StdinOnce&quot;: false, &quot;Env&quot;: [ &quot;PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin&quot; ], &quot;Cmd&quot;: [ &quot;/bin/bash&quot; ], &quot;Image&quot;: &quot;300e315adb2f&quot;, &quot;Volumes&quot;: null, &quot;WorkingDir&quot;: &quot;&quot;, &quot;Entrypoint&quot;: null, &quot;OnBuild&quot;: null, &quot;Labels&quot;: { &quot;org.label-schema.build-date&quot;: &quot;20201204&quot;, &quot;org.label-schema.license&quot;: &quot;GPLv2&quot;, &quot;org.label-schema.name&quot;: &quot;CentOS Base Image&quot;, &quot;org.label-schema.schema-version&quot;: &quot;1.0&quot;, &quot;org.label-schema.vendor&quot;: &quot;CentOS&quot; } }, &quot;NetworkSettings&quot;: { &quot;Bridge&quot;: &quot;&quot;, &quot;SandboxID&quot;: &quot;49205c904c996ac8534688930c458ed478263ec01a9553322f3f9bfc53502631&quot;, &quot;HairpinMode&quot;: false, &quot;LinkLocalIPv6Address&quot;: &quot;&quot;, &quot;LinkLocalIPv6PrefixLen&quot;: 0, &quot;Ports&quot;: {}, &quot;SandboxKey&quot;: &quot;/var/run/docker/netns/49205c904c99&quot;, &quot;SecondaryIPAddresses&quot;: null, &quot;SecondaryIPv6Addresses&quot;: null, &quot;EndpointID&quot;: &quot;54be23afbd52e46a0d8570f6f4ff68b2b2b33f186728c161baa1f6f0b3bfa9db&quot;, &quot;Gateway&quot;: &quot;172.17.0.1&quot;, &quot;GlobalIPv6Address&quot;: &quot;&quot;, &quot;GlobalIPv6PrefixLen&quot;: 0, &quot;IPAddress&quot;: &quot;172.17.0.2&quot;, &quot;IPPrefixLen&quot;: 16, &quot;IPv6Gateway&quot;: &quot;&quot;, &quot;MacAddress&quot;: &quot;02:42:ac:11:00:02&quot;, &quot;Networks&quot;: { &quot;bridge&quot;: { &quot;IPAMConfig&quot;: null, &quot;Links&quot;: null, &quot;Aliases&quot;: null, &quot;NetworkID&quot;: &quot;6226106a822b92267889ded6757eab78f6165c65aa638e373c3ae472efe73c86&quot;, &quot;EndpointID&quot;: &quot;54be23afbd52e46a0d8570f6f4ff68b2b2b33f186728c161baa1f6f0b3bfa9db&quot;, &quot;Gateway&quot;: &quot;172.17.0.1&quot;, &quot;IPAddress&quot;: &quot;172.17.0.2&quot;, &quot;IPPrefixLen&quot;: 16, &quot;IPv6Gateway&quot;: &quot;&quot;, &quot;GlobalIPv6Address&quot;: &quot;&quot;, &quot;GlobalIPv6PrefixLen&quot;: 0, &quot;MacAddress&quot;: &quot;02:42:ac:11:00:02&quot;, &quot;DriverOpts&quot;: null } } } }] docker 查看进程 docker top containt 进入当前的容器 docker exec -it centos1 /bin/bash# 方式二docker attach centos1 从容器中copy #从docker中往外拷贝文件docker cp 容器id:/test.txt ./ 制作自己的容器 ➜ Shell docker commit -m='我自己的centos' -a='mrhy' c1c9a9a26dd3 centos-my:1.0sha256:c3349319374d0630c0a70afa4a3bf7de8c31385f40e60b145d786eb1969e97d6➜ Shell docker imagesREPOSITORY TAG IMAGE ID CREATED SIZEcentos-my 1.0 c3349319374d 2 minutes ago 209MBmysql 5.7 450379344707 7 days ago 449MBmysql latest cbe8815cbea8 7 days ago 546MBcentos latest 300e315adb2f 4 months ago 209MBzookeeper latest ea93faa92337 4 months ago 253MBredis latest 84c5f6e03bf0 7 months ago 104MBnginx latest 7e4d58f0e5f3 7 months ago 133MBbde2020/hive latest a65dc394c508 2 years ago 1.17GB docker 挂载卷 docker run -d -p 3306:3306 --name mysql-master -v=/Users/mrhy/environment/mysql/master/config/my.cnf:/etc/my.cnf -v=/Users/mrhy/environment/mysql/master/data:/var/lib/mysql -e MYSQL_ROOT_PASSWORD=*** --privileged=true mysql:5.7 -v 指的挂载当前文件: 容器内部文件 DockerFiledockerFile 核心是用来构建docker的镜像的文件，是命令参数脚本 构建步骤 编写一个dockerfile文件 docker build 构建成为一个镜像 docker run 运行镜像 发布到dockerhub 或者阿里云 Dockerfile构建过程基础知识 每个指令都必须是大写字母 执行从上到下的顺序 #表示注释 每一个指令都会创建一个新的镜像层，并提交 dockerfile是面向开发的 DockerFile：构建文件，定义了一切的步骤。 DockerImages：通过DockerFile构建出来的镜像 DockerContainer：Docker镜像的实例 DockerFile的指令 todo 补全注释 FROM #基础镜像，一切开始的源头MAINTAINER #镜像是谁写的，姓名+邮箱RUN #镜像构建的时候需要运行的命令ADD #WORKDIR # VOLUME # 挂载的目录EXPOST # 保留端口配置CMD ENTRYPOINTONBUILDCOPYENV 实战1 Centos截取一张centos7的dockerfile，然后按照他的格式去写 在此基础上，写自己的dockerfile FROM centosMAINTAINER mrhy&lt;1923589540@qq.com&gt;ENV MYPATH /usr/localWORKDIR $MYPATHRUN yum -y install vimRUN yum -y install net-toolsEXPOSE 80CMD echo $MYPATHCMD echo &quot;----end----&quot;CMD /bin/bash 构建dockerfile镜像 docker build -f MyCentos7 -t mycentos:0.1 . #注意最后这个点，表示当前路径 构建容器 docker run -it mycentos:0.1 这样一个新的centos7的镜像和容器就诞生了 查看一个镜像的构建过程 ➜ ~ docker imagesREPOSITORY TAG IMAGE ID CREATED SIZEmycentos 0.1 46ccf0eb330b 9 minutes ago 292MByandex/clickhouse-server latest 47415eaf3a8d 5 days ago 683MBzookeeper latest 2e2f6a1661fd 3 weeks ago 270MBredis latest 08502081bff6 7 weeks ago 105MBdocker/desktop-kubernetes kubernetes-v1.21.2-cni-v0.8.5-critools-v1.17.0-debian a502c6d66bd7 8 weeks ago 299MBwurstmeister/kafka latest c3b059ede60e 2 months ago 507MB➜ ~ docker history 46ccf0eb330b #查看一个镜像的构建过程IMAGE CREATED CREATED BY SIZE COMMENT46ccf0eb330b 9 minutes ago CMD [&quot;/bin/sh&quot; &quot;-c&quot; &quot;/bin/bash&quot;] 0B buildkit.dockerfile.v0&lt;missing&gt; 9 minutes ago CMD [&quot;/bin/sh&quot; &quot;-c&quot; &quot;echo \\&quot;----end----\\&quot;&quot;] 0B buildkit.dockerfile.v0&lt;missing&gt; 9 minutes ago CMD [&quot;/bin/sh&quot; &quot;-c&quot; &quot;echo $MYPATH&quot;] 0B buildkit.dockerfile.v0&lt;missing&gt; 9 minutes ago EXPOSE map[80/tcp:{}] 0B buildkit.dockerfile.v0&lt;missing&gt; 9 minutes ago RUN /bin/sh -c yum -y install net-tools # bu… 14.4MB buildkit.dockerfile.v0&lt;missing&gt; 9 minutes ago RUN /bin/sh -c yum -y install vim # buildkit 68.1MB buildkit.dockerfile.v0&lt;missing&gt; 10 minutes ago WORKDIR /usr/local 0B buildkit.dockerfile.v0&lt;missing&gt; 10 minutes ago ENV MYPATH=/usr/local 0B buildkit.dockerfile.v0&lt;missing&gt; 10 minutes ago MAINTAINER mrhy&lt;1923589540@qq.com&gt; 0B buildkit.dockerfile.v0&lt;missing&gt; 8 months ago /bin/sh -c #(nop) CMD [&quot;/bin/bash&quot;] 0B&lt;missing&gt; 8 months ago /bin/sh -c #(nop) LABEL org.label-schema.sc… 0B&lt;missing&gt; 8 months ago /bin/sh -c #(nop) ADD file:bd7a2aed6ede423b7… 209MB 实战2 Tomcat 准备文件 创建Dockerfile FROM centosLABEL author=&quot;mrhy&lt;mrhy1996@qq.com&gt;&quot;COPY readme.txt /usr/local/readme.txtADD apache-tomcat-8.5.69.tar.gz /usr/local/ADD jdk-8u291-linux-x64.tar.gz /usr/local/RUN yum -y install vimENV MYPATH=/usr/local/WORKDIR ${MYPATH}ENV JAVA_HOME /usr/local/jdk1.8.0_291ENV CLASS_HOME $JAVA_HOME/lib/dt.jar:$JAVA_HOME/lib/tools.jarENV CATALINA_HOME /usr/local/apache-tomcat-8.5.69ENV CATALINA_BASH /usr/local/apache-tomcat-8.5.69ENV PATH $PATH:$JAVA_HOME/bin:$CATALINA_HOME/lib:$CATALINA_HOME/binEXPOSE 8080CMD /usr/local/apache-tomcat-8.5.69/bin/startup.sh &amp;&amp; tail -F /usr/local/apache-tomcat-8.5.69/logs/catalina.out 构建镜像 docker build -t mrhytomcat8:0.1 . 查看docker镜像 docker images 发现自己刚发布的镜像 生成容器 docker run -itd -p 9090:8080 -v /Users/cooper/Projects/Cooper/DockerFiles/Tomcat8/webapps:/usr/local/apache-tomcat-8.5.69/webapps -v /Users/cooper/Projects/Cooper/DockerFiles/Tomcat8/logs:/usr/local/apache-tomcat-8.5.69/logs --name mrhytomcat8 mrhytomcat8:0.1 访问 发布自己的DockerFile 地址：https://hub.docker.com/ 创建自己的账号 在我们的服务器上提交自己的镜像 登录完成后就能发布 docker push mrhy1996/mrhytomcat8:1.0 Docker网络理解Docker0Docker技巧 docker容器访问宿主机的地址 宿主机地址为：host.docker.internal","link":"/docker-command/"},{"title":"docker安装clickhouse","text":"今天抽时间记一下docker安装clickhouse的过程，希望下次能用到的时候直接cv，不用走弯路 安装clickhouse docker run -d --name clickhouse-server-1 --ulimit nofile=262144:262144 yandex/clickhouse-server 启动clickhouse docker start clickhouse-server-1 启用cli 进入交互模式 docker run -it --rm --link clickhouse-server-1:clickhouse-server yandex/clickhouse-client --host clickhouse-server","link":"/docker-clickhouse/"},{"title":"docker简介及安装","text":"什么是dockerDocker 是一个开源的应用容器引擎，基于 Go 语言并遵从 Apache2.0 协议开源。 Docker 可以让开发者打包他们的应用以及依赖包到一个轻量级、可移植的容器中，然后发布到任何流行的 Linux 机器上，也可以实现虚拟化。 容器是完全使用沙箱机制，相互之间不会有任何接口（类似 iPhone 的 app）,更重要的是容器性能开销极低。 docker与传统的虚拟机有什么区别 vm与docker框架，直观上来讲vm多了一层guest OS，同时Hypervisor会对硬件资源进行虚拟化，docker直接使用硬件资源，所以资源利用率相对docker低也是比较容易理解的 docker的安装与常见问题环境 操作系统：macos 10.15.6 窗口：iTerm2 工具包：homebrew 安装 打开iterm窗口 输入命令 brew cask install docker 等待docker安装完成 docker安装完成之后在启动台中会有docker启动项 打开docker，第一启动会有点慢，甚至可能会出现一直staring的现象，一会说一下解决办法 更换国内镜像 打开preferences-&gt;docker engine {&quot;experimental&quot;:true,&quot;debug&quot;:true,&quot;registry-mirrors&quot;:[ &quot;https://docker.mirrors.ustc.edu.cn&quot; ]} 重启docker docker初始化一直starting的解决办法点击preference，打开右上角小虫子形状的按钮，点击clean/purge data，稍等一会看问题是否可以解决，笔者是通过这种方式解决的问题","link":"/tool-docker-docker-install/"},{"title":"Jenkins的搭建与使用","text":"最近老是往服务器部署东西，每次都是scp之类的非常麻烦，于是决定研究一下jenkins，来解决自己的这个难题 1. Jenkins官网https://www.jenkins.io/ 2. 下载与安装https://www.jenkins.io/download/lts/macos/ 我的电脑是macos，找到了mac的安装方法，借助了brew，来安装 安装jenkins之前要记得安装openjdk11 brew install jenkins-lts 启动jenkins brew services start jenkins-lts 重启jenkins brew services restart jenkins-lts 升级jenkins brew upgrade jenkins-lts sendFile mmap","link":"/jenkins/"},{"title":"k8s的API","text":"今天我们一起来看一下k8s的api都有哪些内容 Kubernetes API具体官网请看：https://kubernetes.io/docs/concepts/overview/kubernetes-api/ API Server 是k8s控制面板的核心，API服务器公开了一个Http Api，允许最终用户、集群的不同部分和外部组件相互通信。它允许您查询和操作k8s中api对象的状态（Pod、命名空间、ConfigMap和事件） 大多数操作都可以通过 kubectl命令行界面或其他命令行工具执行，例如 kubeadm，后者又使用 API。但是，您也可以使用 REST 调用直接访问 API。 如果您正在使用 Kubernetes API 编写应用程序，请考虑使用其中一个客户端库。 关于API的我们暂时更新这些内容，之后补充API Kubelet：运行在cluster所有节点上，负责启动POD和容器； Kubeadm：用于初始化cluster的一个工具； Kubectl：kubectl是kubenetes命令行工具，通过kubectl可以部署和管理应用，查看各种资源，创建，删除和更新组件 Haproxy 和 keepalived","link":"/k8s-api/"},{"title":"k8s组件学习","text":"今天，我们看一下k8s的具体的组件有哪些，以及他的作用 官方参考文档地址：https://kubernetes.io/docs/concepts/overview/components/ Kubernetes组件当你部署k8s的时候，你会部署一个集群。集群中有众多组件组成，包含很多被称之为==nodes==的工作节点，每一个集群至少包含一个node节点。 ==node==中托管着很多pod。然后node和pod均由==control plane== 去管理，在生产环境中 control panel通常有很多，集群中通常也有很多node 下面我们详细介绍一下各个组件的作用 Control plane 组件Control plane 能够对整个集群做出全局的策略，比如任务调度，集群的检测和响应 Control plane可以在集群中的任何机器上运行。但是，为简单起见，设置脚本通常在同一台机器上启动所有控制平面组件，并且不在这台机器上运行用户容器 kube-apiserverapi server是k8s为了暴露api接口的一个组件，是control panel 的一个前端。 他被设计为可以横向拓展，所以你可以部署很多机器，并且可以在这些机器中做负载均衡。 etcd一致且高度可用的键值存储，用作 Kubernetes 的所有集群数据的后备存储。 kube-scheduler观察新创建并且未分配的pod，并且选择一个node去运行。 调度决策考虑的因素包括：个人和集体资源需求、硬件/软件/策略约束、亲和性和反亲和性规范、数据局部性、工作负载间干扰和截止日期。 individual and collective resource requirements, hardware/software/policy constraints, affinity and anti-affinity specifications, data locality, inter-workload interference, and deadlines. kube-controller-manager运行控制器进程的控制平面组件 从逻辑上讲，每个控制器都是一个单独的进程，但为了降低复杂性，它们都被编译成一个二进制文件并在一个进程中运行。 这些控制器的一些类型是： Node controller：负责在节点宕机时进行通知和响应。 Job controller：监视代表一次性任务的作业对象，然后创建 Pod 以运行这些任务以完成。 Endpoints controller：填充 Endpoints 对象（即加入 Services &amp; Pods）。 Service Account &amp; Token controllers：为新命名空间创建默认帐户和 API 访问令牌。 cloud-controller-manager一个 Kubernetes控制平面嵌入云特定控制逻辑的组件。云控制器管理器允许您将集群链接到云提供商的 API，并将与该云平台交互的组件与仅与您的集群交互的组件分开。 cloud-controller-manager 仅运行特定于您的云提供商的控制器。如果您在自己的场所运行 Kubernetes，或者在您自己 PC 内的学习环境中运行 Kubernetes，则集群没有云控制器管理器。 与 kube-controller-manager 一样，cloud-controller-manager 将几个逻辑上独立的控制循环组合成一个二进制文件，您可以将其作为单个进程运行。您可以水平扩展（运行多个副本）以提高性能或帮助容忍故障。 以下控制器可以具有云提供商依赖项： 节点控制器：用于检查云提供商以确定节点停止响应后是否已在云中删除 路由控制器：用于在底层云基础设施中设置路由 服务控制器：用于创建、更新和删除云提供商负载均衡器 Node Components节点组件在每个节点上运行，维护运行的 pod 并提供 Kubernetes 运行时环境。 kubelet运行在每个代理上的代理节点。在集群中，它确保容器正在运行pod ==kubelet 不管理不是由 Kubernetes 创建的容器。== kube-proxy 是一个网络代理，运行在集群中的每一个节点中，实现 Kubernetes 的一部分 服务概念。 Container runtimeAddons插件 DNS所有k8s集群都应该有域名系统 Web UI (Dashboard)dashboard 是用于k8s集群的通用、基于web的ui Container Resource Monitoring容器资源监视器 Cluster-level Logging集群级日志记录 插件COREDNS可以为集群中的svc创建一个域名Ip的对应关系解析 DashBoardui界面 INGRESS CONTROLLER官方只能实现四层代理，INGRESS 实现七层代理 FEDETATION提供一个可以跨集群中心的多k8s的统一管理系统","link":"/k8s-component/"},{"title":"什么是k8s","text":"从今天起，开始整理k8s的笔记，深入学习k8s，第一节，什么是k8s，以及为什么引入k8s。 k8s的官网地址为：https://kubernetes.io/docs/concepts/overview/what-is-kubernetes/ 什么是k8sKubernetes 是一个可移植、可扩展的开源平台，用于管理容器化工作负载和服务，有助于声明式配置和自动化。它拥有庞大且快速发展的生态系统。Kubernetes 服务、支持和工具广泛可用。 Kubernetes 这个名字来源于希腊语，意思是舵手或飞行员。K8s 作为一个缩写，是通过计算“K”和“s”之间的八个字母得出的。Google 于 2014 年开源了 Kubernetes 项目。Kubernetes 结合了 Google 超过 15 年大规模运行生产工作负载的经验以及来自社区的最佳创意和实践。 k8s的作用在了解这个问题之前，我们首先看一下早起架构的演变 传统的架构 早期，部署在物理服务器上运行应用程序。无法为物理服务器中的应用程序定义资源边界，这会导致资源分配问题。例如，如果多个应用程序在物理服务器上运行，则可能会出现一个应用程序占用大部分资源的情况，结果其他应用程序的性能就会不佳。一个解决方案是在不同的物理服务器上运行每个应用程序。但这并没有扩展，因为资源没有得到充分利用，而且组织维护许多物理服务器的成本很高。 虚拟机部署时代 这个时候看第二张图，在操作系统之上又引出了一个Hypervisor层，翻译过来就是管理层，管理层用来去管理在物理机上的各个虚拟机，虚拟机之间是独立的，不能相互访问，在虚拟机中部署的程序是相互独立的，这样就能解决上面的问题。但是这种部署，是比较重的，因为我们可以看到每一个虚拟机都要有一个操作系统 容器化部署时代 容器类似于虚拟机，但它们具有放松的隔离属性，可以在应用程序之间共享操作系统（OS）。因此，容器被认为是轻量级的。与 VM 类似，容器有自己的文件系统、CPU 份额、内存、进程空间等。由于它们与底层基础架构分离，因此它们可以跨云和操作系统分布移植。 容器之所以流行，是因为它们提供了额外的好处，例如： 敏捷的应用程序创建和部署：与使用 VM 映像相比，容器映像创建的简便性和效率更高。 持续开发、集成和部署：提供可靠且频繁的容器映像构建和部署以及快速高效的回滚（由于映像不变性）。 Dev 和 Ops 的关注点分离：在构建/发布时而不是部署时创建应用程序容器映像，从而将应用程序与基础架构解耦。 可观察性：不仅可以显示操作系统级别的信息和指标，还可以显示应用程序运行状况和其他信号。 开发、测试和生产之间的环境一致性：在笔记本电脑上运行与在云中运行相同。 云和操作系统分发可移植性：在 Ubuntu、RHEL、CoreOS、本地、主要公共云和其他任何地方运行。 以应用程序为中心的管理：将抽象级别从在虚拟硬件上运行操作系统提高到使用逻辑资源在操作系统上运行应用程序。 松散耦合、分布式、弹性、自由的微服务：应用程序被分解成更小的、独立的部分，并且可以动态部署和管理——而不是在一台大型单一用途机器上运行的单一堆栈。 资源隔离：可预测的应用程序性能。 资源利用：高效率、高密度。 了解完应用部署的架构演变之后，让我们再回头看一下，为什么需要k8s。 容器是捆绑和运行应用程序的好方法。在生产环境中，您需要管理运行应用程序的容器并确保没有停机。例如，如果一个容器出现故障，则需要启动另一个容器。如果这种行为由系统处理，会不会更容易？ 这就是 Kubernetes 来救援的方式！Kubernetes 为您提供了一个框架来弹性地运行分布式系统。它负责您的应用程序的扩展和故障转移，提供部署模式等等。例如，Kubernetes 可以轻松地为您的系统管理金丝雀部署。 Kubernetes 为您提供： 服务发现和负载平衡 Kubernetes 可以使用 DNS 名称或使用自己的 IP 地址公开容器。如果容器的流量很高，Kubernetes 能够负载均衡和分配网络流量，从而使部署稳定。 存储编排 Kubernetes 允许您自动挂载您选择的存储系统，例如本地存储、公共云提供商等。 自动推出和回滚 您可以使用 Kubernetes 描述已部署容器的所需状态，它可以以受控的速率将实际状态更改为所需状态。例如，您可以自动化 Kubernetes 为您的部署创建新容器、删除现有容器并将其所有资源用于新容器。 自动装箱 你为 Kubernetes 提供了一个节点集群，它可以用来运行容器化的任务。你告诉 Kubernetes 每个容器需要多少 CPU 和内存 (RAM)。Kubernetes 可以将容器安装到您的节点上，以充分利用您的资源。 自我修复 Kubernetes 会重新启动失败的容器、替换容器、杀死不响应用户定义的健康检查的容器，并且在它们准备好服务之前不会将它们通告给客户端。 秘密和配置管理 Kubernetes 允许您存储和管理敏感信息，例如密码、OAuth 令牌和 SSH 密钥。您可以部署和更新机密和应用程序配置，而无需重新构建容器映像，也无需在堆栈配置中公开机密。 k8s不是什么Kubernetes 不是一个传统的、包罗万象的 PaaS（平台即服务）系统。由于 Kubernetes 在容器级别而不是硬件级别运行，因此它提供了一些常见于 PaaS 产品的普遍适用的功能，例如部署、扩展、负载平衡，并允许用户集成他们的日志记录、监控和警报解决方案。然而，Kubernetes 并不是单一的，这些默认的解决方案是可选的和可插拔的。Kubernetes 为构建开发者平台提供了构建块，但在重要的地方保留了用户的选择和灵活性。 Kubernetes： 不限制支持的应用程序类型。Kubernetes 旨在支持极其多样化的工作负载，包括无状态、有状态和数据处理工作负载。如果应用程序可以在容器中运行，它应该在 Kubernetes 上运行良好。 不部署源代码，也不构建您的应用程序。持续集成、交付和部署 (CI/CD) 工作流程由组织文化和偏好以及技术要求决定。 不提供应用级服务，例如中间件（例如，消息总线）、数据处理框架（例如，Spark）、数据库（例如，MySQL）、缓存，也不提供集群存储系统（例如，Ceph）作为内置服务。此类组件可以在 Kubernetes 上运行，和/或可以由运行在 Kubernetes 上的应用程序通过可移植机制（例如Open Service Broker ）访问。 不规定日志记录、监控或警报解决方案。它提供了一些集成作为概念证明，以及收集和导出指标的机制。 不提供也不强制要求配置语言/系统（例如 Jsonnet）。它提供了一个声明性 API，可以针对任意形式的声明性规范。 不提供也不采用任何全面的机器配置、维护、管理或自我修复系统。 此外，Kubernetes 不仅仅是一个编排系统。事实上，它消除了编排的需要。编排的技术定义是执行已定义的工作流：首先执行 A，然后执行 B，然后执行 C。相比之下，Kubernetes 包含一组独立的、可组合的控制过程，它们不断地将当前状态驱动到提供的所需状态。从 A 到 C 的方式无关紧要。也不需要集中控制。这导致了一个更易于使用、更强大、更健壮、弹性和可扩展的系统。","link":"/k8s-introduce/"},{"title":"maven知识点","text":"今天整理一下maven的配置，希望自己所学的东西能够及时形成笔记. 基本信息&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;project xmlns=&quot;http://maven.apache.org/POM/4.0.0&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xsi:schemaLocation=&quot;http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd&quot;&gt; &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt; &lt;groupId&gt;com.mrhy&lt;/groupId&gt; &lt;artifactId&gt;mrhy-grow-up&lt;/artifactId&gt; &lt;packaging&gt;pom&lt;/packaging&gt; &lt;version&gt;1.0-SNAPSHOT&lt;/version&gt; &lt;modules&gt; &lt;module&gt;mrhy-common&lt;/module&gt; &lt;module&gt;mrhy-middleware&lt;/module&gt; &lt;/modules&gt; &lt;properties&gt; &lt;maven.compiler.source&gt;8&lt;/maven.compiler.source&gt; &lt;maven.compiler.target&gt;8&lt;/maven.compiler.target&gt; &lt;maven.compiler.source&gt;8&lt;/maven.compiler.source&gt; &lt;maven.compiler.target&gt;8&lt;/maven.compiler.target&gt; &lt;project.version&gt;1.0-SNAPSHOT&lt;/project.version&gt; &lt;spring.version&gt;2.5.1&lt;/spring.version&gt; &lt;clickhouse.version&gt;0.2.4&lt;/clickhouse.version&gt; &lt;lombok.version&gt;1.18.16&lt;/lombok.version&gt; &lt;commons-lang3.version&gt;3.12.0&lt;/commons-lang3.version&gt; &lt;fastjson.version&gt;1.2.58&lt;/fastjson.version&gt; &lt;mybatis.version&gt;2.2.0&lt;/mybatis.version&gt; &lt;disruptor.version&gt;3.4.2&lt;/disruptor.version&gt; &lt;swagger.version&gt;2.9.2&lt;/swagger.version&gt; &lt;commons-io.version&gt;2.10.0&lt;/commons-io.version&gt; &lt;redisson.version&gt;3.16.1&lt;/redisson.version&gt; &lt;/properties&gt; &lt;dependencyManagement&gt; &lt;dependencies&gt; &lt;!-- https://mvnrepository.com/artifact/org.springframework.boot/spring-boot-starter-parent --&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-parent&lt;/artifactId&gt; &lt;version&gt;${spring.version}&lt;/version&gt; &lt;type&gt;pom&lt;/type&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-dependencies&lt;/artifactId&gt; &lt;version&gt;${spring.version}&lt;/version&gt; &lt;type&gt;pom&lt;/type&gt; &lt;scope&gt;import&lt;/scope&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.mybatis.spring.boot&lt;/groupId&gt; &lt;artifactId&gt;mybatis-spring-boot-starter&lt;/artifactId&gt; &lt;version&gt;${mybatis.version}&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;com.alibaba&lt;/groupId&gt; &lt;artifactId&gt;fastjson&lt;/artifactId&gt; &lt;version&gt;${fastjson.version}&lt;/version&gt; &lt;/dependency&gt; &lt;!--lombok--&gt; &lt;dependency&gt; &lt;groupId&gt;org.projectlombok&lt;/groupId&gt; &lt;artifactId&gt;lombok&lt;/artifactId&gt; &lt;version&gt;${lombok.version}&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;ru.yandex.clickhouse&lt;/groupId&gt; &lt;artifactId&gt;clickhouse-jdbc&lt;/artifactId&gt; &lt;version&gt;${clickhouse.version}&lt;/version&gt; &lt;/dependency&gt; &lt;!-- 工具类--&gt; &lt;dependency&gt; &lt;groupId&gt;commons-io&lt;/groupId&gt; &lt;artifactId&gt;commons-io&lt;/artifactId&gt; &lt;version&gt;${commons-io.version}&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.commons&lt;/groupId&gt; &lt;artifactId&gt;commons-lang3&lt;/artifactId&gt; &lt;version&gt;${commons-lang3.version}&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;com.lmax&lt;/groupId&gt; &lt;artifactId&gt;disruptor&lt;/artifactId&gt; &lt;version&gt;${disruptor.version}&lt;/version&gt; &lt;/dependency&gt; &lt;!--swagger--&gt; &lt;dependency&gt; &lt;groupId&gt;io.springfox&lt;/groupId&gt; &lt;artifactId&gt;springfox-swagger2&lt;/artifactId&gt; &lt;version&gt;${swagger.version}&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;io.springfox&lt;/groupId&gt; &lt;artifactId&gt;springfox-swagger-ui&lt;/artifactId&gt; &lt;version&gt;${swagger.version}&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.redisson&lt;/groupId&gt; &lt;artifactId&gt;redisson&lt;/artifactId&gt; &lt;version&gt;${redisson.version}&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;com.mrhy&lt;/groupId&gt; &lt;artifactId&gt;mrhy-common&lt;/artifactId&gt; &lt;version&gt;${project.version}&lt;/version&gt; &lt;/dependency&gt; &lt;/dependencies&gt; &lt;/dependencyManagement&gt;&lt;/project&gt;","link":"/maven/"},{"title":"prometheus小记","text":"prometheus lables是什么时候赋值的 metrics几种主要类型 gauges（瞬时状态，没有规则的变化） counters（从0开始，理想状态下，不会降低） histograms scree 命令 prometheus 配置文件参数 scrape_interval 抓取采样数据的时间间隔 evaluation_interval 监控数据规则的评估频率 scrape_config 抓取设置","link":"/prometheus-learn/"},{"title":"","text":"MySql数据库的知识总结1. mysql数据库常用的存储引擎MyISAM、InnoDB、MERGE、MEMORY(HEAP)、BDB(BerkeleyDB)、EXAMPLE、FEDERATED、ARCHIVE、CSV、BLACKHOLE。 2. MyISAM 和InnoDB的区别 3. mysql 的架构图 特点：可插拔式 4. Mysql 索引导致性能下降的原因有 查询磁盘问题 top sql写的烂 索引失效(单值索引、复合索引) id name email weixinNumber 单值索引 create index idx_user_name on user(name); 复合索引 create index idx_user_name_email on user(name,email); 关联查询太多join（设计缺陷） 服务器调优及各个参数设置 常见的join查询 七种join理论 索引是什么？Mysql官方的定义为索引（Index）是帮助MySql高效获取数据的数据结构，本质就是数据结构 索引的目的就是为了提高查询效率，类似于字典 索引的创建方式 主键 alter table tableName add primary key(column_list); 唯一索引 alter table tableName add unique index index_name(column_list); 普通索引 alter table tableName add index index_name(column_list); 全文索引 alter table tableName add fullText index_name index_name(column_list); 哪些字段适合创建索引？主键自动创建索引频繁作为查询条件的应该作为索引查询中与其他表关联的字段大部分情况下选择组合索引查询中排序的字段查询中统计或者分组的字段 哪些情况不适合建索引表记录太少 300万经常增删改的列数据重复且分布平均的字段 索引的类型聚集索引、非聚集索引、聚簇索引、稀疏索引、稠密索引 聚集索引非聚集索引聚簇索引稀疏索引稠密索引5. mysql 性能分析explain关键字 能干嘛 表的读取顺序 数据读取的读取操作类型 哪些索引可以被使用 哪些索引实际被使用 表之间的引用 每张表有多少行被优化器查询 id 介绍 谁的顺序高，谁先执行，如果相同则从上到下执行 select_type介绍 simple：简单的select查询，不包含子查询或者union primary：查询中包含复杂的子部分，最外层被标记为 subquery：在select 或者where列表中包含子查询 derived：临时表 union：若第二个select出现在union之后，被标记为union，若union包含在from子句的子查询中，外层select奖杯标记为derived union result：从union表获取结果的select。 type介绍 all： 全表扫描 index：全索引扫描 range：只检索给定范围内的检索 ref：非唯一性索引扫描，返回匹配某个单独值的所有行。本质上也是一种索引访问，他返回所有匹配某个单独值的行。 eq_ref：唯一性索引扫描，对于每个索引建，表中只有一条记录与之匹配，常见主键或者唯一索引扫描 const：表示通过索引一次就找到了，const用于比较primary key 或者unique索引。因为只匹配一行数据，所以很快。 system ：表只有一行数据，const类型的特例，平时不会出现。 null 从好到差依次为：system&gt;const&gt;eq_ref&gt;ref&gt;range&gt;index&gt;all possible_keys key key_len possible_keys:显示可能应用到这张表中的索引，一个或多个，查询涉及到的字段上若存在索引，则该索引将被列出，但不一定被查询使用 key：实际使用的索引 key_len:表示索引中使用的字节数，可通过该列计算查询中使用的索引的长度，在不损失精确性的情况下，长度越短越好 rows 介绍 根据表统计信息及索引选用情况，大致估算出找出所需的记录所需要读取的行数。 extra介绍 Using filesort：文件内排序 using temporary：使用临时表 using index：表示不错 6.索引优化及失效的情况 最佳左前缀法则：1.带头大哥不能死2.中间兄弟不能断 ==小表驱动大表== Exists 或者in 的使用 如果 A小于b 则用 in select * from a where id=(select id from b) 如果 a表的数据集大约b，则用exists select * from a where exists (select 1 from b where a.id=b.id) ==为排序增加索引== 7.实际过程中分析sql 观察，至少跑一天，查看生产环境的慢sql 开启慢查询日志 explain+慢sql分析 show profile 运维经理或者dba性能调优 8. 慢sql日志查询 默认mysql数据库没有开启慢查询日志 配置一下 借助 mysqldumpslow去查看 9. Show profiles 使用（记录曾经执行过的sql）使用步骤 查询profiles 状态 show VARIABLES like ‘profiling%’ 开启profiles set profiling=on;(已经开启过的可以不开启) show profiles; 查询曾经使用过的sql show profile cpu,block io for query **(Query_id); 结论 如果sql中出现这4个问题，则表示问题有点大 10.全局查询日志==永远不用在生产环境启用这个功能== set global general_log =1; Set global log_output=’TABLE’; select * from mysql.general_log 11.Mysql锁机制(MyIsam)查看锁的状态 show open tables; unlock tables; 按照锁的类型来分 读锁（共享锁） 命令 lock table * read; 加上读锁，自己能写这个表，不能读写其他表，不能update这个表 其他 能读 不能写（死锁） 写锁（排他锁） 命令 lock table * write; 下面的table locks waited 越高，证明性能越不好。 12. mysql 事务 13. mysql 事务隔离级别 14. mysql 行锁15. 索引操作不当，行锁变成表锁16. 间隙锁 17. 如何锁定一行？for update 18 性能总结​ 19 mysql 主从复制 原理 slave会从master读取binlog日志来进行同步。 三步骤+原理图 步骤 修改主机配置文件 server-id=1; log-bin=自己本地路径/mysqlin 从机修改id server-id=2; log-bin=mysql-bin 重启服务 主机授权 grant replication slave, replication client on *.* to 'rep'@'192.168.159.%' identified by 'Mysql@123';flush privileges 查看主机状态 show master status; 从机配置 CHANGE MASTER TO MASTER_HOST='172.16.1.52', #这是主库的IP（域名也可以需要做解析）MASTER_PORT=3306, #主库的端口，从库端口和主库不可以相同MASTER_USER='rep', #这是主库上创建用来复制的用户repMASTER_PASSWORD='123456' #rep的密码MASTER_LOG_FILE='mysql-bin.000025', #这里是show masterstatus时看到的查询二进制日志文件名称，这里不能多空格MASTER_LOG_POS=9155; #这里是show master status时看到的二进制日志偏移量，不能多空格 开启从机复制 start slave ; 查看从机状态 show slave status\\G 如果Slave_IO_RUNING:YES 和 SLAVE_SQL_RUNING:YES 则证明配置成功; 停从机 stop slave;","link":"/undefined/"},{"title":"mysql主从复制","text":"","link":"/mysql-master-slaver-copy/"},{"title":"mysql常见的优化","text":"sql的优化主要是围绕着在查询语句的时候尽量使用索引避免全表扫描。 此贴记录一下mysql中常见的优化语句 使用索引 对查询进行优化，应尽量避免全表扫描，首先应考虑在 where 及 order by 涉及的列上建立索引。 避免判断null值 mysql 在使用is null 和is not null 的时候均不使用索引 不要使用%like mysql 在使用%like 的时候不会使用索引 复合索引要符合最左前缀法则(带头大哥不能死，中间兄弟不能断) 小表驱动大表 Exists 或者in 的使用 如果 A小于b 则用 in select * from a where id in (select id from b) 如果 a表的数据集大约b，则用exists select * from a where exists (select 1 from b where a.id=b.id) 不要使用!= 或者&lt;&gt; 尽量避免使用or select id from table where a=1 or b=2;# 修改为select id from table where a=1 union allselect id from table where b=2 常见的sql写法优化 # 大数据分页# 低性能SELECT c1,c2,cn… FROM table LIMIT n,m# 高性能select c1,c2,cn... from table where id in(select id from table limit n,m)# 尽量避免使用selec * # 注意 exist 和 in 使用的时机select num from a where num in(select num from b)#用下面的语句替换：select num from a where exists(select 1 from b where num=a.num)","link":"/mysql-optimize/"},{"title":"mysql知识总结","text":"","link":"/mysql-knowledge/"},{"title":"mysql索引","text":"","link":"/mysql-index/"},{"title":"","text":"redis 的数据类型 String字符串（可以为整形） hash （hash散列值 key必须唯一） list（实现队列，元素不唯一，先入先出原则） set （集合）各不相同 sort set（有序集合） 五大基本数据类型String########master:0&gt;set name majiju&quot;OK&quot;master:0&gt;get name&quot;majiju&quot;master:0&gt;exists name&quot;1&quot;master:0&gt;exists mingzi&quot;0&quot;master:0&gt;get name&quot;majiju&quot;master:0&gt;strlen name&quot;6&quot;master:0&gt;append name &quot;，hello&quot;&quot;14&quot;master:0&gt;get name&quot;majiju，hello&quot;##########master:0&gt;keys * #获取全部的key值 1) &quot;name&quot; 2) &quot;fans&quot;master:0&gt;get fans #获取fans 的值&quot;0&quot;master:0&gt;incr fans # 自增+1&quot;1&quot;master:0&gt;incr fans&quot;2&quot;master:0&gt;incr fans&quot;3&quot;master:0&gt;decr fans #自减 -1&quot;2&quot;master:0&gt;decr fans&quot;1&quot;master:0&gt;decr fans&quot;0&quot;master:0&gt;decr fans&quot;-1&quot;master:0&gt;decr fans&quot;-2&quot;master:0&gt;decr fans&quot;-3&quot;master:0&gt;incrby fans 9 #增加9&quot;6&quot;master:0&gt;decrby fans 2 # 减2&quot;4&quot;################ 字符串范围master:0&gt;getrange name 0 -1&quot;majiju，hello&quot;master:0&gt;getrange name 0 3&quot;maji&quot;################ 替换master:0&gt;setrange name 1 zhang&quot;14&quot;master:0&gt;get name&quot;mzhang，hello&quot;###############setex(set with expire) ##设置过期时间setnx(set if not exist) ## 不存在时设置master:0&gt;setex key1 30 &quot;zhanghongqin&quot;&quot;OK&quot;master:0&gt;ttl key1&quot;25&quot;master:0&gt;ttl key1&quot;22&quot;master:0&gt;ttl key1&quot;21&quot;master:0&gt;get key1&quot;zhanghongqin&quot;master:0&gt;setnx key2 majiju # 设置key2的值，第一次设置成功&quot;1&quot;master:0&gt;setnx key2 majiju1 # 设置 key2的值，第二次设置失败&quot;0&quot;master:0&gt;get key1nullmaster:0&gt;ttl key1&quot;-2&quot;#######################master:0&gt;mset k1 v1 k2 v2 k3 v3 #批量设置key value&quot;OK&quot;master:0&gt;keys * 1) &quot;k2&quot; 2) &quot;k1&quot; 3) &quot;k3&quot;master:0&gt;mget k1 k2 k3 #批量获取值 1) &quot;v1&quot; 2) &quot;v2&quot; 3) &quot;v3&quot;master:0&gt;msetnx k1 v2 k4 v4 # 批量setnx 原子性操作&quot;0&quot;###############对象master:0&gt;mset user:1:name &quot;majiju&quot; user:1:age 12&quot;OK&quot;master:0&gt;mget user:1:name user:1:age 1) &quot;majiju&quot; 2) &quot;12&quot;master:0&gt;set user:2 {name:majiju,age:12}&quot;OK&quot;master:0&gt;get user:2&quot;{name:majiju,age:12}&quot;############getset（获取前一个值，并设置下一个）master:0&gt;getset db redisnullmaster:0&gt;getset db mongo&quot;redis&quot; list（集合）# redis中可以把list玩成栈 队列 阻塞队列# 所有的list的命令都是l或者r开头的# list中左边为上面的#############################################lpushrpushmaster:0&gt;lpush list one &quot;1&quot;master:0&gt;lpush list two&quot;2&quot;master:0&gt;lpush list three&quot;3&quot;master:0&gt;lrange list 0 -1 #获取list中的 1) &quot;three&quot; 2) &quot;two&quot; 3) &quot;one&quot;master:0&gt;lrange list 0 1 1) &quot;three&quot; 2) &quot;two&quot;master:0&gt;rpush list xx&quot;4&quot;master:0&gt;lrange list 0 -1 1) &quot;three&quot; 2) &quot;two&quot; 3) &quot;one&quot; 4) &quot;xx&quot;#############################################lpoprpopmaster:0&gt;lpop list # 抛出头部&quot;three&quot;master:0&gt;rpop list # 抛出尾部&quot;xx&quot;master:0&gt;lrange list 0 -1 1) &quot;two&quot; 2) &quot;one&quot;#############################################lindex（通过下标去查询list中的元素）master:0&gt;lindex list 0&quot;two&quot;master:0&gt;lindex list 1&quot;one&quot;master:0&gt;lindex list 2null#############################################llen(查询redis中list的长度)master:0&gt;flushdb&quot;OK&quot;master:0&gt;lpush list one&quot;1&quot;master:0&gt;lpush list two&quot;2&quot;master:0&gt;lpush list three&quot;3&quot;master:0&gt;lpush list four&quot;4&quot;master:0&gt;llen list&quot;4&quot;#############################################lrem(移除指定的值)master:0&gt;lrem list 1 one&quot;1&quot;master:0&gt;lrem list 1 two&quot;1&quot;master:0&gt;lrem list 1 two&quot;0&quot;master:0&gt;lpush list three&quot;3&quot;master:0&gt;lrange list 0 -1 1) &quot;three&quot; 2) &quot;four&quot; 3) &quot;three&quot;master:0&gt;rpush list one&quot;4&quot;master:0&gt;lrem list 1 three&quot;1&quot;master:0&gt;lrange list 0 -1 1) &quot;four&quot; 2) &quot;three&quot; 3) &quot;one&quot;master:0&gt;rpush list one&quot;4&quot;master:0&gt;lrem list 2 one&quot;2&quot;#############################################ltrim(截取list的长度)master:0&gt;lpush list one &quot;1&quot;master:0&gt;lpush list one1&quot;2&quot;master:0&gt;lpush list one2&quot;3&quot;master:0&gt;lpush list one3&quot;4&quot;master:0&gt;ltrim list 0 1 # 通过下标截取指定的长度&quot;OK&quot;master:0&gt;lrange list 0 -1 1) &quot;one3&quot; 2) &quot;one2&quot;############################################## rpoplpush(移除列表的最后一个元素，并把它移动到新的列表中) master:0&gt;rpush list one&quot;1&quot;master:0&gt;rpush list one1&quot;2&quot;master:0&gt;rpush list one2&quot;3&quot;master:0&gt;rpush list one3&quot;4&quot;master:0&gt;rpoplpush list list1&quot;one3&quot;master:0&gt;lrange list 0 -1 1) &quot;one&quot; 2) &quot;one1&quot; 3) &quot;one2&quot;master:0&gt;lrange list1 0 -1 1) &quot;one3&quot;############################################## lset(更新数组中指定位置的值) master:0&gt;rpush list one1&quot;1&quot;master:0&gt;rpush list one2&quot;2&quot;master:0&gt;rpush list one3&quot;3&quot;master:0&gt;lrange list 0 -1 1) &quot;one1&quot; 2) &quot;one2&quot; 3) &quot;one3&quot;master:0&gt;lset list 0 one888 # 替换指定下标的值&quot;OK&quot;master:0&gt;lrange list 0 -1 1) &quot;one888&quot; 2) &quot;one2&quot; 3) &quot;one3&quot;master:0&gt;lset list 88 one888 # 如果不存在就会报错&quot;ERR index out of range&quot;##############################################linsert(在指定的某个值的前面或者后面插入)master:0&gt;lrange list 0 -1 1) &quot;one888&quot; 2) &quot;one2&quot; 3) &quot;one3&quot;master:0&gt;linsert list before one2 one1 #在one2的前面插入one1&quot;4&quot;master:0&gt;lrange list 0 -1 1) &quot;one888&quot; 2) &quot;one1&quot; 3) &quot;one2&quot; 4) &quot;one3&quot; master:0&gt;linsert list after one3 one5 # 在one3的后面插入one5&quot;5&quot;master:0&gt;lrange list 0 -1 1) &quot;one888&quot; 2) &quot;one1&quot; 3) &quot;one2&quot; 4) &quot;one3&quot; 5) &quot;one5&quot; ############################################## set（集合）set 集合中的值不能重复set中的命令都是以s开头的##############################################master:0&gt;sadd myset 'hello' # set中新增元素&quot;1&quot;master:0&gt;sadd myset 'tom'&quot;1&quot;master:0&gt;smembers myset #查看set集合中的成员 1) &quot;hello&quot; 2) &quot;tom&quot;master:0&gt;sismember myset hello #判断某个值是否是成员中的&quot;1&quot;master:0&gt;sismember myset world&quot;0&quot; master:0&gt;scard myset # 获取set集合中元素的个数&quot;2&quot;##############################################srem 移除指定集合中特定的元素master:0&gt;srem myset aa&quot;0&quot;master:0&gt;srem myset hello&quot;1&quot;master:0&gt;scard myset&quot;1&quot;master:0&gt;smembers myset 1) &quot;tom&quot;##############################################srandmember (随机获取set集合中某个元素)master:0&gt;smembers myset 1) &quot;zhanghua&quot; 2) &quot;lihua&quot; 3) &quot;lilei&quot; 4) &quot;tom&quot;master:0&gt;srandmember myset &quot;zhanghua&quot;master:0&gt;srandmember myset &quot;lilei&quot;master:0&gt;srandmember myset &quot;tom&quot;master:0&gt;srandmember myset &quot;tom&quot;master:0&gt;srandmember myset 2 #随机获取set集合中的2个元素 1) &quot;tom&quot; 2) &quot;lilei&quot;##############################################spop(随机弹出集合中的某个元素)master:0&gt;smembers myset 1) &quot;zhanghua&quot; 2) &quot;lihua&quot; 3) &quot;lilei&quot; 4) &quot;tom&quot;master:0&gt;spop myset&quot;lilei&quot;master:0&gt;smembers myset 1) &quot;zhanghua&quot; 2) &quot;lihua&quot; 3) &quot;tom&quot;##############################################smove (移动指定的值到另外一个集合中)master:0&gt;smembers myset 1) &quot;zhanghua&quot; 2) &quot;lihua&quot; 3) &quot;tom&quot;master:0&gt;smove myset myset2 tom # 移动指定的值到另外的集合中&quot;1&quot;master:0&gt;smembers myset 1) &quot;zhanghua&quot; 2) &quot;lihua&quot;master:0&gt;smembers myset2 1) &quot;tom&quot;##############################################微博 b站 的共同关注 set集合中也可以去做 - 交集 - 并集 - 差集master:0&gt;sadd key1 a&quot;1&quot;master:0&gt;sadd key1 b&quot;1&quot;master:0&gt;sadd key1 c&quot;1&quot;master:0&gt;sadd key2 c&quot;1&quot;master:0&gt;sadd key2 d&quot;1&quot;master:0&gt;sdiff key1 key2 # 取a集合与b集合的差集 1) &quot;a&quot; 2) &quot;b&quot;master:0&gt;sinter key1 key2 # 取a集合与b集合的交集 1) &quot;c&quot;master:0&gt;sunion key1 key2 # 取a集合与b集合的并集 1) &quot;a&quot; 2) &quot;c&quot; 3) &quot;b&quot; 4) &quot;d&quot;master:0&gt;sunion key1 key3 # key3 不存在 但是不会报错 1) &quot;a&quot; 2) &quot;c&quot; 3) &quot;b&quot;master:0&gt;sdiff key1 key3 1) &quot;a&quot; 2) &quot;c&quot; 3) &quot;b&quot;master:0&gt;sinter key1 key3 Hash（哈希）数据结构类似于 k-（k，v） 所有的hash的命令都是以h开头 本质和String没有太大区别 ########################################################master:0&gt;hset myhash name majiju # 往hash中set值&quot;1&quot;master:0&gt;hset myhash age 12 &quot;1&quot;master:0&gt;hget myhash name # 获取hash中某个值&quot;majiju&quot;master:0&gt;hget myhash age&quot;12&quot;master:0&gt;hgetall myhash # 获取 1) &quot;name&quot; 2) &quot;majiju&quot; 3) &quot;age&quot; 4) &quot;12&quot;master:0&gt;hmset myhash name zhanghongqin age 11&quot;OK&quot;master:0&gt;hget myhash name age&quot;ERR wrong number of arguments for 'hget' command&quot;master:0&gt;hmget myhash name age 1) &quot;zhanghongqin&quot; 2) &quot;11&quot;master:0&gt;hgetall myhash 1) &quot;name&quot; 2) &quot;zhanghongqin&quot; 3) &quot;age&quot; 4) &quot;11&quot; master:0&gt;hdel myhash name # 删除指定的key&quot;1&quot;master:0&gt;hgetall myhash 1) &quot;age&quot; 2) &quot;11&quot; ######################################################## hlen 查看有多少键值对 master:0&gt;hlen myhash&quot;1&quot; ######################################################## hexists(查看hash中是否存在指定的key) master:0&gt;hexists myhash age&quot;1&quot;master:0&gt;hexists myhash name&quot;0&quot;########################################################hkeys(查询hash中所有的key)hvals（查询hash中所有的value）master:0&gt;hkeys myhash 1) &quot;age&quot; 2) &quot;name&quot;master:0&gt;hvals myhash 1) &quot;11&quot; 2) &quot;majiju&quot;########################################################hincrby (hash中的decrby就是后面的步长变成负数)master:0&gt;hincrby myhash age 1&quot;12&quot;master:0&gt;hincrby myhash age 1&quot;13&quot;master:0&gt;hincrby myhash age -1&quot;12&quot;######################################################## hsetnx(set if not exist)master:0&gt;hsetnx myhash grade 2&quot;1&quot;master:0&gt;hsetnx myhash grade 2&quot;0&quot; Hash 中一般用键值对 比如用户信息 hash更适合对象的存储，string 更适合值的存储 Zset（有序集合）在set的基础上增加了一个值 set key1 v1 Zset k1 score v1 master:0&gt;zadd myset 1 one # 添加值&quot;1&quot;master:0&gt;zadd myset 2 two &quot;1&quot;master:0&gt;zadd myset 3 three 4 froue # 添加多个值&quot;2&quot;master:0&gt;zrange myset 0 -1 #查找值 1) &quot;one&quot; 2) &quot;two&quot; 3) &quot;three&quot; 4) &quot;froue&quot;########################################################zrangebyscore(给zset进行排序 -inf 表示负无穷 +inf 表示正无穷)master:0&gt;zrangebyscore myset -inf +inf # 升序排列 1) &quot;one&quot; 2) &quot;two&quot; 3) &quot;three&quot; 4) &quot;froue&quot;master:0&gt;zrangebyscore myset -inf +inf withscores # 带着key value 1) &quot;one&quot; 2) &quot;1&quot; 3) &quot;two&quot; 4) &quot;2&quot; 5) &quot;three&quot; 6) &quot;3&quot; 7) &quot;froue&quot; 8) &quot;4&quot; master:0&gt;ZREVRANGE myset 0 -1 # 从大到小排列 1) &quot;eight&quot; 2) &quot;six&quot; 3) &quot;froue&quot; 4) &quot;three&quot; 5) &quot;two&quot; ########################################################master:0&gt;zrange myset 0 -1 # 查询集合中的范围 1) &quot;one&quot; 2) &quot;two&quot; 3) &quot;three&quot; 4) &quot;froue&quot;master:0&gt;zrem myset one #移除&quot;1&quot;master:0&gt;zrange myset 0 -1 1) &quot;two&quot; 2) &quot;three&quot; 3) &quot;froue&quot;master:0&gt;zcard myset&quot;3&quot;########################################################zcount 查询指定区间的数量master:0&gt;zcount myset 6 88&quot;2&quot; 案例： 存储班级成绩 存储工资表排列 带权重执行 排行榜实现 三种特殊数据类型地理空间（geospatial）redis 在3.2版本推出的 朋友的定位、附近的人、打车距离 geoadd 规则：不能直接添加南北两极的数据 将指定的地理空间位置（经度、纬度、名称）添加到指定的key中 master:0&gt;geoadd china:city 116.405285 39.904989 beijing&quot;1&quot;master:0&gt;geoadd china:city 121.472644 31.231706 shanghai&quot;1&quot;master:0&gt;geoadd china:city 106.504962 29.533155 chongqing&quot;1&quot;master:0&gt;geoadd china:city 114.085947 22.547 shenzhen&quot;1&quot;master:0&gt;geoadd china:city 120.153576 30.287459 hangzhou 108.948024 34.263161 xian&quot;2&quot; geopos(获取指定的经度 纬度) master:0&gt;geopos china:city shenzhen hangzhou 1) 1) &quot;114.08594459295272827&quot; 2) &quot;22.54699993773966327&quot; 2) 1) &quot;120.15357345342636108&quot; 2) &quot;30.28745790721532671&quot; Geodist (两个人之间的指定距离) m 表示单位为米。 km 表示单位为千米。 mi 表示单位为英里。 ft 表示单位为英尺。 如果用户没有显式地指定单位参数， 那么 GEODIST 默认使用米作为单位。 GEODIST 命令在计算距离时会假设地球为完美的球形， 在极限情况下， 这一假设最大会造成 0.5% 的误差。 master:0&gt;geodist china:city beijing hangzhou # 查看北京到杭州的直线距离&quot;1122483.2754&quot;master:0&gt;geodist china:city beijing shanghai km&quot;1067.5980&quot; GEORADIUS 命令 - 以给定的经纬度为中心， 找出某一半径内的元素 master:0&gt;GEORADIUS china:city 110 30 1000 km 获取经度 longtitude 110 纬度 30 附近范围内 半径为1000km以内的城市 1) &quot;chongqing&quot; 2) &quot;xian&quot; 3) &quot;shenzhen&quot; 4) &quot;hangzhou&quot;master:0&gt;GEORADIUS china:city 110 30 1000 km withcoord count 1 #获取指定数量的城市 返回编码 1) 1) &quot;chongqing&quot; 2) 1) &quot;106.50495976209640503&quot; 2) &quot;29.53315530684997015&quot;master:0&gt;GEORADIUS china:city 110 30 1000 km withdist count 1 #显示到中心的距离 1) 1) &quot;chongqing&quot; 2) &quot;341.4052&quot; GEORADIUSBYMEMBER 找出位于指定范围内的元素，中心点是由给定的位置元素决定 master:0&gt;GEORADIUSBYMEMBEr china:city beijing 1000 km # 找出指定范围内的元素 1) &quot;beijing&quot; 2) &quot;xian&quot; Redis GEOHASH 命令 - 返回一个或多个位置元素的 Geohash 表示 # 将二维的经纬度转换为一维的字符串对象master:0&gt;geohash china:city beijing chongqing 1) &quot;wx4g0b7xrt0&quot; 2) &quot;wm78p86e170&quot; Geo 底层是基于zset 进行封装的，所以zset的命令可以使用在geo身上 master:0&gt;zrange china:city 0 -1 1) &quot;chongqing&quot; 2) &quot;xian&quot; 3) &quot;shenzhen&quot; 4) &quot;hangzhou&quot; 5) &quot;shanghai&quot; 6) &quot;beijing&quot;master:0&gt;zrem china:city chongqing&quot;1&quot;master:0&gt;zrange china:city 0 -1 1) &quot;xian&quot; 2) &quot;shenzhen&quot; 3) &quot;hangzhou&quot; 4) &quot;shanghai&quot; 5) &quot;beijing&quot; HyperLogLogRedis 在 2.8.9 版本添加了 HyperLogLog 结构。 Redis HyperLogLog 是用来做基数统计的算法，HyperLogLog 的优点是，在输入元素的数量或者体积非常非常大时，计算基数所需的空间总是固定 的、并且是很小的。 在 Redis 里面，每个 HyperLogLog 键只需要花费 12 KB 内存，就可以计算接近 2^64 个不同元素的基 数。这和计算基数时，元素越多耗费内存就越多的集合形成鲜明对比。 但是，因为 HyperLogLog 只会根据输入元素来计算基数，而不会储存输入元素本身，所以 HyperLogLog 不能像集合那样，返回输入的各个元素。 什么是基数? 比如数据集 {1, 3, 5, 7, 5, 7, 8}， 那么这个数据集的基数集为 {1, 3, 5 ,7, 8}, 基数(不重复元素)为5。 基数估计就是在误差可接受的范围内，快速计算基数。 master:0&gt;pfadd mykey a b c d e a # 添加元素&quot;1&quot;master:0&gt;pfcount mykey&quot;5&quot;master:0&gt;pfadd mykey2 a b e&quot;1&quot;master:0&gt;pfmerge mykey3 mykey mykey2 # 合并key1 key2 到mykey3&quot;OK&quot;master:0&gt;pfcount mykey3&quot;5&quot; 应用场景：统计网站的登录人数 BitMap 位存储,只有 0 1 两种状态 master:0&gt;setbit mykey 0 1&quot;0&quot;master:0&gt;setbit mykey 1 1&quot;0&quot;master:0&gt;setbit mykey 2 1&quot;0&quot;master:0&gt;setbit mykey 3 1&quot;0&quot;master:0&gt;setbit mykey 4 1&quot;0&quot;master:0&gt;setbit mykey 5 1&quot;0&quot;master:0&gt;setbit mykey 6 0&quot;0&quot;master:0&gt;getbit mykey 3&quot;1&quot;master:0&gt;bitcount mykey # 查看为1的数据&quot;6&quot; redis 事务redis事务本质：一组命令的集合（所有的事务都需要序列化） 一次性、顺序性、排他性 ------队列 set set set ---执行 –redis事务没有隔离的概念– redis事务 开启事务（multi） 命令入队 执行事务（exec） 正常执行事务 master:0&gt;multi #开启事务&quot;OK&quot;master:0&gt;set k1 v1&quot;QUEUED&quot;master:0&gt;set k2 v2&quot;QUEUED&quot;master:0&gt;get k2&quot;QUEUED&quot;master:0&gt;exec #执行事务 1) &quot;OK&quot; 2) &quot;OK&quot; 3) &quot;OK&quot; 4) &quot;OK&quot; 5) &quot;OK&quot; 6) &quot;v2&quot; 7) &quot;OK&quot; 取消事务 master:0&gt;multi&quot;OK&quot;master:0&gt;set k1 v1&quot;QUEUED&quot;master:0&gt;discard # 取消事务&quot;OK&quot;master:0&gt;get k1 null 事务错误 master:0&gt;multi&quot;OK&quot;master:0&gt;set key1 aa&quot;QUEUED&quot;master:0&gt;incr key1&quot;QUEUED&quot;master:0&gt;set key2 v2&quot;QUEUED&quot;master:0&gt;get key2&quot;QUEUED&quot;master:0&gt;exec 1) &quot;OK&quot; 2) &quot;OK&quot; 3) &quot;OK&quot; 4) &quot;ERR value is not an integer or out of range&quot; 5) &quot;OK&quot; 6) &quot;OK&quot; 7) &quot;OK&quot; 8) &quot;v2&quot; 9) &quot;OK&quot; 监控悲观锁 很悲观，认为什么时候都会出问题 乐观锁 乐观锁 很乐观 认为什么时候都不会出问题，所以不会加锁，更新的时候判断一下版本号 获取version 更新时比较version 正常执行 master:0&gt;set money 100&quot;OK&quot;master:0&gt;set out 0&quot;OK&quot;master:0&gt;watch money #监视money对象&quot;OK&quot;master:0&gt;multi&quot;OK&quot;master:0&gt;decrby money 20&quot;QUEUED&quot;master:0&gt;incrby out 20&quot;QUEUED&quot;master:0&gt;exec 1) &quot;OK&quot; 2) &quot;80&quot; 3) &quot;OK&quot; 4) &quot;20&quot; 5) &quot;OK&quot; 多线程异常执行 使用watch可以当做redis乐观锁 Redis持久化RDB(Redis data base) 是什么 rdb通俗一点来讲就是快照，类似于我们虚拟机中的快照功能，可以把某一时刻的状态全都保存下来。 优点 适合大规模的数据恢复。 如果业务对数据完整性和一致性要求不高，RDB是很好的选择。 缺点 数据的完整性和一致性不高，因为RDB可能在最后一次备份时宕机了。 备份时占用内存，因为Redis 在备份时会独立创建一个子进程，将数据写入到一个临时文件（此时内存中的数据是原来的两倍哦），最后再将临时文件替换之前的备份文件。所以Redis 的持久化和数据的恢复要选择在夜深人静的时候执行是比较合理的。 什么时候触发 达到执行条件 shutdown save bgsave flushall AOF(Append only File)Redis 默认不开启。它的出现是为了弥补RDB的不足（数据的不一致性），所以它采用日志的形式来记录每个写操作，并追加到文件中。Redis 重启的会根据日志文件的内容将写指令从前到后执行一次以完成数据的恢复工作。 从配置文件了解AOF打开 redis.conf 文件，找到 APPEND ONLY MODE 对应内容1 redis 默认关闭，开启需要手动把no改为yes appendonly yes 2 指定本地数据库文件名，默认值为 appendonly.aof appendfilename &quot;appendonly.aof&quot; 3 指定更新日志条件 # appendfsync alwaysappendfsync everysec# appendfsync no 解说：always：同步持久化，每次发生数据变化会立刻写入到磁盘中。性能较差当数据完整性比较好（慢，安全）everysec：出厂默认推荐，每秒异步记录一次（默认值）no：不同步 4 配置重写触发机制 auto-aof-rewrite-percentage 100auto-aof-rewrite-min-size 64mb 解说：当AOF文件大小是上次rewrite后大小的一倍且文件大于64M时触发。一般都设置为3G，64M太小了。 触发AOF快照根据配置文件触发，可以是每次执行触发，可以是每秒触发，可以不同步。 根据AOF文件恢复数据正常情况下，将appendonly.aof 文件拷贝到redis的安装目录的bin目录下，重启redis服务即可。但在实际开发中，可能因为某些原因导致appendonly.aof 文件格式异常，从而导致数据还原失败，可以通过命令redis-check-aof –fix appendonly.aof 进行修复 。从下面的操作演示中体会。 AOF的重写机制前面也说到了，AOF的工作原理是将写操作追加到文件中，文件的冗余内容会越来越多。所以聪明的 Redis 新增了重写机制。当AOF文件的大小超过所设定的阈值时，Redis就会对AOF文件的内容压缩。 重写的原理：Redis 会fork出一条新进程，读取内存中的数据，并重新写到一个临时文件中。并没有读取旧文件。最后替换旧的aof文件。 触发机制：当AOF文件大小是上次rewrite后大小的一倍且文件大于64M时触发。这里的“一倍”和“64M” 可以通过配置文件修改。 AOF 的优缺点优点：数据的完整性和一致性更高缺点：因为AOF记录的内容多，文件会越来越大，数据恢复也会越来越慢 Redis主从复制概念主从复制：是指将一台redis服务器上的数据复制到其他redis服务器，前者称为master（主）服务器，后者称为slaver（从）；数据库的复制是单向的，意思是：只能从主服务器复制到从服务器。一般来讲，主服务器负责写，从服务器负责读。 默认情况下，每一台redis都是一个主节点；且一个主节点可以有多个从节点，但是从节点只能有1个主节点 作用 数据冗余：主从复制实现了数据的热备份，是持久化的另外一种表现形式。 数据恢复：当主节点出现问题时，可以由从节点提供服务，实现快速的故障恢复 负载均衡：在主从复制的模式下，特别是针对于一些写少读多的场景，主节点负责写，多个从节点负责读，大大减轻了单个节点的压力。提高redis的并发量 高可用基石：除了上述的作用之外，主从复制还是哨兵和集群能够实施的基础 只要在公司中一定要进行主从复制 环境配置master:0&gt;info replication #查看当前服务器配置信息&quot;# Replicationrole:masterconnected_slaves:0master_replid:c5e8d7364f28b7b341759951496479915967b745master_replid2:0000000000000000000000000000000000000000master_repl_offset:0second_repl_offset:-1repl_backlog_active:0repl_backlog_size:1048576repl_backlog_first_byte_offset:0repl_backlog_histlen:0&quot; 子节点找老大 slaver1:0&gt;slaveof 127.0.0.1 6379 这种的主从配置是临时的，实际情况中应该按照配置文件的方式去配置，如果从机重启了，这时候重启从机就会立马变成主机，只要是变成从机，立马会同步 复制原理 从机连接到主机之后，发送一个sync信号到master，master收到信号之后立马同步一份数据传输到从机，从机拿到数据之后进行 全量复制，进行数据恢复，之后的数据都进行增量复制 模式一： 模式二： 以上两种在工作中都不会使用 如果没有老大了，这时候如何选择出老大呢？手动！(哨兵模式恢复之前) slaver1:0&gt;slaveof no one&quot;OK&quot;slaver1:0&gt;info replication&quot;# Replicationrole:masterconnected_slaves:0master_replid:b890a8a5cb5a6e7fc259c08b94106cd16496b9b6master_replid2:4d46c2b14ceb5fc1c4e43dc2b53ec6338aa8de68master_repl_offset:4966second_repl_offset:4967repl_backlog_active:1repl_backlog_size:1048576repl_backlog_first_byte_offset:1repl_backlog_histlen:4966 哨兵模式（自动选取老大的模式） 概念 主从切换技术的方法是：主机宕机之后需要手动选举出新的老大当做主机，费时费力。redis从2.8之后开始提供哨兵模式 配置哨兵文件sentinel.conf # sentinel monitor 被监控的名称 host port 1sentinel monitor redis-master 127.0.0.1 16379 1 后面的数字1，代表主机挂了，slave投票看让谁成为主机，票数最多的就会成为哨兵 缓存穿透和雪崩缓存穿透概念（查不到）用户想要查询一个值，先去redis中请求，发现redis中没有，也就是所谓的redis没有命中，然后往数据库去查询，数据库中也没有，，于是本次查询失败。当用户很多，并且有大量的这种请求的时候，会对持久化的数据库造成压力，这就相当于出现了缓存穿透。 解决方案 布隆过滤器 布隆过滤器是一种数据结构，使用起来也比较方便，只需要导入包即可 缓存空值 缓存击穿（针对于某个点集中攻击）解决方案 热点数据不过期 分布式锁 缓存雪崩（集体失效）解决方案 redis高可用 限流降级 数据预热","link":"/undefined/"},{"title":"redis集群模式","text":"","link":"/redis-cluster/"},{"title":"jvm类加载的过程","text":"架构师就是要脚踏实地，一步一步的往目标走去。今天先从jvm类加载过程开始学起，之前有过一些心得和理解，今天算是重温+复习 类加载的过程一个java的类加载到jvm内存中的过程，首类加载一共有如下几个步骤： 加载，验证，准备，解析，初始化，使用，卸载。 其中初始化和解析的顺序不固定，有如下四种情况必须对类进行初始化 遇到new、getstatic、putstatic、invokestatic这4条字节码指令时，如果类没有初始化，则先初始化。 反射调用时 初始化一个类时，如果父类还未初始化，则先触发父类初始化 当虚拟机启动时，需要制定一个要执行的主类，虚拟机要初始化这个主类 加载(Loading)加载阶段，需要完成的三件事情 通过一个类的全限定名来获取定义此类的二进制字节流 将该二进制流中的静态存储结构转化为方法去运行时数据结构 在内存中生成该类的 Class 对象，作为该类的数据访问入口 验证(Verification) 文件格式验证 验证字节流是否符合 Class 文件的规范，如主次版本号是否在当前虚拟机范围内，常量池中的常量是否有不被支持的类型 元数据验证 对字节码描述的信息进行语义分析，如这个类是否有父类，是否集成了不被继承的类等 字节码验证 是整个验证过程中最复杂的一个阶段，通过验证数据流和控制流的分析，确定程序语义是否正确，主要针对方法体的验证。如：方法中的类型转换是否正确，跳转指令是否正确等 符号引用验证 这个动作在后面的解析过程中发生，主要是为了确保解析动作能正确执行 准备(Preparation)准备阶段是为类的静态变量分配内存并将其初始化为默认值，这些内存都将在方法区中进行分配。准备阶段不分配类中的实例变量的内存，实例变量将会在对象实例化时随着对象一起分配在 Java 堆中 解析(Resolution)虚拟机将常量池内的符号引用替换为直接引用的过程 也就是说将堆内存内的静态变量符号修改为已经申请了空间的静态变量地址的过程 符号引用以一组符号来描述所引用的目标，符号可以是任何形式的字面量，只要使用时能够无歧义的定位到目标即可，使用符号引用时，被引用的目标不一定已经加载到内存中。 直接引用可以是直接指向目标的指针，相对偏移量，一个能间接定位到目标的句柄，使用直接引用时，引用的目标必定已经存在于虚拟机的内存中了。 ==为什么在解析阶段要符号引用转直接引用？== 个人理解，如果使用符号引用，虚拟机其实也不知道具体引用的类的内存地址，那么也就无法真正的调用到该类，所以要把符号引用转为直接引用，这样就能够真正定位到类在内存中的地址，如果符号引用转直接引用失败，就说明类还没有被加载到内存中，就会报错。 初始化(Initialization)初始化时类加载的最后一步，前面的类加载过程，除了在加载阶段用户应用程序可以通过自定义类加载器参与之外，其余动作完全由虚拟机主导和控制。到了初始化阶段，才真正开始执行类中定义的Java 程序代码 初始化阶段才真正开始执行类中定义的 Java 程序代码。初始化阶段是虚拟机执行类构造器 clinit() 方法的过程。在准备阶段，类变量已经赋过一次系统要求的初始值，而在初始化阶段，根据程序员通过程序制定的主观计划去初始化类变量和其它资源。 clinit()是由编译器自动收集类中所有类变量的赋值动作和静态语句块中的语句合并产生的，编译器收集的顺序由语句在源文件中出现的顺序决定。特别注意的是，静态语句块只能访问到定义在它之前的类变量，定义在它之后的类变量只能赋值，不能访问。 接口中不可以使用静态语句块，但仍然有类变量初始化的赋值操作，因此接口与类一样都会生成 clinit() 方法。但接口与类不同的是，执行接口的 clinit() 方法不需要先执行父接口的 clinit() 方法。只有当父接口中定义的变量使用时，父接口才会初始化。另外，接口的实现类在初始化时也一样不会执行接口的 clinit() 方法。 虚拟机会保证一个类的 clinit() 方法在多线程环境下被正确的加锁和同步，如果多个线程同时初始化一个类，只会有一个线程执行这个类的 clinit() 方法，其它线程都会阻塞等待，直到活动线程执行 clinit() 方法完毕。如果在一个类的 clinit() 方法中有耗时的操作，就可能造成多个线程阻塞 使用(Using)卸载(Unloading)","link":"/jvm-load/"},{"title":"单例模式","text":"单例模式（Singleton Pattern）是 Java 中最简单的设计模式之一。这种类型的设计模式属于创建型模式，它提供了一种创建对象的最佳方式。这种模式涉及到一个单一的类，该类负责创建自己的对象，同时确保只有单个对象被创建。这个类提供了一种访问其唯一的对象的方式，可以直接访问，不需要实例化该类的对象。 介绍意图：保证一个类仅有一个实例，并提供一个访问它的全局访问点。 主要解决：一个全局使用的类频繁地创建与销毁。 何时使用：当您想控制实例数目，节省系统资源的时候。 如何解决：判断系统是否已经有这个单例，如果有则返回，如果没有则创建。 关键代码：构造函数是私有的。 应用实例： 1、一个班级只有一个班主任。2、Windows 是多进程多线程的，在操作一个文件的时候，就不可避免地出现多个进程或线程同时操作一个文件的现象，所以所有文件的处理必须通过唯一的实例来进行。3、一些设备管理器常常设计为单例模式，比如一个电脑有两台打印机，在输出的时候就要处理不能两台打印机打印同一个文件。优点： 1、在内存里只有一个实例，减少了内存的开销，尤其是频繁的创建和销毁实例（比如管理学院首页页面缓存）。2、避免对资源的多重占用（比如写文件操作）。缺点：没有接口，不能继承，与单一职责原则冲突，一个类应该只关心内部逻辑，而不关心外面怎么样来实例化。 使用场景： 1、要求生产唯一序列号。2、WEB 中的计数器，不用每次刷新都在数据库里加一次，用单例先缓存起来。3、创建的一个对象需要消耗的资源过多，比如 I/O 与数据库的连接等。注意事项：getInstance() 方法中需要使用同步锁 synchronized (Singleton.class) 防止多线程同时进入造成 instance 被多次实例化。 几种单例模式的创建方式懒汉模式（线程不安全）public class Singleton { private static Singleton instance; private Singleton (){} public static Singleton getInstance() { if (instance == null) { instance = new Singleton(); } return instance; } } 懒汉模式（线程安全）public class Singleton { private static Singleton instance; private Singleton (){} public static synchronized Singleton getInstance() { if (instance == null) { instance = new Singleton(); } return instance; } } 饿汉模式线程是否安全：是优点：没有加锁，执行效率高缺点：类加载时就初始化，浪费内存 public class Singleton { private static Singleton instance = new Singleton(); private Singleton (){} public static Singleton getInstance() { return instance; } } 双检锁/双重校验锁public class Singleton { private volatile static Singleton singleton; private Singleton (){} public static Singleton getSingleton() { if (singleton == null) { synchronized (Singleton.class) { if (singleton == null) { singleton = new Singleton(); } } } return singleton; } } 登记式/静态内部类public class Singleton { private static class SingletonHolder { private static final Singleton INSTANCE = new Singleton(); } private Singleton (){} public static final Singleton getInstance() { return SingletonHolder.INSTANCE; } } 参考 菜鸟教程-单例模式","link":"/design-pattern-singleton/"},{"title":"原型模式","text":"在有些系统中，存在大量相同或相似对象的创建问题，如果用传统的构造函数来创建对象，会比较复杂且耗时耗资源，用原型模式生成对象就很高效，就像孙悟空拔下猴毛轻轻一吹就变出很多孙悟空一样简单。 原型模式的定义与特点原型（Prototype）模式的定义如下：用一个已经创建的实例作为原型，通过复制该原型对象来创建一个和原型相同或相似的新对象。在这里，原型实例指定了要创建的对象的种类。用这种方式创建对象非常高效，根本无须知道对象创建的细节。例如，Windows 操作系统的安装通常较耗时，如果复制就快了很多。在生活中复制的例子非常多，这里不一一列举了。 原型模式的优点： Java 自带的原型模式基于内存二进制流的复制，在性能上比直接 new 一个对象更加优良。 可以使用深克隆方式保存对象的状态，使用原型模式将对象复制一份，并将其状态保存起来，简化了创建对象的过程，以便在需要的时候使用（例如恢复到历史某一状态），可辅助实现撤销操作。 原型模式的缺点： 需要为每一个类都配置一个 clone 方法 clone 方法位于类的内部，当对已有类进行改造的时候，需要修改代码，违背了开闭原则。 当实现深克隆时，需要编写较为复杂的代码，而且当对象之间存在多重嵌套引用时，为了实现深克隆，每一层对象对应的类都必须支持深克隆，实现起来会比较麻烦。因此，深克隆、浅克隆需要运用得当。 原型模式的结构与实现由于 Java 提供了对象的 clone() 方法，所以用 Java 实现原型模式很简单。 1. 模式的结构原型模式包含以下主要角色。 抽象原型类：规定了具体原型对象必须实现的接口。 具体原型类：实现抽象原型类的 clone() 方法，它是可被复制的对象。 访问类：使用具体原型类中的 clone() 方法来复制新的对象。 代码public class Realizetype implements Cloneable{ Realizetype() { System.out.println(&quot;具体原型创建成功！&quot;); } @Override public Realizetype clone() throws CloneNotSupportedException { return (Realizetype)super.clone(); }} public class PrototypeTest { public static void main(String[] args) throws CloneNotSupportedException { Realizetype obj1 = new Realizetype(); Realizetype obj2 = (Realizetype) obj1.clone(); System.out.println(&quot;obj1==obj2?&quot; + (obj1 == obj2)); }}","link":"/design-pattern-prototype/"},{"title":"工厂模式","text":"介绍一下简单工厂、工厂方法、抽象工厂模式 一、简单工厂模式 定义 定义一个工厂类，他可以根据不同的参数，创建不同的实例。众多实例之间有着共同的父类或者接口。实现此功能的方法一般为静态方法 优点 需要什么，仅仅需要传入一个或者多个参数，就能获取到实例，而无需知道其内部的实现过程 ==在简单工厂模式中，用于创建实例的方法通常为静态方法（static），因此简单工厂模式又被成为 静态工厂方法== 代码 package simple;/** * @author mrhy * @date 2021/1/2 09:37 * Copyright (C), 2018-2021 */public class BaoShiJie implements Car { @Override public String name() { return &quot;我是保时捷&quot;; }} package simple;/** * 兰博基尼 * * @author mrhy * @date 2021/1/2 09:38 * Copyright (C), 2018-2021 */public class LanBoJiNi implements Car { @Override public String name() { return &quot;我是兰博基尼&quot;; }} package simple;/** * 汽车接口 * * @author mrhy * @date 2021/1/2 09:36 * Copyright (C), 2018-2021 */public interface Car { String name();} package simple;/** * 汽车工厂 * * @author mrhy * @date 2021/1/2 09:39 * Copyright (C), 2018-2021 */public class CarFactory { public static Car createCar(String name) { if (&quot;lanbo&quot;.equals(name)) { return new LanBoJiNi(); } else if (&quot;baoshijie&quot;.equals(name)) { return new BaoShiJie(); } else { throw new RuntimeException(&quot;未找到车&quot;); } } public static void main(String[] args) { Car lanbo = CarFactory.createCar(&quot;lanbo&quot;); System.out.println(lanbo.name()); Car baoshijie = CarFactory.createCar(&quot;baoshijie&quot;); System.out.println(baoshijie.name()); }} 二、工厂方法模式 定义 工厂方法模式（FACTORY METHOD）是一种常用的类创建型设计模式,此模式的核心精神是封装类中变化的部分，提取其中个性化善变的部分为独立类，通过依赖注入以达到解耦、复用和方便后期维护拓展的目的。它的核心结构有四个角色，分别是抽象工厂；具体工厂；抽象产品；具体产品 优点 封装内部的实现过程，便于拓展 uml图 代码 package factorymethod;/** * 汽车接口 * * @author mrhy * @date 2021/1/2 09:36 * Copyright (C), 2018-2021 */public interface Car { String name();} package factorymethod;/** * 工厂方法的抽象类 * * @author mrhy * @date 2021/1/3 10:15 * Copyright (C), 2018-2021 */public interface Factory { Car newCar();} package factorymethod;/** * @author mrhy * @date 2021/1/2 09:37 * Copyright (C), 2018-2021 */public class BaoShiJie implements Car { @Override public String name() { return &quot;我是保时捷&quot;; }} package factorymethod;import simple.CarFactory;/** * 汽车工厂 * * @author mrhy * @date 2021/1/2 09:39 * Copyright (C), 2018-2021 */public class BaoshijieCarFactory implements Factory { @Override public Car newCar() { return new BaoShiJie(); }} package factorymethod;/** * @author mrhy * @date 2021/1/3 10:21 * Copyright (C), 2018-2021 */public class LanboCarFactory implements Factory { @Override public Car newCar() { return new LanBoJiNi(); } public static void main(String[] args) { Car baoshijie = new BaoshijieCarFactory().newCar(); System.out.println(baoshijie.name()); LanboCarFactory lanboCarFactory = new LanboCarFactory(); System.out.println(lanboCarFactory.newCar().name()); }} package factorymethod;/** * 兰博基尼 * * @author mrhy * @date 2021/1/2 09:38 * Copyright (C), 2018-2021 */public class LanBoJiNi implements Car { @Override public String name() { return &quot;我是兰博基尼&quot;; }} 三、抽象工厂模式 定义 工厂的工厂，主要面对的对象为产品族 uml 代码 package abstractfactory;/** * @author mrhy * @date 2021/1/4 13:54 * Copyright (C), 2018-2021 */public class Customer { public static void main(String[] args) { MiFactory miFactory = new MiFactory(); Phone phone = miFactory.getPhone(); phone.open(); phone.close(); phone.sendMsg(); phone.call(); Router router = miFactory.getRouter(); router.open(); router.close(); router.sendSgn(); HuaweiFactory huaweiFactory = new HuaweiFactory(); Phone phone2 = huaweiFactory.getPhone(); phone2.open(); phone2.close(); phone2.sendMsg(); phone2.call(); Router router2 = huaweiFactory.getRouter(); router2.open(); router2.close(); router2.sendSgn(); }} package abstractfactory;/** * @author mrhy * @date 2021/1/4 13:53 * Copyright (C), 2018-2021 */public class HuaweiFactory implements IFactory { @Override public Phone getPhone() { return new HuaweiIPhone(); } @Override public Router getRouter() { return new HuaWeiRouter(); }} package abstractfactory;/** * 小米手机 * * @author mrhy * @date 2021/1/4 09:50 * Copyright (C), 2018-2021 */public class HuaweiIPhone implements Phone{ @Override public void open() { System.out.println(&quot;华为手机开机&quot;); } @Override public void close() { System.out.println(&quot;华为手机关机&quot;); } @Override public void sendMsg() { System.out.println(&quot;华为手机发短信&quot;); } @Override public void call() { System.out.println(&quot;华为手机打电话&quot;); }} package abstractfactory;/** * @author mrhy * @date 2021/1/4 11:19 * Copyright (C), 2018-2021 */public class HuaWeiRouter implements Router { @Override public void open() { System.out.println(&quot;华为路由器开机&quot;); } @Override public void close() { System.out.println(&quot;华为路由器关闭&quot;); } @Override public void sendSgn() { System.out.println(&quot;华为路由器广播&quot;); }} package abstractfactory;/** * 工厂的基类接口 * * @author mrhy * @date 2021/1/4 13:50 * Copyright (C), 2018-2021 */public interface IFactory { Phone getPhone(); Router getRouter();} package abstractfactory;/** * 小米工厂 * * @author mrhy * @date 2021/1/4 13:52 * Copyright (C), 2018-2021 */public class MiFactory implements IFactory { @Override public Phone getPhone() { return new XiaomiIPhone(); } @Override public Router getRouter() { return new XiaomiRouter(); }} package abstractfactory;/** * 手机接口 * * @author mrhy * @date 2021/1/4 09:44 * Copyright (C), 2018-2021 */public interface Phone { void open(); void close(); void sendMsg(); void call();} package abstractfactory;/** * 路由器 * * @author mrhy * @date 2021/1/4 09:49 * Copyright (C), 2018-2021 */public interface Router { void open(); void close(); void sendSgn();} package abstractfactory;/** * 小米手机 * * @author mrhy * @date 2021/1/4 09:50 * Copyright (C), 2018-2021 */public class XiaomiIPhone implements Phone{ @Override public void open() { System.out.println(&quot;小米手机开机&quot;); } @Override public void close() { System.out.println(&quot;小米手机关机&quot;); } @Override public void sendMsg() { System.out.println(&quot;小米手机发短信&quot;); } @Override public void call() { System.out.println(&quot;小米手机打电话&quot;); }} package abstractfactory;/** * @author mrhy * @date 2021/1/4 11:19 * Copyright (C), 2018-2021 */public class XiaomiRouter implements Router { @Override public void open() { System.out.println(&quot;小米路由器开机&quot;); } @Override public void close() { System.out.println(&quot;小米路由器关闭&quot;); } @Override public void sendSgn() { System.out.println(&quot;小米路由器广播&quot;); }} package abstractfactory;/** * @author mrhy * @date 2021/1/4 13:54 * Copyright (C), 2018-2021 */public class Customer { public static void main(String[] args) { MiFactory miFactory = new MiFactory(); Phone phone = miFactory.getPhone(); phone.open(); phone.close(); phone.sendMsg(); phone.call(); Router router = miFactory.getRouter(); router.open(); router.close(); router.sendSgn(); HuaweiFactory huaweiFactory = new HuaweiFactory(); Phone phone2 = huaweiFactory.getPhone(); phone2.open(); phone2.close(); phone2.sendMsg(); phone2.call(); Router router2 = huaweiFactory.getRouter(); router2.open(); router2.close(); router2.sendSgn(); }}","link":"/design-pattern-factory/"},{"title":"redis配置文件详解","text":"今天一起了解一下redis的配置文件，相当于是一个复习 # Redis configuration file example.## Note that in order to read the configuration file, Redis must be# started with the file path as first argument:## ./redis-server /path/to/redis.conf# 对大小写不敏感# Note on units: when memory size is needed, it is possible to specify# it in the usual form of 1k 5GB 4M and so forth:## 1k =&gt; 1000 bytes# 1kb =&gt; 1024 bytes# 1m =&gt; 1000000 bytes# 1mb =&gt; 1024*1024 bytes# 1g =&gt; 1000000000 bytes# 1gb =&gt; 1024*1024*1024 bytes## units are case insensitive so 1GB 1Gb 1gB are all the same.################################## INCLUDES #################################### Include one or more other config files here. This is useful if you# have a standard template that goes to all Redis servers but also need# to customize a few per-server settings. Include files can include# other files, so use this wisely.## Notice option &quot;include&quot; won't be rewritten by command &quot;CONFIG REWRITE&quot;# from admin or Redis Sentinel. Since Redis always uses the last processed# line as value of a configuration directive, you'd better put includes# at the beginning of this file to avoid overwriting config change at runtime.## If instead you are interested in using includes to override configuration# options, it is better to use include as the last line.## include /path/to/local.conf# include /path/to/other.conf################################## MODULES ###################################### Load modules at startup. If the server is not able to load modules# it will abort. It is possible to use multiple loadmodule directives.## loadmodule /path/to/my_module.so# loadmodule /path/to/other_module.so################################## NETWORK ###################################### By default, if no &quot;bind&quot; configuration directive is specified, Redis listens# for connections from all the network interfaces available on the server.# It is possible to listen to just one or multiple selected interfaces using# the &quot;bind&quot; configuration directive, followed by one or more IP addresses.## Examples:## bind 192.168.1.100 10.0.0.1# bind 127.0.0.1 ::1## ~~~ WARNING ~~~ If the computer running Redis is directly exposed to the# internet, binding to all the interfaces is dangerous and will expose the# instance to everybody on the internet. So by default we uncomment the# following bind directive, that will force Redis to listen only into# the IPv4 loopback interface address (this means Redis will be able to# accept connections only from clients running into the same computer it# is running).## IF YOU ARE SURE YOU WANT YOUR INSTANCE TO LISTEN TO ALL THE INTERFACES# JUST COMMENT THE FOLLOWING LINE.# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~bind 127.0.0.1 ::1# Protected mode is a layer of security protection, in order to avoid that# Redis instances left open on the internet are accessed and exploited.## When protected mode is on and if:## 1) The server is not binding explicitly to a set of addresses using the# &quot;bind&quot; directive.# 2) No password is configured.## The server only accepts connections from clients connecting from the# IPv4 and IPv6 loopback addresses 127.0.0.1 and ::1, and from Unix domain# sockets.## By default protected mode is enabled. You should disable it only if# you are sure you want clients from other hosts to connect to Redis# even if no authentication is configured, nor a specific set of interfaces# are explicitly listed using the &quot;bind&quot; directive.protected-mode yes# Accept connections on the specified port, default is 6379 (IANA #815344).# If port 0 is specified Redis will not listen on a TCP socket.# 端口号port 6379# TCP listen() backlog.## In high requests-per-second environments you need an high backlog in order# to avoid slow clients connections issues. Note that the Linux kernel# will silently truncate it to the value of /proc/sys/net/core/somaxconn so# make sure to raise both the value of somaxconn and tcp_max_syn_backlog# in order to get the desired effect.tcp-backlog 511# Unix socket.## Specify the path for the Unix socket # that will be used to listen for# incoming connections. There is no default, so Redis will not listen# on a unix socket when not specified.## unixsocket /tmp/redis.sock# unixsocketperm 700# Close the connection after a client is idle for N seconds (0 to disable)timeout 0# TCP keepalive.## If non-zero, use SO_KEEPALIVE to send TCP ACKs to clients in absence# of communication. This is useful for two reasons:## 1) Detect dead peers.# 2) Take the connection alive from the point of view of network# equipment in the middle.## On Linux, the specified value (in seconds) is the period used to send ACKs.# Note that to close the connection the double of the time is needed.# On other kernels the period depends on the kernel configuration.## A reasonable value for this option is 300 seconds, which is the new# Redis default starting with Redis 3.2.1.tcp-keepalive 300################################# TLS/SSL ###################################### By default, TLS/SSL is disabled. To enable it, the &quot;tls-port&quot; configuration# directive can be used to define TLS-listening ports. To enable TLS on the# default port, use:## port 0# tls-port 6379# Configure a X.509 certificate and private key to use for authenticating the# server to connected clients, masters or cluster peers. These files should be# PEM formatted.## tls-cert-file redis.crt # tls-key-file redis.key# Configure a DH parameters file to enable Diffie-Hellman (DH) key exchange:## tls-dh-params-file redis.dh# Configure a CA certificate(s) bundle or directory to authenticate TLS/SSL# clients and peers. Redis requires an explicit configuration of at least one# of these, and will not implicitly use the system wide configuration.## tls-ca-cert-file ca.crt# tls-ca-cert-dir /etc/ssl/certs# By default, clients (including replica servers) on a TLS port are required# to authenticate using valid client side certificates.## If &quot;no&quot; is specified, client certificates are not required and not accepted.# If &quot;optional&quot; is specified, client certificates are accepted and must be# valid if provided, but are not required.## tls-auth-clients no# tls-auth-clients optional# By default, a Redis replica does not attempt to establish a TLS connection# with its master.## Use the following directive to enable TLS on replication links.## tls-replication yes# By default, the Redis Cluster bus uses a plain TCP connection. To enable# TLS for the bus protocol, use the following directive:## tls-cluster yes# Explicitly specify TLS versions to support. Allowed values are case insensitive# and include &quot;TLSv1&quot;, &quot;TLSv1.1&quot;, &quot;TLSv1.2&quot;, &quot;TLSv1.3&quot; (OpenSSL &gt;= 1.1.1) or# any combination. To enable only TLSv1.2 and TLSv1.3, use:## tls-protocols &quot;TLSv1.2 TLSv1.3&quot;# Configure allowed ciphers. See the ciphers(1ssl) manpage for more information# about the syntax of this string.## Note: this configuration applies only to &lt;= TLSv1.2.## tls-ciphers DEFAULT:!MEDIUM# Configure allowed TLSv1.3 ciphersuites. See the ciphers(1ssl) manpage for more# information about the syntax of this string, and specifically for TLSv1.3# ciphersuites.## tls-ciphersuites TLS_CHACHA20_POLY1305_SHA256# When choosing a cipher, use the server's preference instead of the client# preference. By default, the server follows the client's preference.## tls-prefer-server-ciphers yes# By default, TLS session caching is enabled to allow faster and less expensive# reconnections by clients that support it. Use the following directive to disable# caching.## tls-session-caching no# Change the default number of TLS sessions cached. A zero value sets the cache# to unlimited size. The default size is 20480.## tls-session-cache-size 5000# Change the default timeout of cached TLS sessions. The default timeout is 300# seconds.## tls-session-cache-timeout 60################################# GENERAL ###################################### By default Redis does not run as a daemon. Use 'yes' if you need it.# Note that Redis will write a pid file in /usr/local/var/run/redis.pid when daemonized.daemonize yes# If you run Redis from upstart or systemd, Redis can interact with your# supervision tree. Options:# supervised no - no supervision interaction# supervised upstart - signal upstart by putting Redis into SIGSTOP mode# supervised systemd - signal systemd by writing READY=1 to $NOTIFY_SOCKET# supervised auto - detect upstart or systemd method based on# UPSTART_JOB or NOTIFY_SOCKET environment variables# Note: these supervision methods only signal &quot;process is ready.&quot;# They do not enable continuous liveness pings back to your supervisor.supervised no# If a pid file is specified, Redis writes it where specified at startup# and removes it at exit.## When the server runs non daemonized, no pid file is created if none is# specified in the configuration. When the server is daemonized, the pid file# is used even if not specified, defaulting to &quot;/usr/local/var/run/redis.pid&quot;.## Creating a pid file is best effort: if Redis is not able to create it# nothing bad happens, the server will start and run normally.pidfile /var/run/redis_6379.pid# Specify the server verbosity level.# This can be one of:# debug (a lot of information, useful for development/testing)# verbose (many rarely useful info, but not a mess like the debug level)# notice (moderately verbose, what you want in production probably)# warning (only very important / critical messages are logged)loglevel notice# Specify the log file name. Also the empty string can be used to force# Redis to log on the standard output. Note that if you use standard# output for logging but daemonize, logs will be sent to /dev/nulllogfile &quot;&quot;# To enable logging to the system logger, just set 'syslog-enabled' to yes,# and optionally update the other syslog parameters to suit your needs.# syslog-enabled no# Specify the syslog identity.# syslog-ident redis# Specify the syslog facility. Must be USER or between LOCAL0-LOCAL7.# syslog-facility local0# Set the number of databases. The default database is DB 0, you can select# a different one on a per-connection basis using SELECT &lt;dbid&gt; where# dbid is a number between 0 and 'databases'-1databases 16# By default Redis shows an ASCII art logo only when started to log to the# standard output and if the standard output is a TTY. Basically this means# that normally a logo is displayed only in interactive sessions.## However it is possible to force the pre-4.0 behavior and always show a# ASCII art logo in startup logs by setting the following option to yes.always-show-logo yes################################ SNAPSHOTTING ################################## Save the DB on disk:## save &lt;seconds&gt; &lt;changes&gt;## Will save the DB if both the given number of seconds and the given# number of write operations against the DB occurred.## In the example below the behaviour will be to save:# after 900 sec (15 min) if at least 1 key changed# after 300 sec (5 min) if at least 10 keys changed# after 60 sec if at least 10000 keys changed## Note: you can disable saving completely by commenting out all &quot;save&quot; lines.## It is also possible to remove all the previously configured save# points by adding a save directive with a single empty string argument# like in the following example:## save &quot;&quot;save 900 1save 300 10save 60 10000# By default Redis will stop accepting writes if RDB snapshots are enabled# (at least one save point) and the latest background save failed.# This will make the user aware (in a hard way) that data is not persisting# on disk properly, otherwise chances are that no one will notice and some# disaster will happen.## If the background saving process will start working again Redis will# automatically allow writes again.## However if you have setup your proper monitoring of the Redis server# and persistence, you may want to disable this feature so that Redis will# continue to work as usual even if there are problems with disk,# permissions, and so forth.stop-writes-on-bgsave-error yes# Compress string objects using LZF when dump .rdb databases?# For default that's set to 'yes' as it's almost always a win.# If you want to save some CPU in the saving child set it to 'no' but# the dataset will likely be bigger if you have compressible values or keys.rdbcompression yes# Since version 5 of RDB a CRC64 checksum is placed at the end of the file.# This makes the format more resistant to corruption but there is a performance# hit to pay (around 10%) when saving and loading RDB files, so you can disable it# for maximum performances.## RDB files created with checksum disabled have a checksum of zero that will# tell the loading code to skip the check.rdbchecksum yes# The filename where to dump the DBdbfilename dump.rdb# Remove RDB files used by replication in instances without persistence# enabled. By default this option is disabled, however there are environments# where for regulations or other security concerns, RDB files persisted on# disk by masters in order to feed replicas, or stored on disk by replicas# in order to load them for the initial synchronization, should be deleted# ASAP. Note that this option ONLY WORKS in instances that have both AOF# and RDB persistence disabled, otherwise is completely ignored.## An alternative (and sometimes better) way to obtain the same effect is# to use diskless replication on both master and replicas instances. However# in the case of replicas, diskless is not always an option.rdb-del-sync-files no# The working directory.## The DB will be written inside this directory, with the filename specified# above using the 'dbfilename' configuration directive.## The Append Only File will also be created inside this directory.## Note that you must specify a directory here, not a file name.dir /usr/local/var/db/redis/################################# REPLICATION ################################## Master-Replica replication. Use replicaof to make a Redis instance a copy of# another Redis server. A few things to understand ASAP about Redis replication.## +------------------+ +---------------+# | Master | ---&gt; | Replica |# | (receive writes) | | (exact copy) |# +------------------+ +---------------+## 1) Redis replication is asynchronous, but you can configure a master to# stop accepting writes if it appears to be not connected with at least# a given number of replicas.# 2) Redis replicas are able to perform a partial resynchronization with the# master if the replication link is lost for a relatively small amount of# time. You may want to configure the replication backlog size (see the next# sections of this file) with a sensible value depending on your needs.# 3) Replication is automatic and does not need user intervention. After a# network partition replicas automatically try to reconnect to masters# and resynchronize with them.## replicaof &lt;masterip&gt; &lt;masterport&gt;# If the master is password protected (using the &quot;requirepass&quot; configuration# directive below) it is possible to tell the replica to authenticate before# starting the replication synchronization process, otherwise the master will# refuse the replica request.## masterauth &lt;master-password&gt;## However this is not enough if you are using Redis ACLs (for Redis version# 6 or greater), and the default user is not capable of running the PSYNC# command and/or other commands needed for replication. In this case it's# better to configure a special user to use with replication, and specify the# masteruser configuration as such:## masteruser &lt;username&gt;## When masteruser is specified, the replica will authenticate against its# master using the new AUTH form: AUTH &lt;username&gt; &lt;password&gt;.# When a replica loses its connection with the master, or when the replication# is still in progress, the replica can act in two different ways:## 1) if replica-serve-stale-data is set to 'yes' (the default) the replica will# still reply to client requests, possibly with out of date data, or the# data set may just be empty if this is the first synchronization.## 2) if replica-serve-stale-data is set to 'no' the replica will reply with# an error &quot;SYNC with master in progress&quot; to all the kind of commands# but to INFO, replicaOF, AUTH, PING, SHUTDOWN, REPLCONF, ROLE, CONFIG,# SUBSCRIBE, UNSUBSCRIBE, PSUBSCRIBE, PUNSUBSCRIBE, PUBLISH, PUBSUB,# COMMAND, POST, HOST: and LATENCY.#replica-serve-stale-data yes# You can configure a replica instance to accept writes or not. Writing against# a replica instance may be useful to store some ephemeral data (because data# written on a replica will be easily deleted after resync with the master) but# may also cause problems if clients are writing to it because of a# misconfiguration.## Since Redis 2.6 by default replicas are read-only.## Note: read only replicas are not designed to be exposed to untrusted clients# on the internet. It's just a protection layer against misuse of the instance.# Still a read only replica exports by default all the administrative commands# such as CONFIG, DEBUG, and so forth. To a limited extent you can improve# security of read only replicas using 'rename-command' to shadow all the# administrative / dangerous commands.replica-read-only yes# Replication SYNC strategy: disk or socket.## New replicas and reconnecting replicas that are not able to continue the# replication process just receiving differences, need to do what is called a# &quot;full synchronization&quot;. An RDB file is transmitted from the master to the# replicas.## The transmission can happen in two different ways:## 1) Disk-backed: The Redis master creates a new process that writes the RDB# file on disk. Later the file is transferred by the parent# process to the replicas incrementally.# 2) Diskless: The Redis master creates a new process that directly writes the# RDB file to replica sockets, without touching the disk at all.## With disk-backed replication, while the RDB file is generated, more replicas# can be queued and served with the RDB file as soon as the current child# producing the RDB file finishes its work. With diskless replication instead# once the transfer starts, new replicas arriving will be queued and a new# transfer will start when the current one terminates.## When diskless replication is used, the master waits a configurable amount of# time (in seconds) before starting the transfer in the hope that multiple# replicas will arrive and the transfer can be parallelized.## With slow disks and fast (large bandwidth) networks, diskless replication# works better.repl-diskless-sync no# When diskless replication is enabled, it is possible to configure the delay# the server waits in order to spawn the child that transfers the RDB via socket# to the replicas.## This is important since once the transfer starts, it is not possible to serve# new replicas arriving, that will be queued for the next RDB transfer, so the# server waits a delay in order to let more replicas arrive.## The delay is specified in seconds, and by default is 5 seconds. To disable# it entirely just set it to 0 seconds and the transfer will start ASAP.repl-diskless-sync-delay 5# -----------------------------------------------------------------------------# WARNING: RDB diskless load is experimental. Since in this setup the replica# does not immediately store an RDB on disk, it may cause data loss during# failovers. RDB diskless load + Redis modules not handling I/O reads may also# cause Redis to abort in case of I/O errors during the initial synchronization# stage with the master. Use only if your do what you are doing.# -----------------------------------------------------------------------------## Replica can load the RDB it reads from the replication link directly from the# socket, or store the RDB to a file and read that file after it was completely# recived from the master.## In many cases the disk is slower than the network, and storing and loading# the RDB file may increase replication time (and even increase the master's# Copy on Write memory and salve buffers).# However, parsing the RDB file directly from the socket may mean that we have# to flush the contents of the current database before the full rdb was# received. For this reason we have the following options:## &quot;disabled&quot; - Don't use diskless load (store the rdb file to the disk first)# &quot;on-empty-db&quot; - Use diskless load only when it is completely safe.# &quot;swapdb&quot; - Keep a copy of the current db contents in RAM while parsing# the data directly from the socket. note that this requires# sufficient memory, if you don't have it, you risk an OOM kill.repl-diskless-load disabled# Replicas send PINGs to server in a predefined interval. It's possible to# change this interval with the repl_ping_replica_period option. The default# value is 10 seconds.## repl-ping-replica-period 10# The following option sets the replication timeout for:## 1) Bulk transfer I/O during SYNC, from the point of view of replica.# 2) Master timeout from the point of view of replicas (data, pings).# 3) Replica timeout from the point of view of masters (REPLCONF ACK pings).## It is important to make sure that this value is greater than the value# specified for repl-ping-replica-period otherwise a timeout will be detected# every time there is low traffic between the master and the replica.## repl-timeout 60# Disable TCP_NODELAY on the replica socket after SYNC?## If you select &quot;yes&quot; Redis will use a smaller number of TCP packets and# less bandwidth to send data to replicas. But this can add a delay for# the data to appear on the replica side, up to 40 milliseconds with# Linux kernels using a default configuration.## If you select &quot;no&quot; the delay for data to appear on the replica side will# be reduced but more bandwidth will be used for replication.## By default we optimize for low latency, but in very high traffic conditions# or when the master and replicas are many hops away, turning this to &quot;yes&quot; may# be a good idea.repl-disable-tcp-nodelay no# Set the replication backlog size. The backlog is a buffer that accumulates# replica data when replicas are disconnected for some time, so that when a# replica wants to reconnect again, often a full resync is not needed, but a# partial resync is enough, just passing the portion of data the replica# missed while disconnected.## The bigger the replication backlog, the longer the time the replica can be# disconnected and later be able to perform a partial resynchronization.## The backlog is only allocated once there is at least a replica connected.## repl-backlog-size 1mb# After a master has no longer connected replicas for some time, the backlog# will be freed. The following option configures the amount of seconds that# need to elapse, starting from the time the last replica disconnected, for# the backlog buffer to be freed.## Note that replicas never free the backlog for timeout, since they may be# promoted to masters later, and should be able to correctly &quot;partially# resynchronize&quot; with the replicas: hence they should always accumulate backlog.## A value of 0 means to never release the backlog.## repl-backlog-ttl 3600# The replica priority is an integer number published by Redis in the INFO# output. It is used by Redis Sentinel in order to select a replica to promote# into a master if the master is no longer working correctly.## A replica with a low priority number is considered better for promotion, so# for instance if there are three replicas with priority 10, 100, 25 Sentinel# will pick the one with priority 10, that is the lowest.## However a special priority of 0 marks the replica as not able to perform the# role of master, so a replica with priority of 0 will never be selected by# Redis Sentinel for promotion.## By default the priority is 100.replica-priority 100# It is possible for a master to stop accepting writes if there are less than# N replicas connected, having a lag less or equal than M seconds.## The N replicas need to be in &quot;online&quot; state.## The lag in seconds, that must be &lt;= the specified value, is calculated from# the last ping received from the replica, that is usually sent every second.## This option does not GUARANTEE that N replicas will accept the write, but# will limit the window of exposure for lost writes in case not enough replicas# are available, to the specified number of seconds.## For example to require at least 3 replicas with a lag &lt;= 10 seconds use:## min-replicas-to-write 3# min-replicas-max-lag 10## Setting one or the other to 0 disables the feature.## By default min-replicas-to-write is set to 0 (feature disabled) and# min-replicas-max-lag is set to 10.# A Redis master is able to list the address and port of the attached# replicas in different ways. For example the &quot;INFO replication&quot; section# offers this information, which is used, among other tools, by# Redis Sentinel in order to discover replica instances.# Another place where this info is available is in the output of the# &quot;ROLE&quot; command of a master.## The listed IP and address normally reported by a replica is obtained# in the following way:## IP: The address is auto detected by checking the peer address# of the socket used by the replica to connect with the master.## Port: The port is communicated by the replica during the replication# handshake, and is normally the port that the replica is using to# listen for connections.## However when port forwarding or Network Address Translation (NAT) is# used, the replica may be actually reachable via different IP and port# pairs. The following two options can be used by a replica in order to# report to its master a specific set of IP and port, so that both INFO# and ROLE will report those values.## There is no need to use both the options if you need to override just# the port or the IP address.## replica-announce-ip 5.5.5.5# replica-announce-port 1234############################### KEYS TRACKING ################################## Redis implements server assisted support for client side caching of values.# This is implemented using an invalidation table that remembers, using# 16 millions of slots, what clients may have certain subsets of keys. In turn# this is used in order to send invalidation messages to clients. Please# to understand more about the feature check this page:## https://redis.io/topics/client-side-caching## When tracking is enabled for a client, all the read only queries are assumed# to be cached: this will force Redis to store information in the invalidation# table. When keys are modified, such information is flushed away, and# invalidation messages are sent to the clients. However if the workload is# heavily dominated by reads, Redis could use more and more memory in order# to track the keys fetched by many clients.## For this reason it is possible to configure a maximum fill value for the# invalidation table. By default it is set to 1M of keys, and once this limit# is reached, Redis will start to evict keys in the invalidation table# even if they were not modified, just to reclaim memory: this will in turn# force the clients to invalidate the cached values. Basically the table# maximum size is a trade off between the memory you want to spend server# side to track information about who cached what, and the ability of clients# to retain cached objects in memory.## If you set the value to 0, it means there are no limits, and Redis will# retain as many keys as needed in the invalidation table.# In the &quot;stats&quot; INFO section, you can find information about the number of# keys in the invalidation table at every given moment.## Note: when key tracking is used in broadcasting mode, no memory is used# in the server side so this setting is useless.## tracking-table-max-keys 1000000################################## SECURITY #################################### Warning: since Redis is pretty fast an outside user can try up to# 1 million passwords per second against a modern box. This means that you# should use very strong passwords, otherwise they will be very easy to break.# Note that because the password is really a shared secret between the client# and the server, and should not be memorized by any human, the password# can be easily a long string from /dev/urandom or whatever, so by using a# long and unguessable password no brute force attack will be possible.# Redis ACL users are defined in the following format:## user &lt;username&gt; ... acl rules ...## For example:## user worker +@list +@connection ~jobs:* on &gt;ffa9203c493aa99## The special username &quot;default&quot; is used for new connections. If this user# has the &quot;nopass&quot; rule, then new connections will be immediately authenticated# as the &quot;default&quot; user without the need of any password provided via the# AUTH command. Otherwise if the &quot;default&quot; user is not flagged with &quot;nopass&quot;# the connections will start in not authenticated state, and will require# AUTH (or the HELLO command AUTH option) in order to be authenticated and# start to work.## The ACL rules that describe what an user can do are the following:## on Enable the user: it is possible to authenticate as this user.# off Disable the user: it's no longer possible to authenticate# with this user, however the already authenticated connections# will still work.# +&lt;command&gt; Allow the execution of that command# -&lt;command&gt; Disallow the execution of that command# +@&lt;category&gt; Allow the execution of all the commands in such category# with valid categories are like @admin, @set, @sortedset, ...# and so forth, see the full list in the server.c file where# the Redis command table is described and defined.# The special category @all means all the commands, but currently# present in the server, and that will be loaded in the future# via modules.# +&lt;command&gt;|subcommand Allow a specific subcommand of an otherwise# disabled command. Note that this form is not# allowed as negative like -DEBUG|SEGFAULT, but# only additive starting with &quot;+&quot;.# allcommands Alias for +@all. Note that it implies the ability to execute# all the future commands loaded via the modules system.# nocommands Alias for -@all.# ~&lt;pattern&gt; Add a pattern of keys that can be mentioned as part of# commands. For instance ~* allows all the keys. The pattern# is a glob-style pattern like the one of KEYS.# It is possible to specify multiple patterns.# allkeys Alias for ~*# resetkeys Flush the list of allowed keys patterns.# &gt;&lt;password&gt; Add this passowrd to the list of valid password for the user.# For example &gt;mypass will add &quot;mypass&quot; to the list.# This directive clears the &quot;nopass&quot; flag (see later).# &lt;&lt;password&gt; Remove this password from the list of valid passwords.# nopass All the set passwords of the user are removed, and the user# is flagged as requiring no password: it means that every# password will work against this user. If this directive is# used for the default user, every new connection will be# immediately authenticated with the default user without# any explicit AUTH command required. Note that the &quot;resetpass&quot;# directive will clear this condition.# resetpass Flush the list of allowed passwords. Moreover removes the# &quot;nopass&quot; status. After &quot;resetpass&quot; the user has no associated# passwords and there is no way to authenticate without adding# some password (or setting it as &quot;nopass&quot; later).# reset Performs the following actions: resetpass, resetkeys, off,# -@all. The user returns to the same state it has immediately# after its creation.## ACL rules can be specified in any order: for instance you can start with# passwords, then flags, or key patterns. However note that the additive# and subtractive rules will CHANGE MEANING depending on the ordering.# For instance see the following example:## user alice on +@all -DEBUG ~* &gt;somepassword## This will allow &quot;alice&quot; to use all the commands with the exception of the# DEBUG command, since +@all added all the commands to the set of the commands# alice can use, and later DEBUG was removed. However if we invert the order# of two ACL rules the result will be different:## user alice on -DEBUG +@all ~* &gt;somepassword## Now DEBUG was removed when alice had yet no commands in the set of allowed# commands, later all the commands are added, so the user will be able to# execute everything.## Basically ACL rules are processed left-to-right.## For more information about ACL configuration please refer to# the Redis web site at https://redis.io/topics/acl# ACL LOG## The ACL Log tracks failed commands and authentication events associated# with ACLs. The ACL Log is useful to troubleshoot failed commands blocked # by ACLs. The ACL Log is stored in memory. You can reclaim memory with # ACL LOG RESET. Define the maximum entry length of the ACL Log below.acllog-max-len 128# Using an external ACL file## Instead of configuring users here in this file, it is possible to use# a stand-alone file just listing users. The two methods cannot be mixed:# if you configure users here and at the same time you activate the exteranl# ACL file, the server will refuse to start.## The format of the external ACL user file is exactly the same as the# format that is used inside redis.conf to describe users.## aclfile /etc/redis/users.acl# IMPORTANT NOTE: starting with Redis 6 &quot;requirepass&quot; is just a compatiblity# layer on top of the new ACL system. The option effect will be just setting# the password for the default user. Clients will still authenticate using# AUTH &lt;password&gt; as usually, or more explicitly with AUTH default &lt;password&gt;# if they follow the new protocol: both will work.## requirepass foobared# Command renaming (DEPRECATED).## ------------------------------------------------------------------------# WARNING: avoid using this option if possible. Instead use ACLs to remove# commands from the default user, and put them only in some admin user you# create for administrative purposes.# ------------------------------------------------------------------------## It is possible to change the name of dangerous commands in a shared# environment. For instance the CONFIG command may be renamed into something# hard to guess so that it will still be available for internal-use tools# but not available for general clients.## Example:## rename-command CONFIG b840fc02d524045429941cc15f59e41cb7be6c52## It is also possible to completely kill a command by renaming it into# an empty string:## rename-command CONFIG &quot;&quot;## Please note that changing the name of commands that are logged into the# AOF file or transmitted to replicas may cause problems.################################### CLIENTS ##################################### Set the max number of connected clients at the same time. By default# this limit is set to 10000 clients, however if the Redis server is not# able to configure the process file limit to allow for the specified limit# the max number of allowed clients is set to the current file limit# minus 32 (as Redis reserves a few file descriptors for internal uses).## Once the limit is reached Redis will close all the new connections sending# an error 'max number of clients reached'.## IMPORTANT: When Redis Cluster is used, the max number of connections is also# shared with the cluster bus: every node in the cluster will use two# connections, one incoming and another outgoing. It is important to size the# limit accordingly in case of very large clusters.## maxclients 10000############################## MEMORY MANAGEMENT ################################# Set a memory usage limit to the specified amount of bytes.# When the memory limit is reached Redis will try to remove keys# according to the eviction policy selected (see maxmemory-policy).## If Redis can't remove keys according to the policy, or if the policy is# set to 'noeviction', Redis will start to reply with errors to commands# that would use more memory, like SET, LPUSH, and so on, and will continue# to reply to read-only commands like GET.## This option is usually useful when using Redis as an LRU or LFU cache, or to# set a hard memory limit for an instance (using the 'noeviction' policy).## WARNING: If you have replicas attached to an instance with maxmemory on,# the size of the output buffers needed to feed the replicas are subtracted# from the used memory count, so that network problems / resyncs will# not trigger a loop where keys are evicted, and in turn the output# buffer of replicas is full with DELs of keys evicted triggering the deletion# of more keys, and so forth until the database is completely emptied.## In short... if you have replicas attached it is suggested that you set a lower# limit for maxmemory so that there is some free RAM on the system for replica# output buffers (but this is not needed if the policy is 'noeviction').## maxmemory &lt;bytes&gt;# MAXMEMORY POLICY: how Redis will select what to remove when maxmemory# is reached. You can select one from the following behaviors:## volatile-lru -&gt; Evict using approximated LRU, only keys with an expire set.# allkeys-lru -&gt; Evict any key using approximated LRU.# volatile-lfu -&gt; Evict using approximated LFU, only keys with an expire set.# allkeys-lfu -&gt; Evict any key using approximated LFU.# volatile-random -&gt; Remove a random key having an expire set.# allkeys-random -&gt; Remove a random key, any key.# volatile-ttl -&gt; Remove the key with the nearest expire time (minor TTL)# noeviction -&gt; Don't evict anything, just return an error on write operations.## LRU means Least Recently Used# LFU means Least Frequently Used## Both LRU, LFU and volatile-ttl are implemented using approximated# randomized algorithms.## Note: with any of the above policies, Redis will return an error on write# operations, when there are no suitable keys for eviction.## At the date of writing these commands are: set setnx setex append# incr decr rpush lpush rpushx lpushx linsert lset rpoplpush sadd# sinter sinterstore sunion sunionstore sdiff sdiffstore zadd zincrby# zunionstore zinterstore hset hsetnx hmset hincrby incrby decrby# getset mset msetnx exec sort## The default is:## maxmemory-policy noeviction# LRU, LFU and minimal TTL algorithms are not precise algorithms but approximated# algorithms (in order to save memory), so you can tune it for speed or# accuracy. For default Redis will check five keys and pick the one that was# used less recently, you can change the sample size using the following# configuration directive.## The default of 5 produces good enough results. 10 Approximates very closely# true LRU but costs more CPU. 3 is faster but not very accurate.## maxmemory-samples 5# Starting from Redis 5, by default a replica will ignore its maxmemory setting# (unless it is promoted to master after a failover or manually). It means# that the eviction of keys will be just handled by the master, sending the# DEL commands to the replica as keys evict in the master side.## This behavior ensures that masters and replicas stay consistent, and is usually# what you want, however if your replica is writable, or you want the replica# to have a different memory setting, and you are sure all the writes performed# to the replica are idempotent, then you may change this default (but be sure# to understand what you are doing).## Note that since the replica by default does not evict, it may end using more# memory than the one set via maxmemory (there are certain buffers that may# be larger on the replica, or data structures may sometimes take more memory# and so forth). So make sure you monitor your replicas and make sure they# have enough memory to never hit a real out-of-memory condition before the# master hits the configured maxmemory setting.## replica-ignore-maxmemory yes# Redis reclaims expired keys in two ways: upon access when those keys are# found to be expired, and also in background, in what is called the# &quot;active expire key&quot;. The key space is slowly and interactively scanned# looking for expired keys to reclaim, so that it is possible to free memory# of keys that are expired and will never be accessed again in a short time.## The default effort of the expire cycle will try to avoid having more than# ten percent of expired keys still in memory, and will try to avoid consuming# more than 25% of total memory and to add latency to the system. However# it is possible to increase the expire &quot;effort&quot; that is normally set to# &quot;1&quot;, to a greater value, up to the value &quot;10&quot;. At its maximum value the# system will use more CPU, longer cycles (and technically may introduce# more latency), and will tollerate less already expired keys still present# in the system. It's a tradeoff betweeen memory, CPU and latecy.## active-expire-effort 1############################# LAZY FREEING ##################################### Redis has two primitives to delete keys. One is called DEL and is a blocking# deletion of the object. It means that the server stops processing new commands# in order to reclaim all the memory associated with an object in a synchronous# way. If the key deleted is associated with a small object, the time needed# in order to execute the DEL command is very small and comparable to most other# O(1) or O(log_N) commands in Redis. However if the key is associated with an# aggregated value containing millions of elements, the server can block for# a long time (even seconds) in order to complete the operation.## For the above reasons Redis also offers non blocking deletion primitives# such as UNLINK (non blocking DEL) and the ASYNC option of FLUSHALL and# FLUSHDB commands, in order to reclaim memory in background. Those commands# are executed in constant time. Another thread will incrementally free the# object in the background as fast as possible.## DEL, UNLINK and ASYNC option of FLUSHALL and FLUSHDB are user-controlled.# It's up to the design of the application to understand when it is a good# idea to use one or the other. However the Redis server sometimes has to# delete keys or flush the whole database as a side effect of other operations.# Specifically Redis deletes objects independently of a user call in the# following scenarios:## 1) On eviction, because of the maxmemory and maxmemory policy configurations,# in order to make room for new data, without going over the specified# memory limit.# 2) Because of expire: when a key with an associated time to live (see the# EXPIRE command) must be deleted from memory.# 3) Because of a side effect of a command that stores data on a key that may# already exist. For example the RENAME command may delete the old key# content when it is replaced with another one. Similarly SUNIONSTORE# or SORT with STORE option may delete existing keys. The SET command# itself removes any old content of the specified key in order to replace# it with the specified string.# 4) During replication, when a replica performs a full resynchronization with# its master, the content of the whole database is removed in order to# load the RDB file just transferred.## In all the above cases the default is to delete objects in a blocking way,# like if DEL was called. However you can configure each case specifically# in order to instead release memory in a non-blocking way like if UNLINK# was called, using the following configuration directives.lazyfree-lazy-eviction nolazyfree-lazy-expire nolazyfree-lazy-server-del noreplica-lazy-flush no# It is also possible, for the case when to replace the user code DEL calls# with UNLINK calls is not easy, to modify the default behavior of the DEL# command to act exactly like UNLINK, using the following configuration# directive:lazyfree-lazy-user-del no################################ THREADED I/O ################################## Redis is mostly single threaded, however there are certain threaded# operations such as UNLINK, slow I/O accesses and other things that are# performed on side threads.## Now it is also possible to handle Redis clients socket reads and writes# in different I/O threads. Since especially writing is so slow, normally# Redis users use pipelining in order to speedup the Redis performances per# core, and spawn multiple instances in order to scale more. Using I/O# threads it is possible to easily speedup two times Redis without resorting# to pipelining nor sharding of the instance.## By default threading is disabled, we suggest enabling it only in machines# that have at least 4 or more cores, leaving at least one spare core.# Using more than 8 threads is unlikely to help much. We also recommend using# threaded I/O only if you actually have performance problems, with Redis# instances being able to use a quite big percentage of CPU time, otherwise# there is no point in using this feature.## So for instance if you have a four cores boxes, try to use 2 or 3 I/O# threads, if you have a 8 cores, try to use 6 threads. In order to# enable I/O threads use the following configuration directive:## io-threads 4## Setting io-threads to 1 will just use the main thread as usually.# When I/O threads are enabled, we only use threads for writes, that is# to thread the write(2) syscall and transfer the client buffers to the# socket. However it is also possible to enable threading of reads and# protocol parsing using the following configuration directive, by setting# it to yes:## io-threads-do-reads no## Usually threading reads doesn't help much.## NOTE 1: This configuration directive cannot be changed at runtime via# CONFIG SET. Aso this feature currently does not work when SSL is# enabled.## NOTE 2: If you want to test the Redis speedup using redis-benchmark, make# sure you also run the benchmark itself in threaded mode, using the# --threads option to match the number of Redis theads, otherwise you'll not# be able to notice the improvements.############################ KERNEL OOM CONTROL ############################### On Linux, it is possible to hint the kernel OOM killer on what processes# should be killed first when out of memory.## Enabling this feature makes Redis actively control the oom_score_adj value# for all its processes, depending on their role. The default scores will# attempt to have background child processes killed before all others, and# replicas killed before masters.oom-score-adj no# When oom-score-adj is used, this directive controls the specific values used# for master, replica and background child processes. Values range -1000 to# 1000 (higher means more likely to be killed).## Unprivileged processes (not root, and without CAP_SYS_RESOURCE capabilities)# can freely increase their value, but not decrease it below its initial# settings.## Values are used relative to the initial value of oom_score_adj when the server# starts. Because typically the initial value is 0, they will often match the# absolute values.oom-score-adj-values 0 200 800############################## APPEND ONLY MODE ################################ By default Redis asynchronously dumps the dataset on disk. This mode is# good enough in many applications, but an issue with the Redis process or# a power outage may result into a few minutes of writes lost (depending on# the configured save points).## The Append Only File is an alternative persistence mode that provides# much better durability. For instance using the default data fsync policy# (see later in the config file) Redis can lose just one second of writes in a# dramatic event like a server power outage, or a single write if something# wrong with the Redis process itself happens, but the operating system is# still running correctly.## AOF and RDB persistence can be enabled at the same time without problems.# If the AOF is enabled on startup Redis will load the AOF, that is the file# with the better durability guarantees.## Please check http://redis.io/topics/persistence for more information.appendonly no# The name of the append only file (default: &quot;appendonly.aof&quot;)appendfilename &quot;appendonly.aof&quot;# The fsync() call tells the Operating System to actually write data on disk# instead of waiting for more data in the output buffer. Some OS will really flush# data on disk, some other OS will just try to do it ASAP.## Redis supports three different modes:## no: don't fsync, just let the OS flush the data when it wants. Faster.# always: fsync after every write to the append only log. Slow, Safest.# everysec: fsync only one time every second. Compromise.## The default is &quot;everysec&quot;, as that's usually the right compromise between# speed and data safety. It's up to you to understand if you can relax this to# &quot;no&quot; that will let the operating system flush the output buffer when# it wants, for better performances (but if you can live with the idea of# some data loss consider the default persistence mode that's snapshotting),# or on the contrary, use &quot;always&quot; that's very slow but a bit safer than# everysec.## More details please check the following article:# http://antirez.com/post/redis-persistence-demystified.html## If unsure, use &quot;everysec&quot;.# appendfsync alwaysappendfsync everysec# appendfsync no# When the AOF fsync policy is set to always or everysec, and a background# saving process (a background save or AOF log background rewriting) is# performing a lot of I/O against the disk, in some Linux configurations# Redis may block too long on the fsync() call. Note that there is no fix for# this currently, as even performing fsync in a different thread will block# our synchronous write(2) call.## In order to mitigate this problem it's possible to use the following option# that will prevent fsync() from being called in the main process while a# BGSAVE or BGREWRITEAOF is in progress.## This means that while another child is saving, the durability of Redis is# the same as &quot;appendfsync none&quot;. In practical terms, this means that it is# possible to lose up to 30 seconds of log in the worst scenario (with the# default Linux settings).## If you have latency problems turn this to &quot;yes&quot;. Otherwise leave it as# &quot;no&quot; that is the safest pick from the point of view of durability.no-appendfsync-on-rewrite no# Automatic rewrite of the append only file.# Redis is able to automatically rewrite the log file implicitly calling# BGREWRITEAOF when the AOF log size grows by the specified percentage.## This is how it works: Redis remembers the size of the AOF file after the# latest rewrite (if no rewrite has happened since the restart, the size of# the AOF at startup is used).## This base size is compared to the current size. If the current size is# bigger than the specified percentage, the rewrite is triggered. Also# you need to specify a minimal size for the AOF file to be rewritten, this# is useful to avoid rewriting the AOF file even if the percentage increase# is reached but it is still pretty small.## Specify a percentage of zero in order to disable the automatic AOF# rewrite feature.auto-aof-rewrite-percentage 100auto-aof-rewrite-min-size 64mb# An AOF file may be found to be truncated at the end during the Redis# startup process, when the AOF data gets loaded back into memory.# This may happen when the system where Redis is running# crashes, especially when an ext4 filesystem is mounted without the# data=ordered option (however this can't happen when Redis itself# crashes or aborts but the operating system still works correctly).## Redis can either exit with an error when this happens, or load as much# data as possible (the default now) and start if the AOF file is found# to be truncated at the end. The following option controls this behavior.## If aof-load-truncated is set to yes, a truncated AOF file is loaded and# the Redis server starts emitting a log to inform the user of the event.# Otherwise if the option is set to no, the server aborts with an error# and refuses to start. When the option is set to no, the user requires# to fix the AOF file using the &quot;redis-check-aof&quot; utility before to restart# the server.## Note that if the AOF file will be found to be corrupted in the middle# the server will still exit with an error. This option only applies when# Redis will try to read more data from the AOF file but not enough bytes# will be found.aof-load-truncated yes# When rewriting the AOF file, Redis is able to use an RDB preamble in the# AOF file for faster rewrites and recoveries. When this option is turned# on the rewritten AOF file is composed of two different stanzas:## [RDB file][AOF tail]## When loading Redis recognizes that the AOF file starts with the &quot;REDIS&quot;# string and loads the prefixed RDB file, and continues loading the AOF# tail.aof-use-rdb-preamble yes################################ LUA SCRIPTING ################################ Max execution time of a Lua script in milliseconds.## If the maximum execution time is reached Redis will log that a script is# still in execution after the maximum allowed time and will start to# reply to queries with an error.## When a long running script exceeds the maximum execution time only the# SCRIPT KILL and SHUTDOWN NOSAVE commands are available. The first can be# used to stop a script that did not yet called write commands. The second# is the only way to shut down the server in the case a write command was# already issued by the script but the user doesn't want to wait for the natural# termination of the script.## Set it to 0 or a negative value for unlimited execution without warnings.lua-time-limit 5000################################ REDIS CLUSTER ################################ Normal Redis instances can't be part of a Redis Cluster; only nodes that are# started as cluster nodes can. In order to start a Redis instance as a# cluster node enable the cluster support uncommenting the following:## cluster-enabled yes# Every cluster node has a cluster configuration file. This file is not# intended to be edited by hand. It is created and updated by Redis nodes.# Every Redis Cluster node requires a different cluster configuration file.# Make sure that instances running in the same system do not have# overlapping cluster configuration file names.## cluster-config-file nodes-6379.conf# Cluster node timeout is the amount of milliseconds a node must be unreachable# for it to be considered in failure state.# Most other internal time limits are multiple of the node timeout.## cluster-node-timeout 15000# A replica of a failing master will avoid to start a failover if its data# looks too old.## There is no simple way for a replica to actually have an exact measure of# its &quot;data age&quot;, so the following two checks are performed:## 1) If there are multiple replicas able to failover, they exchange messages# in order to try to give an advantage to the replica with the best# replication offset (more data from the master processed).# Replicas will try to get their rank by offset, and apply to the start# of the failover a delay proportional to their rank.## 2) Every single replica computes the time of the last interaction with# its master. This can be the last ping or command received (if the master# is still in the &quot;connected&quot; state), or the time that elapsed since the# disconnection with the master (if the replication link is currently down).# If the last interaction is too old, the replica will not try to failover# at all.## The point &quot;2&quot; can be tuned by user. Specifically a replica will not perform# the failover if, since the last interaction with the master, the time# elapsed is greater than:## (node-timeout * replica-validity-factor) + repl-ping-replica-period## So for example if node-timeout is 30 seconds, and the replica-validity-factor# is 10, and assuming a default repl-ping-replica-period of 10 seconds, the# replica will not try to failover if it was not able to talk with the master# for longer than 310 seconds.## A large replica-validity-factor may allow replicas with too old data to failover# a master, while a too small value may prevent the cluster from being able to# elect a replica at all.## For maximum availability, it is possible to set the replica-validity-factor# to a value of 0, which means, that replicas will always try to failover the# master regardless of the last time they interacted with the master.# (However they'll always try to apply a delay proportional to their# offset rank).## Zero is the only value able to guarantee that when all the partitions heal# the cluster will always be able to continue.## cluster-replica-validity-factor 10# Cluster replicas are able to migrate to orphaned masters, that are masters# that are left without working replicas. This improves the cluster ability# to resist to failures as otherwise an orphaned master can't be failed over# in case of failure if it has no working replicas.## Replicas migrate to orphaned masters only if there are still at least a# given number of other working replicas for their old master. This number# is the &quot;migration barrier&quot;. A migration barrier of 1 means that a replica# will migrate only if there is at least 1 other working replica for its master# and so forth. It usually reflects the number of replicas you want for every# master in your cluster.## Default is 1 (replicas migrate only if their masters remain with at least# one replica). To disable migration just set it to a very large value.# A value of 0 can be set but is useful only for debugging and dangerous# in production.## cluster-migration-barrier 1# By default Redis Cluster nodes stop accepting queries if they detect there# is at least an hash slot uncovered (no available node is serving it).# This way if the cluster is partially down (for example a range of hash slots# are no longer covered) all the cluster becomes, eventually, unavailable.# It automatically returns available as soon as all the slots are covered again.## However sometimes you want the subset of the cluster which is working,# to continue to accept queries for the part of the key space that is still# covered. In order to do so, just set the cluster-require-full-coverage# option to no.## cluster-require-full-coverage yes# This option, when set to yes, prevents replicas from trying to failover its# master during master failures. However the master can still perform a# manual failover, if forced to do so.## This is useful in different scenarios, especially in the case of multiple# data center operations, where we want one side to never be promoted if not# in the case of a total DC failure.## cluster-replica-no-failover no# This option, when set to yes, allows nodes to serve read traffic while the# the cluster is in a down state, as long as it believes it owns the slots. ## This is useful for two cases. The first case is for when an application # doesn't require consistency of data during node failures or network partitions.# One example of this is a cache, where as long as the node has the data it# should be able to serve it. ## The second use case is for configurations that don't meet the recommended # three shards but want to enable cluster mode and scale later. A # master outage in a 1 or 2 shard configuration causes a read/write outage to the# entire cluster without this option set, with it set there is only a write outage.# Without a quorum of masters, slot ownership will not change automatically. ## cluster-allow-reads-when-down no# In order to setup your cluster make sure to read the documentation# available at http://redis.io web site.########################## CLUSTER DOCKER/NAT support ######################### In certain deployments, Redis Cluster nodes address discovery fails, because# addresses are NAT-ted or because ports are forwarded (the typical case is# Docker and other containers).## In order to make Redis Cluster working in such environments, a static# configuration where each node knows its public address is needed. The# following two options are used for this scope, and are:## * cluster-announce-ip# * cluster-announce-port# * cluster-announce-bus-port## Each instruct the node about its address, client port, and cluster message# bus port. The information is then published in the header of the bus packets# so that other nodes will be able to correctly map the address of the node# publishing the information.## If the above options are not used, the normal Redis Cluster auto-detection# will be used instead.## Note that when remapped, the bus port may not be at the fixed offset of# clients port + 10000, so you can specify any port and bus-port depending# on how they get remapped. If the bus-port is not set, a fixed offset of# 10000 will be used as usually.## Example:## cluster-announce-ip 10.1.1.5# cluster-announce-port 6379# cluster-announce-bus-port 6380################################## SLOW LOG #################################### The Redis Slow Log is a system to log queries that exceeded a specified# execution time. The execution time does not include the I/O operations# like talking with the client, sending the reply and so forth,# but just the time needed to actually execute the command (this is the only# stage of command execution where the thread is blocked and can not serve# other requests in the meantime).## You can configure the slow log with two parameters: one tells Redis# what is the execution time, in microseconds, to exceed in order for the# command to get logged, and the other parameter is the length of the# slow log. When a new command is logged the oldest one is removed from the# queue of logged commands.# The following time is expressed in microseconds, so 1000000 is equivalent# to one second. Note that a negative number disables the slow log, while# a value of zero forces the logging of every command.slowlog-log-slower-than 10000# There is no limit to this length. Just be aware that it will consume memory.# You can reclaim memory used by the slow log with SLOWLOG RESET.slowlog-max-len 128################################ LATENCY MONITOR ############################### The Redis latency monitoring subsystem samples different operations# at runtime in order to collect data related to possible sources of# latency of a Redis instance.## Via the LATENCY command this information is available to the user that can# print graphs and obtain reports.## The system only logs operations that were performed in a time equal or# greater than the amount of milliseconds specified via the# latency-monitor-threshold configuration directive. When its value is set# to zero, the latency monitor is turned off.## By default latency monitoring is disabled since it is mostly not needed# if you don't have latency issues, and collecting data has a performance# impact, that while very small, can be measured under big load. Latency# monitoring can easily be enabled at runtime using the command# &quot;CONFIG SET latency-monitor-threshold &lt;milliseconds&gt;&quot; if needed.latency-monitor-threshold 0############################# EVENT NOTIFICATION ############################### Redis can notify Pub/Sub clients about events happening in the key space.# This feature is documented at http://redis.io/topics/notifications## For instance if keyspace events notification is enabled, and a client# performs a DEL operation on key &quot;foo&quot; stored in the Database 0, two# messages will be published via Pub/Sub:## PUBLISH __keyspace@0__:foo del# PUBLISH __keyevent@0__:del foo## It is possible to select the events that Redis will notify among a set# of classes. Every class is identified by a single character:## K Keyspace events, published with __keyspace@&lt;db&gt;__ prefix.# E Keyevent events, published with __keyevent@&lt;db&gt;__ prefix.# g Generic commands (non-type specific) like DEL, EXPIRE, RENAME, ...# $ String commands# l List commands# s Set commands# h Hash commands# z Sorted set commands# x Expired events (events generated every time a key expires)# e Evicted events (events generated when a key is evicted for maxmemory)# t Stream commands# m Key-miss events (Note: It is not included in the 'A' class)# A Alias for g$lshzxet, so that the &quot;AKE&quot; string means all the events# (Except key-miss events which are excluded from 'A' due to their# unique nature).## The &quot;notify-keyspace-events&quot; takes as argument a string that is composed# of zero or multiple characters. The empty string means that notifications# are disabled.## Example: to enable list and generic events, from the point of view of the# event name, use:## notify-keyspace-events Elg## Example 2: to get the stream of the expired keys subscribing to channel# name __keyevent@0__:expired use:## notify-keyspace-events Ex## By default all notifications are disabled because most users don't need# this feature and the feature has some overhead. Note that if you don't# specify at least one of K or E, no events will be delivered.notify-keyspace-events &quot;&quot;############################### GOPHER SERVER ################################## Redis contains an implementation of the Gopher protocol, as specified in# the RFC 1436 (https://www.ietf.org/rfc/rfc1436.txt).## The Gopher protocol was very popular in the late '90s. It is an alternative# to the web, and the implementation both server and client side is so simple# that the Redis server has just 100 lines of code in order to implement this# support.## What do you do with Gopher nowadays? Well Gopher never *really* died, and# lately there is a movement in order for the Gopher more hierarchical content# composed of just plain text documents to be resurrected. Some want a simpler# internet, others believe that the mainstream internet became too much# controlled, and it's cool to create an alternative space for people that# want a bit of fresh air.## Anyway for the 10nth birthday of the Redis, we gave it the Gopher protocol# as a gift.## --- HOW IT WORKS? ---## The Redis Gopher support uses the inline protocol of Redis, and specifically# two kind of inline requests that were anyway illegal: an empty request# or any request that starts with &quot;/&quot; (there are no Redis commands starting# with such a slash). Normal RESP2/RESP3 requests are completely out of the# path of the Gopher protocol implementation and are served as usually as well.## If you open a connection to Redis when Gopher is enabled and send it# a string like &quot;/foo&quot;, if there is a key named &quot;/foo&quot; it is served via the# Gopher protocol.## In order to create a real Gopher &quot;hole&quot; (the name of a Gopher site in Gopher# talking), you likely need a script like the following:## https://github.com/antirez/gopher2redis## --- SECURITY WARNING ---## If you plan to put Redis on the internet in a publicly accessible address# to server Gopher pages MAKE SURE TO SET A PASSWORD to the instance.# Once a password is set:## 1. The Gopher server (when enabled, not by default) will still serve# content via Gopher.# 2. However other commands cannot be called before the client will# authenticate.## So use the 'requirepass' option to protect your instance.## To enable Gopher support uncomment the following line and set# the option from no (the default) to yes.## gopher-enabled no############################### ADVANCED CONFIG ################################ Hashes are encoded using a memory efficient data structure when they have a# small number of entries, and the biggest entry does not exceed a given# threshold. These thresholds can be configured using the following directives.hash-max-ziplist-entries 512hash-max-ziplist-value 64# Lists are also encoded in a special way to save a lot of space.# The number of entries allowed per internal list node can be specified# as a fixed maximum size or a maximum number of elements.# For a fixed maximum size, use -5 through -1, meaning:# -5: max size: 64 Kb &lt;-- not recommended for normal workloads# -4: max size: 32 Kb &lt;-- not recommended# -3: max size: 16 Kb &lt;-- probably not recommended# -2: max size: 8 Kb &lt;-- good# -1: max size: 4 Kb &lt;-- good# Positive numbers mean store up to _exactly_ that number of elements# per list node.# The highest performing option is usually -2 (8 Kb size) or -1 (4 Kb size),# but if your use case is unique, adjust the settings as necessary.list-max-ziplist-size -2# Lists may also be compressed.# Compress depth is the number of quicklist ziplist nodes from *each* side of# the list to *exclude* from compression. The head and tail of the list# are always uncompressed for fast push/pop operations. Settings are:# 0: disable all list compression# 1: depth 1 means &quot;don't start compressing until after 1 node into the list,# going from either the head or tail&quot;# So: [head]-&gt;node-&gt;node-&gt;...-&gt;node-&gt;[tail]# [head], [tail] will always be uncompressed; inner nodes will compress.# 2: [head]-&gt;[next]-&gt;node-&gt;node-&gt;...-&gt;node-&gt;[prev]-&gt;[tail]# 2 here means: don't compress head or head-&gt;next or tail-&gt;prev or tail,# but compress all nodes between them.# 3: [head]-&gt;[next]-&gt;[next]-&gt;node-&gt;node-&gt;...-&gt;node-&gt;[prev]-&gt;[prev]-&gt;[tail]# etc.list-compress-depth 0# Sets have a special encoding in just one case: when a set is composed# of just strings that happen to be integers in radix 10 in the range# of 64 bit signed integers.# The following configuration setting sets the limit in the size of the# set in order to use this special memory saving encoding.set-max-intset-entries 512# Similarly to hashes and lists, sorted sets are also specially encoded in# order to save a lot of space. This encoding is only used when the length and# elements of a sorted set are below the following limits:zset-max-ziplist-entries 128zset-max-ziplist-value 64# HyperLogLog sparse representation bytes limit. The limit includes the# 16 bytes header. When an HyperLogLog using the sparse representation crosses# this limit, it is converted into the dense representation.## A value greater than 16000 is totally useless, since at that point the# dense representation is more memory efficient.## The suggested value is ~ 3000 in order to have the benefits of# the space efficient encoding without slowing down too much PFADD,# which is O(N) with the sparse encoding. The value can be raised to# ~ 10000 when CPU is not a concern, but space is, and the data set is# composed of many HyperLogLogs with cardinality in the 0 - 15000 range.hll-sparse-max-bytes 3000# Streams macro node max size / items. The stream data structure is a radix# tree of big nodes that encode multiple items inside. Using this configuration# it is possible to configure how big a single node can be in bytes, and the# maximum number of items it may contain before switching to a new node when# appending new stream entries. If any of the following settings are set to# zero, the limit is ignored, so for instance it is possible to set just a# max entires limit by setting max-bytes to 0 and max-entries to the desired# value.stream-node-max-bytes 4096stream-node-max-entries 100# Active rehashing uses 1 millisecond every 100 milliseconds of CPU time in# order to help rehashing the main Redis hash table (the one mapping top-level# keys to values). The hash table implementation Redis uses (see dict.c)# performs a lazy rehashing: the more operation you run into a hash table# that is rehashing, the more rehashing &quot;steps&quot; are performed, so if the# server is idle the rehashing is never complete and some more memory is used# by the hash table.## The default is to use this millisecond 10 times every second in order to# actively rehash the main dictionaries, freeing memory when possible.## If unsure:# use &quot;activerehashing no&quot; if you have hard latency requirements and it is# not a good thing in your environment that Redis can reply from time to time# to queries with 2 milliseconds delay.## use &quot;activerehashing yes&quot; if you don't have such hard requirements but# want to free memory asap when possible.activerehashing yes# The client output buffer limits can be used to force disconnection of clients# that are not reading data from the server fast enough for some reason (a# common reason is that a Pub/Sub client can't consume messages as fast as the# publisher can produce them).## The limit can be set differently for the three different classes of clients:## normal -&gt; normal clients including MONITOR clients# replica -&gt; replica clients# pubsub -&gt; clients subscribed to at least one pubsub channel or pattern## The syntax of every client-output-buffer-limit directive is the following:## client-output-buffer-limit &lt;class&gt; &lt;hard limit&gt; &lt;soft limit&gt; &lt;soft seconds&gt;## A client is immediately disconnected once the hard limit is reached, or if# the soft limit is reached and remains reached for the specified number of# seconds (continuously).# So for instance if the hard limit is 32 megabytes and the soft limit is# 16 megabytes / 10 seconds, the client will get disconnected immediately# if the size of the output buffers reach 32 megabytes, but will also get# disconnected if the client reaches 16 megabytes and continuously overcomes# the limit for 10 seconds.## By default normal clients are not limited because they don't receive data# without asking (in a push way), but just after a request, so only# asynchronous clients may create a scenario where data is requested faster# than it can read.## Instead there is a default limit for pubsub and replica clients, since# subscribers and replicas receive data in a push fashion.## Both the hard or the soft limit can be disabled by setting them to zero.client-output-buffer-limit normal 0 0 0client-output-buffer-limit replica 256mb 64mb 60client-output-buffer-limit pubsub 32mb 8mb 60# Client query buffers accumulate new commands. They are limited to a fixed# amount by default in order to avoid that a protocol desynchronization (for# instance due to a bug in the client) will lead to unbound memory usage in# the query buffer. However you can configure it here if you have very special# needs, such us huge multi/exec requests or alike.## client-query-buffer-limit 1gb# In the Redis protocol, bulk requests, that are, elements representing single# strings, are normally limited ot 512 mb. However you can change this limit# here, but must be 1mb or greater## proto-max-bulk-len 512mb# Redis calls an internal function to perform many background tasks, like# closing connections of clients in timeout, purging expired keys that are# never requested, and so forth.## Not all tasks are performed with the same frequency, but Redis checks for# tasks to perform according to the specified &quot;hz&quot; value.## By default &quot;hz&quot; is set to 10. Raising the value will use more CPU when# Redis is idle, but at the same time will make Redis more responsive when# there are many keys expiring at the same time, and timeouts may be# handled with more precision.## The range is between 1 and 500, however a value over 100 is usually not# a good idea. Most users should use the default of 10 and raise this up to# 100 only in environments where very low latency is required.hz 10# Normally it is useful to have an HZ value which is proportional to the# number of clients connected. This is useful in order, for instance, to# avoid too many clients are processed for each background task invocation# in order to avoid latency spikes.## Since the default HZ value by default is conservatively set to 10, Redis# offers, and enables by default, the ability to use an adaptive HZ value# which will temporary raise when there are many connected clients.## When dynamic HZ is enabled, the actual configured HZ will be used# as a baseline, but multiples of the configured HZ value will be actually# used as needed once more clients are connected. In this way an idle# instance will use very little CPU time while a busy instance will be# more responsive.dynamic-hz yes# When a child rewrites the AOF file, if the following option is enabled# the file will be fsync-ed every 32 MB of data generated. This is useful# in order to commit the file to the disk more incrementally and avoid# big latency spikes.aof-rewrite-incremental-fsync yes# When redis saves RDB file, if the following option is enabled# the file will be fsync-ed every 32 MB of data generated. This is useful# in order to commit the file to the disk more incrementally and avoid# big latency spikes.rdb-save-incremental-fsync yes# Redis LFU eviction (see maxmemory setting) can be tuned. However it is a good# idea to start with the default settings and only change them after investigating# how to improve the performances and how the keys LFU change over time, which# is possible to inspect via the OBJECT FREQ command.## There are two tunable parameters in the Redis LFU implementation: the# counter logarithm factor and the counter decay time. It is important to# understand what the two parameters mean before changing them.## The LFU counter is just 8 bits per key, it's maximum value is 255, so Redis# uses a probabilistic increment with logarithmic behavior. Given the value# of the old counter, when a key is accessed, the counter is incremented in# this way:## 1. A random number R between 0 and 1 is extracted.# 2. A probability P is calculated as 1/(old_value*lfu_log_factor+1).# 3. The counter is incremented only if R &lt; P.## The default lfu-log-factor is 10. This is a table of how the frequency# counter changes with a different number of accesses with different# logarithmic factors:## +--------+------------+------------+------------+------------+------------+# | factor | 100 hits | 1000 hits | 100K hits | 1M hits | 10M hits |# +--------+------------+------------+------------+------------+------------+# | 0 | 104 | 255 | 255 | 255 | 255 |# +--------+------------+------------+------------+------------+------------+# | 1 | 18 | 49 | 255 | 255 | 255 |# +--------+------------+------------+------------+------------+------------+# | 10 | 10 | 18 | 142 | 255 | 255 |# +--------+------------+------------+------------+------------+------------+# | 100 | 8 | 11 | 49 | 143 | 255 |# +--------+------------+------------+------------+------------+------------+## NOTE: The above table was obtained by running the following commands:## redis-benchmark -n 1000000 incr foo# redis-cli object freq foo## NOTE 2: The counter initial value is 5 in order to give new objects a chance# to accumulate hits.## The counter decay time is the time, in minutes, that must elapse in order# for the key counter to be divided by two (or decremented if it has a value# less &lt;= 10).## The default value for the lfu-decay-time is 1. A Special value of 0 means to# decay the counter every time it happens to be scanned.## lfu-log-factor 10# lfu-decay-time 1########################### ACTIVE DEFRAGMENTATION ######################### What is active defragmentation?# -------------------------------## Active (online) defragmentation allows a Redis server to compact the# spaces left between small allocations and deallocations of data in memory,# thus allowing to reclaim back memory.## Fragmentation is a natural process that happens with every allocator (but# less so with Jemalloc, fortunately) and certain workloads. Normally a server# restart is needed in order to lower the fragmentation, or at least to flush# away all the data and create it again. However thanks to this feature# implemented by Oran Agra for Redis 4.0 this process can happen at runtime# in an &quot;hot&quot; way, while the server is running.## Basically when the fragmentation is over a certain level (see the# configuration options below) Redis will start to create new copies of the# values in contiguous memory regions by exploiting certain specific Jemalloc# features (in order to understand if an allocation is causing fragmentation# and to allocate it in a better place), and at the same time, will release the# old copies of the data. This process, repeated incrementally for all the keys# will cause the fragmentation to drop back to normal values.## Important things to understand:## 1. This feature is disabled by default, and only works if you compiled Redis# to use the copy of Jemalloc we ship with the source code of Redis.# This is the default with Linux builds.## 2. You never need to enable this feature if you don't have fragmentation# issues.## 3. Once you experience fragmentation, you can enable this feature when# needed with the command &quot;CONFIG SET activedefrag yes&quot;.## The configuration parameters are able to fine tune the behavior of the# defragmentation process. If you are not sure about what they mean it is# a good idea to leave the defaults untouched.# Enabled active defragmentation# activedefrag no# Minimum amount of fragmentation waste to start active defrag# active-defrag-ignore-bytes 100mb# Minimum percentage of fragmentation to start active defrag# active-defrag-threshold-lower 10# Maximum percentage of fragmentation at which we use maximum effort# active-defrag-threshold-upper 100# Minimal effort for defrag in CPU percentage, to be used when the lower# threshold is reached# active-defrag-cycle-min 1# Maximal effort for defrag in CPU percentage, to be used when the upper# threshold is reached# active-defrag-cycle-max 25# Maximum number of set/hash/zset/list fields that will be processed from# the main dictionary scan# active-defrag-max-scan-fields 1000# Jemalloc background thread for purging will be enabled by defaultjemalloc-bg-thread yes# It is possible to pin different threads and processes of Redis to specific# CPUs in your system, in order to maximize the performances of the server.# This is useful both in order to pin different Redis threads in different# CPUs, but also in order to make sure that multiple Redis instances running# in the same host will be pinned to different CPUs.## Normally you can do this using the &quot;taskset&quot; command, however it is also# possible to this via Redis configuration directly, both in Linux and FreeBSD.## You can pin the server/IO threads, bio threads, aof rewrite child process, and# the bgsave child process. The syntax to specify the cpu list is the same as# the taskset command:## Set redis server/io threads to cpu affinity 0,2,4,6:# server_cpulist 0-7:2## Set bio threads to cpu affinity 1,3:# bio_cpulist 1,3## Set aof rewrite child process to cpu affinity 8,9,10,11:# aof_rewrite_cpulist 8-11## Set bgsave child process to cpu affinity 1,10,11# bgsave_cpulist 1,10-11","link":"/redis-config/"},{"title":"jvm运行时数据区","text":"今天重温jvm运行时的数据区，写成文档，使自己有更深的理解 一张图镇楼 Java的内存布局分为5大块，分别是堆区、方法区、虚拟机栈、本地方法栈、程序计数器。 按照线程是否共享： 堆区和方法区是线程共享的，创建进程就创建该区域虚拟机栈、本地方法栈、程序计数器是线程私有的，创建线程时该区域才会产生。 方法区 首先声明一点方法区只是一个概念， 在1.8以前实现方式是永久代（PermGen），且在jvm中开辟内存，stringTable在常量池中。 在1.8后，实现方式改成了元空间（Metaspace），且在本地内存中开辟 组成 运行时常量池 首先写一段代码，然后编译成字节码文件 public class TestCooper { public static Long aa = 3L; static { aa = 1L; } public static void main(String[] args) { System.out.println(&quot;111&quot;); }} javac -g TestCooper.java 然后反编译 javap -v TestCooper.class Classfile /Users/cooper/Projects/Cooper/StudyPlan/mrhy-grow-up/mrhy-jvm/src/main/java/com/mrhy/jvm/classloader/TestCooper.class Last modified 2022-10-28; size 762 bytes MD5 checksum e8ce86c6f3ab010de04cb529fa73b6ff Compiled from &quot;TestCooper.java&quot;public class com.mrhy.jvm.classloader.TestCooper minor version: 0 major version: 52 flags: ACC_PUBLIC, ACC_SUPERConstant pool: #1 = Methodref #10.#27 // java/lang/Object.&quot;&lt;init&gt;&quot;:()V #2 = Fieldref #28.#29 // java/lang/System.out:Ljava/io/PrintStream; #3 = String #30 // 111 #4 = Methodref #31.#32 // java/io/PrintStream.println:(Ljava/lang/String;)V #5 = Long 3l #7 = Methodref #33.#34 // java/lang/Long.valueOf:(J)Ljava/lang/Long; #8 = Fieldref #9.#35 // com/mrhy/jvm/classloader/TestCooper.aa:Ljava/lang/Long; #9 = Class #36 // com/mrhy/jvm/classloader/TestCooper #10 = Class #37 // java/lang/Object #11 = Utf8 aa #12 = Utf8 Ljava/lang/Long; #13 = Utf8 &lt;init&gt; #14 = Utf8 ()V #15 = Utf8 Code #16 = Utf8 LineNumberTable #17 = Utf8 LocalVariableTable #18 = Utf8 this #19 = Utf8 Lcom/mrhy/jvm/classloader/TestCooper; #20 = Utf8 main #21 = Utf8 ([Ljava/lang/String;)V #22 = Utf8 args #23 = Utf8 [Ljava/lang/String; #24 = Utf8 &lt;clinit&gt; #25 = Utf8 SourceFile #26 = Utf8 TestCooper.java #27 = NameAndType #13:#14 // &quot;&lt;init&gt;&quot;:()V #28 = Class #38 // java/lang/System #29 = NameAndType #39:#40 // out:Ljava/io/PrintStream; #30 = Utf8 111 #31 = Class #41 // java/io/PrintStream #32 = NameAndType #42:#43 // println:(Ljava/lang/String;)V #33 = Class #44 // java/lang/Long #34 = NameAndType #45:#46 // valueOf:(J)Ljava/lang/Long; #35 = NameAndType #11:#12 // aa:Ljava/lang/Long; #36 = Utf8 com/mrhy/jvm/classloader/TestCooper #37 = Utf8 java/lang/Object #38 = Utf8 java/lang/System #39 = Utf8 out #40 = Utf8 Ljava/io/PrintStream; #41 = Utf8 java/io/PrintStream #42 = Utf8 println #43 = Utf8 (Ljava/lang/String;)V #44 = Utf8 java/lang/Long #45 = Utf8 valueOf #46 = Utf8 (J)Ljava/lang/Long;{ public static java.lang.Long aa; descriptor: Ljava/lang/Long; flags: ACC_PUBLIC, ACC_STATIC public com.mrhy.jvm.classloader.TestCooper(); descriptor: ()V flags: ACC_PUBLIC Code: stack=1, locals=1, args_size=1 0: aload_0 1: invokespecial #1 // Method java/lang/Object.&quot;&lt;init&gt;&quot;:()V 4: return LineNumberTable: line 8: 0 LocalVariableTable: Start Length Slot Name Signature 0 5 0 this Lcom/mrhy/jvm/classloader/TestCooper; public static void main(java.lang.String[]); descriptor: ([Ljava/lang/String;)V flags: ACC_PUBLIC, ACC_STATIC Code: stack=2, locals=1, args_size=1 0: getstatic #2 // Field java/lang/System.out:Ljava/io/PrintStream; 3: ldc #3 // String 111 5: invokevirtual #4 // Method java/io/PrintStream.println:(Ljava/lang/String;)V 8: return LineNumberTable: line 16: 0 line 17: 8 LocalVariableTable: Start Length Slot Name Signature 0 9 0 args [Ljava/lang/String; static {}; descriptor: ()V flags: ACC_STATIC Code: stack=2, locals=0, args_size=0 0: ldc2_w #5 // long 3l 3: invokestatic #7 // Method java/lang/Long.valueOf:(J)Ljava/lang/Long; 6: putstatic #8 // Field aa:Ljava/lang/Long; 9: lconst_1 10: invokestatic #7 // Method java/lang/Long.valueOf:(J)Ljava/lang/Long; 13: putstatic #8 // Field aa:Ljava/lang/Long; 16: return LineNumberTable: line 9: 0 line 12: 9 line 13: 16}SourceFile: &quot;TestCooper.java&quot; 可以看到，一个字节码信息包含类基本信息、常量池、类方法定义（包含了虚拟机指令） 常量池：就是一张表，虚拟机指令根据这张常量表找到要执行的类型、方法名、参数类型、字面量等信息。 运行时的常量池：就是当某个类被加载的时候，他的常量池信息会加入到运行时的常量池中，并把里面的符号地址改为真实地址。 StringTable俗话说的串池，本质是一个HashTable，不可扩容 特性如下 1、常量池中的字符串仅是符号，第一次用到时才变为对象2、利用串池的机制，来避免重复创建字符串对象3、字符串变量拼接的原理是 StringBuilder （1.8）4、字符串常量拼接的原理是编译期优化5、可以使用 intern 方法，主动将串池中还没有的字符串对象放入串池5.1、1.8 将这个字符串对象尝试放入串池，如果有则并不会放入，如果没有则放入串池， 会把串池中的对象返回5.2、1.6 将这个字符串对象尝试放入串池，如果有则并不会放入，如果没有会把此对象复制一份，放入串池， 会把串池中的对象返回 例子： 先写一串代码 package com.mrhy.jvm.methodarea;/** * @author cooper * @description * @date 2022/10/28 10:11 */public class StringTableDemo { public static void main(String[] args) { String a = &quot;a&quot;; String b = &quot;b&quot;; String c = &quot;ab&quot;; }} 然后编译+反编译 javac -g StringTableDemo.javajavap -v StringTableDemo 输出如下 Classfile /Users/cooper/Projects/Cooper/StudyPlan/mrhy-grow-up/mrhy-jvm/src/main/java/com/mrhy/jvm/methodarea/StringTableDemo.class Last modified 2022-10-28; size 534 bytes MD5 checksum 296186739ed59757670edf8cc51a808b Compiled from &quot;StringTableDemo.java&quot;public class com.mrhy.jvm.methodarea.StringTableDemo minor version: 0 major version: 52 flags: ACC_PUBLIC, ACC_SUPERConstant pool: #1 = Methodref #6.#24 // java/lang/Object.&quot;&lt;init&gt;&quot;:()V #2 = String #18 // a #3 = String #20 // b #4 = String #25 // ab #5 = Class #26 // com/mrhy/jvm/methodarea/StringTableDemo #6 = Class #27 // java/lang/Object #7 = Utf8 &lt;init&gt; #8 = Utf8 ()V #9 = Utf8 Code #10 = Utf8 LineNumberTable #11 = Utf8 LocalVariableTable #12 = Utf8 this #13 = Utf8 Lcom/mrhy/jvm/methodarea/StringTableDemo; #14 = Utf8 main #15 = Utf8 ([Ljava/lang/String;)V #16 = Utf8 args #17 = Utf8 [Ljava/lang/String; #18 = Utf8 a #19 = Utf8 Ljava/lang/String; #20 = Utf8 b #21 = Utf8 c #22 = Utf8 SourceFile #23 = Utf8 StringTableDemo.java #24 = NameAndType #7:#8 // &quot;&lt;init&gt;&quot;:()V #25 = Utf8 ab #26 = Utf8 com/mrhy/jvm/methodarea/StringTableDemo #27 = Utf8 java/lang/Object{ public com.mrhy.jvm.methodarea.StringTableDemo(); descriptor: ()V flags: ACC_PUBLIC Code: stack=1, locals=1, args_size=1 0: aload_0 1: invokespecial #1 // Method java/lang/Object.&quot;&lt;init&gt;&quot;:()V 4: return LineNumberTable: line 8: 0 LocalVariableTable: Start Length Slot Name Signature 0 5 0 this Lcom/mrhy/jvm/methodarea/StringTableDemo; public static void main(java.lang.String[]); descriptor: ([Ljava/lang/String;)V flags: ACC_PUBLIC, ACC_STATIC Code: stack=1, locals=4, args_size=1 0: ldc #2 // String a 2: astore_1 3: ldc #3 // String b 5: astore_2 6: ldc #4 // String ab 8: astore_3 9: return LineNumberTable: line 10: 0 line 11: 3 line 12: 6 line 13: 9 LocalVariableTable: Start Length Slot Name Signature 0 10 0 args [Ljava/lang/String; 3 7 1 a Ljava/lang/String; 6 4 2 b Ljava/lang/String; 9 1 3 c Ljava/lang/String;} 常量池中的信息，会被加载到运行时的常量池中，这时候的a，b,c都是常量池中的符号，还没有变成java字符串对象 此时StringTable为空[] 当执行到ldc #2的时候，此时先看StringTable中有没有”a“变量，没有则会新增，会把a符号变为”a“字符串变量， StringTable [“a”] 当执行到ldc #3的时候，此时先看StringTable中有没有”b“变量，没有则会新增，会把a符号变为”a“字符串变量， StringTable [“a”,”b”] 当执行到ldc #4的时候，此时先看StringTable中有没有”ab“变量，没有则会新增，会把a符号变为”a“字符串变量 StringTable [“a”,”b”,”ab”] 再将代码变形，新增加s4 public class StringTableDemo { public static void main(String[] args) { String a = &quot;a&quot;; String b = &quot;b&quot;; String f=&quot;a&quot;; String c = &quot;ab&quot;; String d = a + b; System.out.println(c==d); }} 继续编译+反编译 Classfile /Users/cooper/Projects/Cooper/StudyPlan/mrhy-grow-up/mrhy-jvm/src/main/java/com/mrhy/jvm/methodarea/StringTableDemo.class Last modified 2022-10-28; size 984 bytes MD5 checksum de818f5c9af6aca12c4e6163d5bf2109 Compiled from &quot;StringTableDemo.java&quot;public class com.mrhy.jvm.methodarea.StringTableDemo minor version: 0 major version: 52 flags: ACC_PUBLIC, ACC_SUPERConstant pool: #1 = Methodref #12.#36 // java/lang/Object.&quot;&lt;init&gt;&quot;:()V #2 = String #24 // a #3 = String #26 // b #4 = String #37 // ab #5 = Class #38 // java/lang/StringBuilder #6 = Methodref #5.#36 // java/lang/StringBuilder.&quot;&lt;init&gt;&quot;:()V #7 = Methodref #5.#39 // java/lang/StringBuilder.append:(Ljava/lang/String;)Ljava/lang/StringBuilder; #8 = Methodref #5.#40 // java/lang/StringBuilder.toString:()Ljava/lang/String; #9 = Fieldref #41.#42 // java/lang/System.out:Ljava/io/PrintStream; #10 = Methodref #43.#44 // java/io/PrintStream.println:(Z)V #11 = Class #45 // com/mrhy/jvm/methodarea/StringTableDemo #12 = Class #46 // java/lang/Object #13 = Utf8 &lt;init&gt; #14 = Utf8 ()V #15 = Utf8 Code #16 = Utf8 LineNumberTable #17 = Utf8 LocalVariableTable #18 = Utf8 this #19 = Utf8 Lcom/mrhy/jvm/methodarea/StringTableDemo; #20 = Utf8 main #21 = Utf8 ([Ljava/lang/String;)V #22 = Utf8 args #23 = Utf8 [Ljava/lang/String; #24 = Utf8 a #25 = Utf8 Ljava/lang/String; #26 = Utf8 b #27 = Utf8 f #28 = Utf8 c #29 = Utf8 d #30 = Utf8 StackMapTable #31 = Class #23 // &quot;[Ljava/lang/String;&quot; #32 = Class #47 // java/lang/String #33 = Class #48 // java/io/PrintStream #34 = Utf8 SourceFile #35 = Utf8 StringTableDemo.java #36 = NameAndType #13:#14 // &quot;&lt;init&gt;&quot;:()V #37 = Utf8 ab #38 = Utf8 java/lang/StringBuilder #39 = NameAndType #49:#50 // append:(Ljava/lang/String;)Ljava/lang/StringBuilder; #40 = NameAndType #51:#52 // toString:()Ljava/lang/String; #41 = Class #53 // java/lang/System #42 = NameAndType #54:#55 // out:Ljava/io/PrintStream; #43 = Class #48 // java/io/PrintStream #44 = NameAndType #56:#57 // println:(Z)V #45 = Utf8 com/mrhy/jvm/methodarea/StringTableDemo #46 = Utf8 java/lang/Object #47 = Utf8 java/lang/String #48 = Utf8 java/io/PrintStream #49 = Utf8 append #50 = Utf8 (Ljava/lang/String;)Ljava/lang/StringBuilder; #51 = Utf8 toString #52 = Utf8 ()Ljava/lang/String; #53 = Utf8 java/lang/System #54 = Utf8 out #55 = Utf8 Ljava/io/PrintStream; #56 = Utf8 println #57 = Utf8 (Z)V{ public com.mrhy.jvm.methodarea.StringTableDemo(); descriptor: ()V flags: ACC_PUBLIC Code: stack=1, locals=1, args_size=1 0: aload_0 1: invokespecial #1 // Method java/lang/Object.&quot;&lt;init&gt;&quot;:()V 4: return LineNumberTable: line 8: 0 LocalVariableTable: Start Length Slot Name Signature 0 5 0 this Lcom/mrhy/jvm/methodarea/StringTableDemo; public static void main(java.lang.String[]); descriptor: ([Ljava/lang/String;)V flags: ACC_PUBLIC, ACC_STATIC Code: stack=3, locals=6, args_size=1 0: ldc #2 // String a 2: astore_1 3: ldc #3 // String b 5: astore_2 6: ldc #2 // String a 8: astore_3 9: ldc #4 // String ab 11: astore 4 13: new #5 // class java/lang/StringBuilder 16: dup 17: invokespecial #6 // Method java/lang/StringBuilder.&quot;&lt;init&gt;&quot;:()V 20: aload_1 21: invokevirtual #7 // Method java/lang/StringBuilder.append:(Ljava/lang/String;)Ljava/lang/StringBuilder; 24: aload_2 25: invokevirtual #7 // Method java/lang/StringBuilder.append:(Ljava/lang/String;)Ljava/lang/StringBuilder; 28: invokevirtual #8 // Method java/lang/StringBuilder.toString:()Ljava/lang/String; 31: astore 5 33: getstatic #9 // Field java/lang/System.out:Ljava/io/PrintStream; 36: aload 4 38: aload 5 40: if_acmpne 47 43: iconst_1 44: goto 48 47: iconst_0 48: invokevirtual #10 // Method java/io/PrintStream.println:(Z)V 51: return LineNumberTable: line 10: 0 line 11: 3 line 12: 6 line 13: 9 line 14: 13 line 15: 33 line 16: 51 LocalVariableTable: Start Length Slot Name Signature 0 52 0 args [Ljava/lang/String; 3 49 1 a Ljava/lang/String; 6 46 2 b Ljava/lang/String; 9 43 3 f Ljava/lang/String; 13 39 4 c Ljava/lang/String; 33 19 5 d Ljava/lang/String; StackMapTable: number_of_entries = 2 frame_type = 255 /* full_frame */ offset_delta = 47 locals = [ class &quot;[Ljava/lang/String;&quot;, class java/lang/String, class java/lang/String, class java/lang/String, class java/lang/String, class java/lang/String ] stack = [ class java/io/PrintStream ] frame_type = 255 /* full_frame */ offset_delta = 0 locals = [ class &quot;[Ljava/lang/String;&quot;, class java/lang/String, class java/lang/String, class java/lang/String, class java/lang/String, class java/lang/String ] stack = [ class java/io/PrintStream, int ]}SourceFile: &quot;StringTableDemo.java&quot; a+b,最后会new出一个新String，所以最后的输出结果是false intern方法public class Demo1_23 { // [&quot;ab&quot;, &quot;a&quot;, &quot;b&quot;] public static void main(String[] args) { String x = &quot;ab&quot;; String s = new String(&quot;a&quot;) + new String(&quot;b&quot;); // 堆 new String(&quot;a&quot;) new String(&quot;b&quot;) new String(&quot;ab&quot;) String s2 = s.intern(); // 将这个字符串对象尝试放入串池，如果有则并不会放入，如果没有则放入串池， 会把串池中的对象返回 System.out.println( s2 == x);//true System.out.println( s == x );//false }} 垃圾回收StringTable有垃圾回收。 /** * @author cooper * @description -Xmx10m -XX:+PrintStringTableStatistics -XX:+PrintGCDetails -verbose:gc * @date 2022/10/29 20:48 */public class StringTableGc { public static void main(String[] args) { int i=0; try{ for (int j = 0; j &lt; 10000; j++) { String.valueOf(j).intern(); i++; } }catch (Throwable e){ e.printStackTrace(); } finally { System.out.println(i); } }} [GC (Allocation Failure) [PSYoungGen: 2048K-&gt;512K(2560K)] 2048K-&gt;560K(9728K), 0.0013573 secs] [Times: user=0.01 sys=0.00, real=0.00 secs] 10000Heap PSYoungGen total 2560K, used 776K [0x00000007bfd00000, 0x00000007c0000000, 0x00000007c0000000) eden space 2048K, 12% used [0x00000007bfd00000,0x00000007bfd42038,0x00000007bff00000) from space 512K, 100% used [0x00000007bff00000,0x00000007bff80000,0x00000007bff80000) to space 512K, 0% used [0x00000007bff80000,0x00000007bff80000,0x00000007c0000000) ParOldGen total 7168K, used 48K [0x00000007bf600000, 0x00000007bfd00000, 0x00000007bfd00000) object space 7168K, 0% used [0x00000007bf600000,0x00000007bf60c000,0x00000007bfd00000) Metaspace used 2968K, capacity 4486K, committed 4864K, reserved 1056768K class space used 317K, capacity 386K, committed 512K, reserved 1048576KDisconnected from the target VM, address: '127.0.0.1:53535', transport: 'socket'SymbolTable statistics:Number of buckets : 20011 = 160088 bytes, avg 8.000Number of entries : 11982 = 287568 bytes, avg 24.000Number of literals : 11982 = 470112 bytes, avg 39.235Total footprint : = 917768 bytesAverage bucket size : 0.599Variance of bucket size : 0.603Std. dev. of bucket size: 0.776Maximum bucket size : 6StringTable statistics:Number of buckets : 60013 = 480104 bytes, avg 8.000Number of entries : 6315 = 151560 bytes, avg 24.000Number of literals : 6315 = 343368 bytes, avg 54.373Total footprint : = 975032 bytesAverage bucket size : 0.105Variance of bucket size : 0.107Std. dev. of bucket size: 0.328Maximum bucket size : 3 可以看到 number of buckets 的对象不是1w+，而是6315，且上述的输出发生了内存回收 StringTable调优底层类似于是hashTable的结构，所以桶的数量决定查找和放的性能 Number of buckets : 60013 = 480104 bytes, avg 8.000 默认为60013，可以通过调整*-XX:StringTableSize=100009* 直接内存定义Direct Memory 常见于NIO操作时，用于数据缓冲区 分配回收成本比较高，但读写性能高 不受JVM内存回收管理 回收原理及机制传统的BIO 直接内存读写机制 直接内存的垃圾回收不是由jvm的内存回收管理的。但是java依然能主动对直接内存进行回收，其实现原理如下 那具体是怎么释放的呢？ 玄机就在allocateDirect（）函数里： unsafe函数释放直接内存（直接内存的释放原理） 那具体是怎么在底层调用unsafe的呢？我们再往下看： 点开allocateDirect（） 点开DirectByteBuffer() 点开Deallocator函数。 cleaner的clean函数会执行Dealloctor的run()方法，调用unsafe函数释放 显示gc导致的直接内存难以回收的应对方法 显示gc指的就是System.gc()指的是程序员写的gc。 堆垃圾回收如何判断对象可以回收 引用计数法 可达分析法 五种引用 强引用 只有所有GC Roots对象都不通过【强引用】引用该对象，该对象才能被垃圾回收 软引用（SoftReference） 仅有软引用引用该对象时，在垃圾回收后，内存仍不足时会再次触发垃圾回收，回收软引用对象 可以配合引用队列来释放软引用自身（ReferenceQueue） public class ReferenceDemo { private static final int _4MB = 4 * 1024 * 1024; public static void main(String[] args) throws IOException { soft(); } public static void soft(){ // 引用队列 // list--------------&gt;SoftReference---------byte[] ReferenceQueue&lt;byte[]&gt;referenceQueue=new ReferenceQueue&lt;&gt;(); List&lt;SoftReference&lt;byte[]&gt;&gt;bytes=new ArrayList&lt;&gt;(); for (int i = 0; i &lt; 5; i++) { //关联了引用对列 SoftReference&lt;byte[]&gt; ref=new SoftReference&lt;&gt;(new byte[_4MB],referenceQueue); System.out.println(ref.get()); bytes.add(ref); } Reference&lt;? extends byte[]&gt; poll = referenceQueue.poll(); while (poll!=null){ bytes.remove(poll); poll=referenceQueue.poll(); } System.out.println(&quot;----------------------&quot;); for (SoftReference&lt;byte[]&gt; aByte : bytes) { System.out.println(aByte.get()); } }} 弱引用（WeakReference） 仅有弱引用引用该对象时，在垃圾回收时，无论内存是否充足，都会回收弱引用对象 可以配合引用队列来释放软引用自身（ReferenceQueue） 虚引用（PhantomReference） 必须配合引用队列使用，主要配合ByteBuffer使用，被引用对象回收时，会将虚引用入队列，由Reference Handler 线程调用虚引用相关方法释放内存 终结器引用（FinalReference） 无需手动编码，但其内部配合引用队列使用，在垃圾回收时，终结器引用入队（被引用对象暂时没有被回收），再由Finalizer线程通过终结器引用找到引用对象并调用他的finalize方法，第二次GC时才能回收被引用对象 垃圾回收算法标记清除算法（Mark Sweep） 标记整理算法（Mark Compact） 复制（Copy） 分代垃圾回收 对象首先分配在伊甸园区域 新生代空间不足时，触发 minor gc，伊甸园和 from 存活的对象使用 copy 复制到 to 中，存活的 对象年龄加 1并且交换 from to minor gc 会引发 stop the world，暂停其它用户的线程，等垃圾回收结束，用户线程才恢复运行 当对象寿命超过阈值时，会晋升至老年代，最大寿命是15(4bit) 当老年代空间不足，会先尝试触发 minor gc，如果之后空间仍不足，那么触发 full gc，STW的时 间更长 相关参数 堆设置 -Xms:初始堆大小 默认为物理内存的1/64 -Xmx:最大堆大小 默认为物理内存的1/4 -Xmn:新生代大小 -XX:NewRatio:设置新生代和老年代的比值。如：为3，表示年轻代与老年代比值为1：3 -XX:SurvivorRatio:新生代中Eden区与两个Survivor区的比值。注意Survivor区有两个。如：为3，表示Eden：Survivor=3：2，一个Survivor区占整个新生代的1/5 -XX:MaxTenuringThreshold:设置转入老年代的存活次数。如果是0，则直接跳过新生代进入老年代 -XX:PermSize、**-XX:MaxPermSize**:分别设置永久代最小大小与最大大小（Java8以前） -XX:MetaspaceSize、**-XX:MaxMetaspaceSize**:分别设置元空间最小大小与最大大小（Java8以后） 收集器设置 -XX:+UseSerialGC:设置串行收集器 -XX:+UseParallelGC:设置并行收集器 -XX:+UseParalledlOldGC:设置并行老年代收集器 -XX:+UseConcMarkSweepGC:设置并发收集器 垃圾回收统计信息 -XX:+PrintGC -XX:+PrintGCDetails -XX:+PrintGCTimeStamps -Xloggc:filename 并行收集器设置 -XX:ParallelGCThreads=n:设置并行收集器收集时使用的CPU数。并行收集线程数。 -XX:MaxGCPauseMillis=n:设置并行收集最大暂停时间 -XX:GCTimeRatio=n:设置垃圾回收时间占程序运行时间的百分比。公式为1/(1+n) 并发收集器设置 -XX:+CMSIncrementalMode:设置为增量模式。适用于单CPU情况。 -XX:ParallelGCThreads=n:设置并发收集器新生代收集方式为并行收集时，使用的CPU数。并行收集线程数。 垃圾回收器串行 单线程 堆内存小，适合个人电脑（cpu少） -XX:+UseSerialGC = Serial + SerialOld Serial 是复制算法（新生代） SerialOld是老年代，采用标记整理的算法 单线程，进行垃圾回收的时候需要各线程跑到安全点，然后由单一垃圾回收线程的进行垃圾回收 吞吐量优先 多线程 堆内存大，多核cpu 让单位时间内，STW时间最短 --jdk 1.8默认的垃圾回收器-XX:+UseParallelGC ~ -XX:+UseParallelOldGC // 自动调整新生代的大小，动态调整 伊甸园和suviver的大小-XX:+UseAdaptiveSizePolicy // 调整吞吐量的大小（1/（1+ratio））,指的垃圾回收的时间的占比，调的是19-XX:GCTimeRatio=ratio // 最大暂停毫秒数 默认是200ms-XX:MaxGCPauseMillis=ms -XX:ParallelGCThreads=n 响应时间优先 多线程 堆内存大，多核cpu 尽可能让单次STW的时间最短 标记清除算法 //如果老年代UseConcMarkSweepGC没有执行彻底，会用SerialOld 保底-XX:+UseConcMarkSweepGC ~ -XX:+UseParNewGC ~ SerialOld// 并行数-XX:ParallelGCThreads=n // 默认是核数的四分之一-XX:ConcGCThreads=threads -XX:CMSInitiatingOccupancyFraction=percent // 调优参数，做重新标记之前，做一次新生代的垃圾参数-XX:+CMSScavengeBeforeRemark 只有初始标记和重新标记的时候才会stw，垃圾回收过程中会产生浮动垃圾，所以CMSInitiatingOccupancyFraction 指空间到达多少后用于垃圾回收 重新标记会扫描全部的堆 G1垃圾回收期定义Garbage First 2004 论文发布 2017 JDK9 默认 使用场景 同时注重吞吐量(Throughput)和低延迟(Low latency)，默认的暂停目标是 200 ms 超大堆内存 会将堆划分为多个大小相等的 Region 整体上是 标记+整理 算法，两个区域之间是 复制 算法 相关jvm参数-XX:+UseG1GC -XX:G1HeapRegionSize=size -XX:MaxGCPauseMillis=time G1垃圾回收阶段 Young Collection会STW Young Collection +CM 在 Young GC 时会进行 GC Root 的初始标记 老年代占用堆空间比例达到阈值时，进行并发标记(不会 STW)，由下面的 JVM 参数决定 -XX:InitiatingHeapOccupancyPercent=percent 默认45%的时候会触发，即整个老年代所占的比例占整个堆内存的45% Mixed Collection会对 E、S、O 进行全面垃圾回收 最终标记(Remark)会 STW 拷贝存活(Evacuation)会 STW -XX:MaxGCPauseMillis=ms Young Collection 跨带引用问题新生代回收的跨代引用(老年代引用新生代)问题 卡表与 Remembered Set 在引用变更时通过 post-write barrier + dirty card queue concurrent refinement threads 更新 Remembered Set JDK 8u20 字符串去重 优点:节省大量内存 缺点:略微多占用了 cpu 时间，新生代回收时间略微增加 -XX:+UseStringDeduplication 将所有新分配的字符串放入一个队列 当新生代回收时，G1并发检查是否有字符串重复 如果它们值一样，让它们引用同一个 char[] 注意，与 String.intern() 不一样 String.intern() 关注的是字符串对象 而字符串去重关注的是 char[] 在 JVM 内部，使用了不同的字符串表 JDK 8u40 并发标记类卸载所有对象都经过并发标记后，就能知道哪些类不再被使用，当一个类加载器的所有类都不再使用，则卸载它所加载的所有类 -XX:+ClassUnloadingWithConcurrentMark 默认启用 JDK 8u60 回收巨型对象 一个对象大于 region 的一半时，称之为巨型对象 G1 不会对巨型对象进行拷贝 回收时被优先考虑 G1 会跟踪老年代所有 incoming 引用，这样老年代 incoming 引用为0 的巨型对象就可以在新生 代垃圾回收时处理掉 JDK 9 并发标记起始时间的调整并发标记必须在堆空间占满前完成，否则退化为 FullGC JDK 9 之前需要使用 -XX:InitiatingHeapOccupancyPercent JDK 9 可以动态调整 -XX:InitiatingHeapOccupancyPercent 用来设置初始值 进行数据采样并动态调整 总会添加一个安全的空档空间 调优 最快的GC是不发生GC新生代调优 所有的new对象都是廉价的 TLAB 死亡对象的回收代价是0 大部分对象用过即死 Minor GC的时间远远低于FULL GC 建议占总内存的大于25% 小于50% 新生代能容纳所有【并发量*（请求-响应）】的数据 幸存区大到【当前活跃对象+需要晋升的对象】 晋升阈值配置得当，让长时间存活对象尽快晋升 –XX:MaxTenuringThreshold=threshold –XX:+PringTenuringDistribution 老年代调优 Full GC 总结SerialGC新生代内存不足发生的垃圾收集 - minor gc 老年代内存不足发生的垃圾收集 - full gc ParallelGC ParallelGC新生代内存不足发生的垃圾收集 - minor gc 老年代内存不足发生的垃圾收集 - full gc CMS新生代内存不足发生的垃圾收集 - minor gc 老年代内存不足 G1新生代内存不足发生的垃圾收集 - minor gc 老年代内存不足 总结：cms和G1垃圾回收期在老年代内存不足时，如果垃圾产生速度比垃圾回收速度快，垃圾回收就会退化为串行垃圾回收，并最终触发Full GC，但是当比垃圾回收速度慢时，则不会触发Full gc 虚拟机栈程序计数器作用： 程序计数器是内存中一块较小的内存空间，主要是用来表示当前线程所执行的字节码的行号指示器。字节码解释器工作时通过计数器的值来选取下一个需要执行的字节码指令，分支、循环、跳转、异常等功能都是需要依赖这个计数器来完成。线程切换恢复到正确的执行位置都需要依靠程序计数器线程私有： 每个线程都有单独的一个程序计数器，线程之间的计数器是互不影响，独立存储的生命周期： 随着线程的创建而创建，随着线程的结束而消亡是否有OOM问题： 程序计数器是唯一一个不会抛出OutOfMemoryError的内存区域 本地方法栈","link":"/jvm-runing-area/"},{"title":"建造者模式","text":"在软件开发过程中有时需要创建一个复杂的对象，这个复杂对象通常由多个子部件按一定的步骤组合而成。例如，计算机是由 CPU、主板、内存、硬盘、显卡、机箱、显示器、键盘、鼠标等部件组装而成的，采购员不可能自己去组装计算机，而是将计算机的配置要求告诉计算机销售公司，计算机销售公司安排技术人员去组装计算机，然后再交给要买计算机的采购员。 生活中这样的例子很多，如游戏中的不同角色，其性别、个性、能力、脸型、体型、服装、发型等特性都有所差异；还有汽车中的方向盘、发动机、车架、轮胎等部件也多种多样；每封电子邮件的发件人、收件人、主题、内容、附件等内容也各不相同。 以上所有这些产品都是由多个部件构成的，各个部件可以灵活选择，但其创建步骤都大同小异。这类产品的创建无法用前面介绍的工厂模式描述，只有建造者模式可以很好地描述该类产品的创建。 模型的定义及特点建造者（Builder）模式的定义：指将一个复杂对象的构造与它的表示分离，使同样的构建过程可以创建不同的表示，这样的设计模式被称为建造者模式。它是将一个复杂的对象分解为多个简单的对象，然后一步一步构建而成。它将变与不变相分离，即产品的组成部分是不变的，但每一部分是可以灵活选择的。 该模式的主要优点如下： 封装性好，构建和表示分离。 扩展性好，各个具体的建造者相互独立，有利于系统的解耦。 客户端不必知道产品内部组成的细节，建造者可以对创建过程逐步细化，而不对其它模块产生任何影响，便于控制细节风险。其缺点如下： 产品的组成部分必须相同，这限制了其使用范围。 如果产品的内部变化复杂，如果产品内部发生变化，则建造者也要同步修改，后期维护成本较大。 建造者（Builder）模式和工厂模式的关注点不同：建造者模式注重零部件的组装过程，而工厂方法模式更注重零部件的创建过程，但两者可以结合使用。 角色 产品角色（Product）：它是包含多个组成部件的复杂对象，由具体建造者来创建其各个零部件。 抽象建造者（Builder）：它是一个包含创建产品各个子部件的抽象方法的接口，通常还包含一个返回复杂产品的方法 getResult()。 具体建造者(Concrete Builder）：实现 Builder 接口，完成复杂产品的各个部件的具体创建方法。 指挥者（Director）：它调用建造者对象中的部件构造与装配方法完成复杂对象的创建，在指挥者中不涉及具体产品的信息。 代码组成部分（子部件） public class PartA { public int a=1;}public class PartB { public int b=1;}public class PartC { public int c=1;} 产品角色 public class Car { PartA partA; PartB partB; PartC partC; public void setPartA(PartA partA) { this.partA = partA; } public void setPartB(PartB partB) { this.partB = partB; } public void setPartC(PartC partC) { this.partC = partC; } public void show(){ System.out.println(partA.a); }} 抽象建造者 abstract class Builder { protected Car car = new Car(); abstract void setPartA(); abstract void setPartB(); abstract void setPartC(); public Car getCar() { return car; }} 具体建造者 public class ConcreteBuilder extends Builder { @Override void setPartA() { car.setPartA(new PartA()); } @Override void setPartB() { car.setPartB(new PartB()); } @Override void setPartC() { car.setPartC(new PartC()); }} 指挥者 public class Director { private Builder builder; public Director(Builder builder) { this.builder = builder; } public Car getCar(){ builder.setPartA(); builder.setPartB(); builder.setPartC(); return builder.getCar(); }} 消费者 public class Custom { public static void main(String[] args) { Director director=new Director(new ConcreteBuilder()); Car car = director.getCar(); car.show(); }}","link":"/design-pattern-builder/"},{"title":"代理模式","text":"为什么要学习代理模式，因为这是springAop的底层实现。[springAop和springMvc] 静态代理角色分析 抽象角色：一般会使用接口或者抽象类来解决 真实角色：被代理的角色 代理角色：代理真实角色，代理真实角色之后一般我们会做一些附属操作 客户 代码 真实角色 public class Host implements Rent { public void rent() { System.out.println(&quot;房东出租房子&quot;); }} 代理 public class Proxy implements Rent { Host host; public Proxy(Host host) { kanfang(); this.host = host; money(); qianhetong(); } public Proxy() { } public void rent() { this.host.rent(); } public void money() { System.out.println(&quot;收钱&quot;); } public void kanfang() { System.out.println(&quot;看房&quot;); } public void qianhetong() { System.out.println(&quot;签合同&quot;); }} 接口（公共事情） public interface Rent { /** * 出租房子 */ void rent();} 客户 public class Client { public static void main(String[] args) { Host host=new Host(); Proxy proxy = new Proxy(host); proxy.rent(); }} 代理模式的好处 使真实角色更加纯粹 公共也交给了代理角色，实现了业务的分工 公共业务实现了拓展可以集中管理 缺点 一个真实角色就会有产生一个代理角色，代码量会翻倍 动态代理 动态代理和静态代理角色一样 动态代理的动态类是自动生成的，不是我们之前写好的 动态代理分为两大类，基于接口的动态代理，基于类的动态代理 基于接口：JDK动态代理 基于类：clib java字节码实现：javassist 需要了解2个类：Proxy 和 invocationHandler（调用处理程序） 代码 动态代理 public class ProxyInvocationHandler implements InvocationHandler { private Object target; public void setTarget(Object target) { this.target = target; } // Foo f = (Foo) Proxy.newProxyInstance(Foo.class.getClassLoader(),// new Class&lt;?&gt;[] { Foo.class },// handler); public Object getProxy() { return Proxy.newProxyInstance(this.getClass().getClassLoader(), this.target.getClass().getInterfaces(), this); } /** * @param proxy * @param method * @param args * @return * @throws Throwable */ public Object invoke(Object proxy, Method method, Object[] args) throws Throwable { Object invoke = method.invoke(target, args); return invoke; }} 真实角色 public class Host implements Rent { public void rent() { System.out.println(&quot;房东出租房子&quot;); }} 接口 public interface Rent { /** * 出租房子 */ void rent();} 客户 public class Client { public static void main(String[] args) { Host2 host=new Host2(); ProxyInvocationHandler pih = new ProxyInvocationHandler(); pih.setTarget(host); Rent2 rent= (Rent2) pih.getProxy(); System.out.println(rent.rent(&quot;a&quot;)); }}","link":"/design-pattern-proxy/"},{"title":"中介者模式","text":"","link":"/design-pattern-factory/"},{"title":"命令模式","text":"","link":"/design-pattern-factory/"},{"title":"备忘录模式","text":"","link":"/design-pattern-factory/"},{"title":"模板方法模式","text":"","link":"/design-pattern-factory/"},{"title":"状态模式","text":"","link":"/design-pattern-factory/"},{"title":"策略模式","text":"","link":"/design-pattern-factory/"},{"title":"观察者模式","text":"","link":"/design-pattern-factory/"},{"title":"解释器模式","text":"","link":"/design-pattern-factory/"},{"title":"责任链模式","text":"","link":"/design-pattern-factory/"},{"title":"迭代子模式","text":"","link":"/design-pattern-factory/"},{"title":"二维数组中的查找","text":"在一个 n * m 的二维数组中，每一行都按照从左到右递增的顺序排序，每一列都按照从上到下递增的顺序排序。请完成一个高效的函数，输入这样的一个二维数组和一个整数，判断数组中是否含有该整数。 示例: 现有矩阵 matrix 如下： [ [1, 4, 7, 11, 15], [2, 5, 8, 12, 19], [3, 6, 9, 16, 22], [10, 13, 14, 17, 24], [18, 21, 23, 26, 30]]给定 target = 5，返回 true。 给定 target = 20，返回 false。 限制： 0 &lt;= n &lt;= 1000 0 &lt;= m &lt;= 1000 来源：力扣（LeetCode）链接：https://leetcode-cn.com/problems/er-wei-shu-zu-zhong-de-cha-zhao-lcof著作权归领扣网络所有。商业转载请联系官方授权，非商业转载请注明出处。 思路如下：从矩阵的右上角开始遍历，如果比target小，则col++，如果比target大，则row–，找到target则返回true，找不到则返回false； class Solution { public boolean findNumberIn2DArray(int[][] matrix, int target) { if(matrix == null || matrix.length == 0) { return false; } // m 代表行数，n代表列数 int m = matrix.length, n = matrix[0].length; int row = 0, col = n - 1; while(row &lt; m &amp;&amp; col &gt;= 0) { if(matrix[row][col] &gt; target) { col--; }else if(matrix[row][col] &lt; target) { row++; }else { return true; } } return false; }}","link":"/problem-leetcode-jianzhi04/"},{"title":"替换空格","text":"请实现一个函数，把字符串 s 中的每个空格替换成”%20”。 示例 1： 输入：s = “We are happy.”输出：”We%20are%20happy.” 限制： 0 &lt;= s 的长度 &lt;= 10000 来源：力扣（LeetCode）链接：https://leetcode-cn.com/problems/ti-huan-kong-ge-lcof著作权归领扣网络所有。商业转载请联系官方授权，非商业转载请注明出处。 解题思路：String转换成char数组，遍历 class Solution { public String replaceSpace(String s) { char[]schar=s.toCharArray(); StringBuilder sb=new StringBuilder(); for(char temp:schar){ if(' '==temp){ sb.append(&quot;%20&quot;); }else{ sb.append(temp); } } return sb.toString(); }}","link":"/problem-leetcode-jianzhi05/"},{"title":"从尾到头打印单链表","text":"输入一个链表的头节点，从尾到头反过来返回每个节点的值（用数组返回）。 示例 1： 输入：head = [1,3,2]输出：[2,3,1] 解题思路：栈是先入后出的，用栈暂存单链表节点的值 /** * Definition for singly-linked list. * public class ListNode { * int val; * ListNode next; * ListNode(int x) { val = x; } * } */class Solution { public int[] reversePrint(ListNode head) { if(head==null){ return new int[]{}; } Stack&lt;Integer&gt;stack=new Stack&lt;&gt;(); ListNode node=head; int length=0; while(node !=null){ stack.push(node.val); node=node.next; length++; } int[]a=new int[length]; for(int i=0;i&lt;length;i++){ a[i]=stack.pop(); } return a; }} 另：用栈的地方就可以用递归，可以思考一下递归的写法。","link":"/problem-leetcode-jianzhi06/"},{"title":"用两个栈实现队列","text":"用两个栈实现一个队列。队列的声明如下，请实现它的两个函数 appendTail 和 deleteHead ，分别完成在队列尾部插入整数和在队列头部删除整数的功能。(若队列中没有元素，deleteHead 操作返回 -1 ) 示例 1： 输入：[“CQueue”,”appendTail”,”deleteHead”,”deleteHead”][[],[3],[],[]]输出：[null,null,3,-1]示例 2： 输入：[“CQueue”,”deleteHead”,”appendTail”,”appendTail”,”deleteHead”,”deleteHead”][[],[],[5],[2],[],[]]输出：[null,-1,null,null,5,2]提示： 1 &lt;= values &lt;= 10000最多会对 appendTail、deleteHead 进行 10000 次调用 来源：力扣（LeetCode）链接：https://leetcode-cn.com/problems/yong-liang-ge-zhan-shi-xian-dui-lie-lcof著作权归领扣网络所有。商业转载请联系官方授权，非商业转载请注明出处。 思路：两个栈实现队列，一个栈（first）负责入，一个栈（second）负责出，当出栈（second）为空时，把入栈（first）的元素放入出栈（second），再进行出栈。 class CQueue { Stack&lt;Integer&gt;firstStack; Stack&lt;Integer&gt;secondStack; public CQueue() { firstStack=new Stack&lt;&gt;(); secondStack=new Stack&lt;&gt;(); } public void appendTail(int value) { firstStack.push(value); } public int deleteHead() { // 如果出栈为空，则看栈1的情况 if(secondStack.isEmpty()){ // 如果栈1也为空，则说明两个栈都没有元素，说明队列为空，返回-1； if(firstStack.isEmpty()){ return -1; }else{ // 遍历栈1，将栈1的元素放入栈2; while(!firstStack.isEmpty()){ secondStack.push( firstStack.pop()); } return secondStack.pop(); } }else{ return secondStack.pop(); } }}/** * Your CQueue object will be instantiated and called as such: * CQueue obj = new CQueue(); * obj.appendTail(value); * int param_2 = obj.deleteHead(); */","link":"/problem-leetcode-jianzhi06/"},{"title":"青蛙跳台阶问题","text":"一只青蛙一次可以跳上1级台阶，也可以跳上2级台阶。求该青蛙跳上一个 n 级的台阶总共有多少种跳法。 答案需要取模 1e9+7（1000000007），如计算初始结果为：1000000008，请返回 1。 示例 1： 输入：n = 2输出：2示例 2： 输入：n = 7输出：21示例 3： 输入：n = 0输出：1提示： 0 &lt;= n &lt;= 100 来源：力扣（LeetCode）链接：https://leetcode-cn.com/problems/qing-wa-tiao-tai-jie-wen-ti-lcof著作权归领扣网络所有。商业转载请联系官方授权，非商业转载请注明出处。 思路：这是典型的斐波那契数列，分析一下，青蛙跳上n级台阶的跳法，比如跳3级的时候有3种，调4级的时候，F（3）+F（2），这个结论思想是：青蛙可以留下1级台阶，也可以留下2级台阶，所以F(n)=F(n-1)+F(n-2) class Solution { public int numWays(int n) { if(n==0){ return 1; } if(n==1){ return 1; } List&lt;Integer&gt;list=new ArrayList&lt;&gt;(); list.add(1); list.add(1); for(int i=2;i&lt;=n;i++){ // 答案需要取模 1e9+7（1000000007），如计算初始结果为：1000000008，请返回 1。 list.add((list.get(i-1)+list.get(i-2))%1000000007); } return list.get(n); }}","link":"/problem-leetcode-jianzhi10-ii/"},{"title":"斐波那契数列","text":"写一个函数，输入 n ，求斐波那契（Fibonacci）数列的第 n 项（即 F(N)）。斐波那契数列的定义如下： F(0) = 0, F(1) = 1F(N) = F(N - 1) + F(N - 2), 其中 N &gt; 1.斐波那契数列由 0 和 1 开始，之后的斐波那契数就是由之前的两数相加而得出。 答案需要取模 1e9+7（1000000007），如计算初始结果为：1000000008，请返回 1。 来源：力扣（LeetCode）链接：https://leetcode-cn.com/problems/fei-bo-na-qi-shu-lie-lcof著作权归领扣网络所有。商业转载请联系官方授权，非商业转载请注明出处。 思路：如果单纯的斐波那契递归来实现的话，leetcode会提示超时，如下代码 class Solution { public int fib(int n) { if(n==0){ return 0; } if(n==1){ return 1; } return fib(n-1)+fib(n-2); }} 所以用一个list 数组去存储 class Solution { public int fib(int n) { if(n==0){ return 0; } if(n==1){ return 1; } List&lt;Integer&gt;list=new ArrayList&lt;&gt;(); list.add(0); list.add(1); for(int i=2;i&lt;=n;i++){ // 答案需要取模 1e9+7（1000000007），如计算初始结果为：1000000008，请返回 1。 list.add((list.get(i-1)+list.get(i-2))%1000000007); } return list.get(n); }}","link":"/problem-leetcode-jianzhi10-i/"},{"title":"旋转数组的最小数字","text":"把一个数组最开始的若干个元素搬到数组的末尾，我们称之为数组的旋转。输入一个递增排序的数组的一个旋转，输出旋转数组的最小元素。例如，数组 [3,4,5,1,2] 为 [1,2,3,4,5] 的一个旋转，该数组的最小值为1。 示例 1： 输入：[3,4,5,1,2]输出：1示例 2： 输入：[2,2,2,0,1]输出：0 来源：力扣（LeetCode）链接：https://leetcode-cn.com/problems/xuan-zhuan-shu-zu-de-zui-xiao-shu-zi-lcof著作权归领扣网络所有。商业转载请联系官方授权，非商业转载请注明出处。 思路：遍历数组，找到数组中当前元素比上一个元素小的最开始的位置，如果没有找到，则数组为升序，返回第一个元素。 class Solution { public int minArray(int[] numbers) { if(numbers.length==1){ return numbers[0]; } for(int i=1;i&lt;numbers.length;i++){ if(numbers[i]&lt;numbers[i-1]){ return numbers[i]; } } return numbers[0]; }}","link":"/problem-leetcode-jianzhi11/"}],"tags":[{"name":"linux","slug":"linux","link":"/tags/linux/"},{"name":"排序算法","slug":"排序算法","link":"/tags/%E6%8E%92%E5%BA%8F%E7%AE%97%E6%B3%95/"},{"name":"算法","slug":"算法","link":"/tags/%E7%AE%97%E6%B3%95/"},{"name":"线上问题","slug":"线上问题","link":"/tags/%E7%BA%BF%E4%B8%8A%E9%97%AE%E9%A2%98/"},{"name":"设计模式","slug":"设计模式","link":"/tags/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/"},{"name":"log4j2","slug":"log4j2","link":"/tags/log4j2/"},{"name":"JVM","slug":"JVM","link":"/tags/JVM/"},{"name":"jvm","slug":"jvm","link":"/tags/jvm/"},{"name":"spring","slug":"spring","link":"/tags/spring/"},{"name":"微服务","slug":"微服务","link":"/tags/%E5%BE%AE%E6%9C%8D%E5%8A%A1/"},{"name":"基础知识点","slug":"基础知识点","link":"/tags/%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86%E7%82%B9/"},{"name":"java","slug":"java","link":"/tags/java/"},{"name":"源码","slug":"源码","link":"/tags/%E6%BA%90%E7%A0%81/"},{"name":"kafka","slug":"kafka","link":"/tags/kafka/"},{"name":"clickhouse","slug":"clickhouse","link":"/tags/clickhouse/"},{"name":"flink","slug":"flink","link":"/tags/flink/"},{"name":"es","slug":"es","link":"/tags/es/"},{"name":"大数据","slug":"大数据","link":"/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/"},{"name":"工具","slug":"工具","link":"/tags/%E5%B7%A5%E5%85%B7/"},{"name":"docker","slug":"docker","link":"/tags/docker/"},{"name":"jenkins","slug":"jenkins","link":"/tags/jenkins/"},{"name":"k8s","slug":"k8s","link":"/tags/k8s/"},{"name":"prometheus","slug":"prometheus","link":"/tags/prometheus/"},{"name":"mysql","slug":"mysql","link":"/tags/mysql/"},{"name":"redis","slug":"redis","link":"/tags/redis/"},{"name":"architect","slug":"architect","link":"/tags/architect/"},{"name":"剑指offer","slug":"剑指offer","link":"/tags/%E5%89%91%E6%8C%87offer/"}],"categories":[]}